%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Copyright (c) 2024 The authors
%
% This work is licensed under a
% Creative Commons Attribution-ShareAlike 4.0 International License.
%
% You should have received a copy of the license along with this
% work. If not, see <http://creativecommons.org/licenses/by-sa/4.0/>.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass{article}

%% Packages
\usepackage{arxiv}
\usepackage{authblk}
\usepackage[T1]{fontenc}   
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage[english]{babel}
\usepackage{mathtools}
\usepackage{enumitem}
\usepackage{amssymb}
\usepackage{microtype}
\usepackage[table,usenames,dvipsnames]{xcolor}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{placeins}
\usepackage{bm}
\usepackage[authoryear]{natbib}
\usepackage[
  colorlinks,
  linkcolor=blue,
  citecolor=blue,
  urlcolor=blue,
  filecolor=blue,
  backref=page
]{hyperref}
\usepackage{graphicx}

%% Graphics path
\graphicspath{%
  {./},
  {./figures}
}

%% Bibliography style
\bibliographystyle{ba.bst}

%% Show links only in year in citations
\citefix{} % defined in arxiv.sty

%% Insert space before backref pages in reference list
\NewCommandCopy{\oldbackref}{\backref}
\renewcommand*{\backref}[1]{
  \hspace{0.1em}\oldbackref{#1}
}

%% Author block typeset
\renewcommand\Authfont{\bfseries}
\setlength{\affilsep}{0em}

%% Space between rows in tables
\renewcommand{\arraystretch}{1.2}

%% Emphasis in tables
\newcommand\firstcolor[1]{\textbf{#1}}
\newcommand\secondcolor[1]{{\color{RoyalBlue}{#1}}}
%\newcommand\secondcolor[1]{\textit{#1}}

%% Maths settings
\numberwithin{equation}{section}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{definition}[theorem]{Definition}

%% Custom commands
\renewcommand{\epsilon}{\varepsilon}

\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\tTheta}{\tilde{\Theta}}
\newcommand{\I}{\mathbb{I}}

\DeclareMathOperator{\Var} {Var}
\DeclareMathOperator{\Cov} {Cov}
\DeclareMathOperator{\supp} {supp}

\newcommand\dotprod[2]{\left\langle#1,#2\right\rangle}

%% Title and author information
\title{A Bayesian approach to functional regression: theory and computation}

\date{\today}

\newbox{\orcid}\sbox{\orcid}{\includegraphics[scale=0.06]{orcid.pdf}} 
\author[1,2]{%
	\href{https://orcid.org/0000-0003-0728-7748}{\usebox{\orcid}\hspace{1mm}José R.~Berrendero\thanks{\texttt{\href{mailto:joser.berrendero@uam.es}{joser.berrendero@uam.es}}}}%
}
\author[1]{%
	\href{https://orcid.org/0009-0004-7554-9193}{\usebox{\orcid}\hspace{1mm}Antonio Coín\thanks{\texttt{\href{mailto:antonio.coin@uam.es}{antonio.coin@uam.es}} (corresponding author)}}%
}
\author[1,2]{%
	\href{https://orcid.org/0000-0002-7993-0096}{\usebox{\orcid}\hspace{1mm}Antonio Cuevas\thanks{\texttt{\href{mailto:antonio.cuevas@uam.es}{antonio.cuevas@uam.es}}}}%
}
\affil[1]{Departamento de Matemáticas, Universidad Autónoma de Madrid (UAM), Madrid, Spain}
\affil[2]{Instituto de Ciencias Matemáticas ICMAT (CSIC-UAM-UC3M-UCM), Madrid, Spain}
\newcommand\shortauthor{J. R. Berrendero, A. Coín and A. Cuevas}

%% PDF metadata
\hypersetup{
pdftitle={A Bayesian approach to functional regression: theory and computation},
pdfsubject={Preprint},
pdfauthor={José R.~Berrendero, Antonio Coín, Antonio Cuevas},
pdfkeywords={functional data analysis, functional regression, reproducing kernel Hilbert space, reversible jump MCMC, Bayesian inference, posterior consistency},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Document body
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

  \maketitle

  %% Abstract
  \begin{abstract}
    We propose a novel Bayesian methodology for inference in functional linear and logistic regression models based on the theory of reproducing kernel Hilbert spaces (RKHS's). We introduce general models that build upon the RKHS generated by the covariance function of the underlying stochastic process, and whose formulation includes as particular cases all finite-dimensional models based on linear combinations of marginals of the process, which can collectively be seen as a dense subspace made of simple approximations. By imposing a suitable prior distribution on this dense functional space we can perform data-driven inference via standard Bayes methodology, estimating the posterior distribution through reversible jump Markov chain Monte Carlo methods. In this context, our contribution is two-fold. First, we derive a theoretical result that guarantees posterior consistency, based on an application of a classic theorem of Doob to our RKHS setting. Second, we show that several prediction strategies stemming from our Bayesian procedure are competitive against other usual alternatives in both simulations and real data sets, including a Bayesian-motivated variable selection method.
  \end{abstract}

  %% Keywords
  \keywords{functional data analysis \and functional regression \and reproducing kernel Hilbert space \and Bayesian inference \and reversible jump MCMC \and posterior consistency}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Introduction}\label{sec:intro}

The problem of predicting a scalar response from a functional covariate is one that has gained traction over the last few decades, as more and more data is being generated with an ever-increasing level of granularity in the measurements. While in principle the functional data could simply be regarded as a discretized vector in a very high dimension, there are often many advantages in taking into account its functional nature, ranging from modeling the correlation among points that are close in the domain, to extracting information that may be hidden in the derivatives of the function in question. As a consequence, numerous proposals have arisen on how to suitably deal with functional data, all of them encompassed under the term Functional Data Analysis (FDA), which essentially explores statistical techniques to process, model and make inference on data varying over a continuum. A partial survey on such methods is \citet{cuevas2014partial} or \citet{goia2016introduction}, while a more detailed exposition of the theory and applications can be found for example in \citet{hsing2015theoretical}.

FDA is undoubtedly an active area of research, which finds applications in a wide variety of fields, such as biomedicine, finance, meteorology or chemistry \citep{ullah2013applications}. Accordingly, there are many recent contributions on how to tackle functional data problems, both from a theoretical and practical standpoint. Chief among them is the approach of reducing the problem to a finite-dimensional one, for example using a truncated basis expansion or spline interpolation methods \citep{muller2005generalized, aguilera2013comparative}. At the same time, much effort has been put into building a sound theoretical basis for FDA, generalizing different frequentist concepts to the infinite-dimensional framework. Examples of this endeavor include the definition of centrality measures and depth-based notions for functional data \citep{lopez2009concept}, functional ANOVA tests \citep{cuevas2004anova}, a functional Mahalanobis distance \citep{galeano2015mahalanobis, berrendero2020mahalanobis}, or an extension of Fisher's discriminant analysis for random functions \citep{shin2008extension}, among many others. As the name suggests, FDA techniques are heavily inspired by functional analysis tools and methods: Hilbert spaces, linear operators, orthonormal bases, and so on. Incidentally, a notion that also intersects with the classical theory of machine learning and pattern recognition, and that has attained popularity in recent years, is that of reproducing kernel Hilbert spaces (RKHS's) and their applications in functional data problems (e.g.~\citealp{kupresanin2010rkhs, yuan2010reproducing, berrendero2018use}). On the other hand, Bayesian inference methods are ubiquitous in the realm of statistics, and though they also make use of random functions, the approach is slightly different from the FDA case. While there are recent works that offer a Bayesian treatment of functional data \citep[e.g.][]{crainiceanu2010bayesian, shi2011gaussian}, there is still no systematic approach to Bayesian methodologies within FDA. It is precisely at this relatively unexplored intersection between FDA and Bayesian methods that our work is aimed.

In particular, our goal is to study functional regression problems, the infinite-dimensional equivalents of the typical statistical regression problems, from a Bayesian perspective. We follow the path started by \citet{ferguson1974prior} of setting a prior distribution on a functional space and using the corresponding posterior for inference. In our case, we use a particular RKHS as the ambient space, resulting in functional regression models that allow for a simple yet efficient Bayesian treatment of functional data. Moreover, we also study the basic theoretical question of posterior consistency in these RKHS models within the proposed Bayesian framework. Consistency and posterior concentration are a type of frequentist validation criteria that have arguably been an active point of research in the last few decades, particularly in infinite-dimensional settings \citep{amewou2003posterior, choi2008remarks}, and also in the functional regression case \citep{lian2016posterior,abraham2020posterior}. To put it simply, posterior consistency ensures that with enough data points, the Bayesian updating mechanism works as intended and the posterior distribution eventually concentrates around the true value of the parameters, supposing the model is well specified. We leverage the properties of RKHS's and existing techniques to show that posterior consistency holds in our models under some mild identifiability conditions, thus providing a coherent background to our Bayesian approach. 

Finally, this theoretical side is complemented by extensive experimentation that showcases the application of the models in various prediction tasks. Following recent trends in Bayesian computation techniques, the posterior distribution is approximated via Markov chain Monte Carlo (MCMC) methods, specifically the \textit{reversible jump} variant (RJMCMC) proposed by \citet{green1995reversible}. This computational work highlights the predictive performance of the proposed functional regression models, especially when compared with other usual frequentist methods. 

\subsubsection*{\(\bm{L^2}\)-models, shortcomings and alternatives}

In this work we are concerned with functional linear and logistic regression models, that is, situations where the goal is to predict a continuous or dichotomous variable from functional observations. Even though these problems can be formally stated with almost no differences from their finite-dimensional counterparts, there are some fundamental challenges as well as some subtle drawbacks that emerge as a result of working in infinite dimensions. To set a common framework, we will consider throughout a scalar response variable \(Y\) (either continuous or binary) which has some dependence on a stochastic \(L^2\)-process \(X=X(t)=X(t, \omega)\) with trajectories in \(L^2[0, 1]\), observed on a dense grid. We will further suppose for simplicity that \(X\) is centered, that is, its mean function \(m(t)=\E[X(t)]\) vanishes for all \(t\in[0,1]\). In addition, we will tacitly assume the existence of a labeled data set \(\mathcal D_n =\{(X_i, Y_i): i=1,\dots, n\}\) of independent observations from \((X, Y)\), and our aim will be to accurately predict the response corresponding to unlabeled samples from \(X\).

The most common scalar-on-function linear regression model is the classical \(L^2\)-model, widely popularized since the first edition (1997) of the pioneering monograph by~\citet{ramsay2005functional}. It can be seen as a generalization of the usual finite-dimensional model, replacing the scalar product in \(\R^d\) for that of the functional space \(L^2[0,1]\) (henceforth denoted by \(\dotprod{\cdot}{\cdot}\)):
\begin{equation}\label{eq:l2-linear-model}
  Y = \alpha_0 + \dotprod{X}{\beta} + \epsilon = \alpha_0 + \int_0^1 X(t)\beta(t)\, dt + \epsilon,
\end{equation}
where \(\alpha_0\in \R\), \(\epsilon\) is a random error term independent from \(X\) with \(\E [\epsilon]=0\), and the functional slope parameter \(\beta=\beta(\cdot)\) is a member of the infinite-dimensional space \(L^2[0, 1]\). In this case, the inference on \(\beta\) is hampered by the fact that \(L^2[0,1]\) is an extremely broad space that contains many non-smooth or ill-behaved functions, so any estimation procedure involving optimization on it will typically be hard. In spite of this, model~\eqref{eq:l2-linear-model} is not flexible enough to include ``simple'' finite-dimensional models based on linear combinations of the marginals, such as \(Y=\alpha_0 + \beta_1 X(t_1)+ \cdots + \beta_p X(t_p) + \epsilon\) for some constants \(\beta_j\in\R\) and instants \(t_j\in[0,1]\), which are especially appealing to practitioners confronted with functional data problems; see \citet{berrendero2024functional} for additional details on this. Moreover, the non-invertibility of the covariance operator associated with \(X\), which plays the role of the covariance matrix in the infinite case, invalidates the usual least squares theory \citep{cardot2011functional}. Thus, some regularization or dimensionality reduction technique is needed for parameter estimation; see \citet{reiss2017methods} for a summary of several widespread methods.

A similar \(L^2\)-based functional logistic equation can be derived for the binary classification problem via the logistic function:
\begin{equation}\label{eq:l2-logistic-model}
  \mathbb P(Y=1 \mid X) = \frac{1}{1 + \exp\{-\alpha_0 - \dotprod{X}{\beta}\}},
\end{equation}
where \(\alpha_0 \in \R\) and \(\beta \in L^2[0, 1]\). In this situation, the most common way of estimating the slope function \(\beta\) is via its maximum likelihood estimator (MLE). The same complications as in the linear case apply here, with the additional problem that in functional settings the MLE does not exist with probability one under fairly general conditions \citep[see][]{berrendero2023functional}.

It turns out that in both scenarios a natural alternative to the \(L^2\)-model is the so-called reproducing kernel Hilbert space (RKHS) model, which instead assumes the unknown functional parameter \(\beta\) to be a member of the RKHS associated with the covariance function of the process \(X\). As we will show later on, not only is this model simpler and arguably easier to interpret, but it also constrains the parameter space to smoother and more manageable functions. In fact, it does include a model based on finite linear combinations of the marginals of \(X\) as a particular case, while also generalizing the aforementioned \(L^2\)-models under some conditions. These RKHS-based models and their idiosyncrasies have been explored in \citet{berrendero2019rkhs, berrendero2024functional} in the linear setting, and in \citet{berrendero2023functional} in the logistic case. 

A major aim of this work is to motivate these models inside the functional framework, while also providing efficient techniques to apply them in practice. Our main contribution is the proposal of a Bayesian approach for inference in these RKHS models, in which a prior distribution is imposed on \(\beta\) to use the posterior probabilities for prediction. Although setting a prior distribution on a functional space is generally a hard task, the specific parametric formulation we propose greatly facilitates this. Similar Bayesian schemes have recently been explored in \citet{grollemund2019bayesian} and \citet{abraham2024informative}, albeit not within a RKHS framework. Another set of techniques extensively studied in this context are variable selection methods, which aim to select the marginals \(\{X(t_j)\}\) of the process that better summarize it according to some optimality criterion (see \citealp{ferraty2010most} or \citealp{berrendero2016variable} by way of illustration). As it happens, some RKHS-based variable selection methods have already been proposed \citep[e.g.][]{bueno2019variable}, but in general they have their own dedicated algorithms and procedures. As will shortly become apparent, our Bayesian methodology allows us to easily isolate the marginal posterior distribution corresponding to a finite set of points \(\{t_j\}\), providing a Bayesian variable selection process along with the other prediction methods that naturally arise from the posterior.

\subsubsection*{Some essentials on RKHS's and notation}\label{sec:rkhs}

The methodology proposed in this work relies heavily on the use of RKHS's, so we outline the main characteristics of these spaces from a probabilistic point of view \citep[for a more detailed account, see for example][]{berlinet2004reproducing}. Let us denote by \(K(t, s)= \mathbb E[X(t)X(s)]\) the covariance function of the centered process \(X\), and in what follows suppose that it is continuous. To construct the corresponding RKHS \(\Hcal(K)\), we start by defining the functional space \(\Hcal_0(K)\) of all finite linear combinations of evaluations of \(K\), that is,
\begin{equation}\label{eq:h0}
  \Hcal_0(K) = \left\{ f \in L^2[0,1]: \ f(\cdot) = \sum_{j=1}^p a_j K(t_j, \cdot), \ p \in \N, \ a_j \in \R, \ t_j \in [0, 1] \right\}.
\end{equation}
This space is endowed with the inner product \(\dotprod{f}{g}_K = \sum_{i, j} a_i b_j K(t_i, s_j)\), given that \(f(\cdot)=\sum_i a_i K(t_i, \cdot) \) and \(g(\cdot)=\sum_j b_j K(s_j, \cdot)\). Then, \(\Hcal(K)\) is defined to be the completion of \(\Hcal_0(K)\) under the norm induced by the scalar product \(\dotprod{\cdot}{\cdot}_K\). As it turns out, functions in this space satisfy the so-called \textit{reproducing property}: \(\dotprod{K(t, \cdot)}{f}_K = f(t)\) for all \(f \in \Hcal(K)\) and \(t \in [0, 1]\). An important consequence of this identity is that \(\Hcal(K)\) is a space of genuine functions and not of equivalence classes, since the values of the functions at specific points are in fact relevant, unlike in \(L^2\)-spaces.

Now, a particularly useful approach in statistics is to regard \(\Hcal(K)\) as an isometric copy of a well-known space. Specifically, via \textit{Loève's isometry} \citep{loeve1948fonctions} one can establish a congruence \(\Psi_X\) between \(\Hcal(K)\) and the linear span of the process, \(\mathcal L(X)\), in the space of all random variables with finite second moment, \(L^2(\Omega)\) \citep[see Lemma 1.1 in][]{lukic2001stochastic}. This isometry is essentially the completion of the correspondence
\begin{equation*}
  \sum_{j=1}^p a_j X(t_j) \longleftrightarrow \sum_{j=1}^p a_j K(t_j, \cdot),
\end{equation*}
and can be formally defined, in terms of its inverse, as \(\Psi^{-1}_X(U)(t) = \E[U X(t)]\) for \(U \in \mathcal L(X)\).
Despite the close connection between the process \(X\) and the space \(\Hcal(K)\), special care must be taken when dealing with concrete realizations, since in general the trajectories of \(X\) do not belong to the corresponding RKHS with probability one \citep[][Corollary~7.1]{lukic2001stochastic}. As a consequence, the expression \(\dotprod{x}{f}_K\) is ill-defined and lacks meaning when \(x\) is a realization of \(X\). However, following Parzen's approach in his seminal work \citep[][Theorem~4E]{parzen1961approach}, we can leverage Loève's isometry and identify \(\dotprod{x}{f}_K \) with the image \( \Psi_x(f) := \Psi_X(f)(\omega)\), for \(x=X(\omega)\) and \(f\in \Hcal(K)\). This notation, viewed as a formal extension of the inner product, often proves to be useful and convenient.

\subsubsection*{Organization of the article}

The rest of the paper is organized as follows. In Section~\ref{sec:methodology} we explain the Bayesian methodology and the functional regression models we propose, including an overview of the reversible jump MCMC scheme. In Section~\ref{sec:consistency} we derive a theoretical posterior consistency result. The empirical results of the experimentation are contained in Section~\ref{sec:results}, along with a short discussion of computational details. Lastly, the conclusions drawn from this work are presented in Section~\ref{sec:conclusion}. Moreover, additional details and results are included in Appendices~\ref{app:model-choice},~\ref{app:theory},~\ref{app:experiments} and~\ref{app:source-code}.


\section{A Bayesian methodology for RKHS-based functional regression models}\label{sec:methodology}

The functional models contemplated here are those obtained by considering a functional parameter \(\alpha \in \Hcal(K)\) and replacing the scalar product for \(\dotprod{X}{\alpha}_K\) in the \(L^2\)-models~\eqref{eq:l2-linear-model} and~\eqref{eq:l2-logistic-model}. As previously explained, this change in perspective has tangible benefits both in theory and practice on account of the RKHS properties. However, to further simplify things we will follow a parametric approach and suppose that \(\alpha\) is in fact a member of the dense subspace \(\Hcal_0(K)\) defined in~\eqref{eq:h0}. Although natural in this context, this crucial assumption may seem too restrictive at first glance. Nonetheless, we will demonstrate that this scheme has a firm theoretical backing and that the resulting models perform sufficiently well in applied settings, seeming not to need the comprehensiveness (and complications) of the larger \(L^2\)-spaces. 

As we said before, with a slight abuse of notation we will understand the expression \(\dotprod{x}{\alpha}_K\) as \(\Psi_x(\alpha)\), where \(x=X(\omega)\) and \(\Psi_x\) is Loève's isometry. Hence, taking into account that \(\alpha \in \Hcal_{0}(K)\) and that \(\Psi_X(K(t, \cdot)) = X(t)\) by definition, we can write \(\dotprod{x}{\alpha}_K \equiv \sum_j \beta_j x(t_j)\) when \(\alpha(\cdot)=\sum_j\beta_j K(t_j, \cdot)\) and \(x\) is a realization of \(X\). In this way we get a simpler, finite-dimensional approximation of the functional RKHS model, which we argue reduces the overall complexity while still capturing most of the relevant information. By restricting the ambient space to \(\Hcal_0(K)\) we are essentially truncating the value of \(p\), the dimensionality, and thus advocating for parsimony. Moreover, the model remains ``truly functional'' in the sense that \(p\) is variable: we are exploiting the RKHS perspective to give a functional nature to \textit{all} finite-dimensional models based on linear combinations of the marginals. Even though we are selecting a set of impact points \(\{t_j\}\) that depends on the observed grid, we can circumvent problems with this approach by smoothing or interpolating the functional data in a pre-processing step.

In view of~\eqref{eq:h0} and Loève's isometry, to set a prior distribution on the unknown function \(\alpha\) (that is, a prior distribution on the functional space \(\Hcal_{0}(K)\)) it suffices to consider a discrete distribution on the number of components \(p\), and then impose \(p\)-dimensional continuous prior distributions on the coefficients \(\beta_j\) and the times \(t_j\) given \(p\). Thanks to this parametric approach, the challenging task of setting a prior distribution on a space of functions is considerably simplified, while simultaneously not constraining the model to any specific distribution (in contrast to, say, Gaussian process regression methods). Moreover, note that our simplifying assumption on \(\alpha\) is not actually very strong, since any prior distribution \(\mathbb{P}_0\) on \(\Hcal_0(K)\) can be directly extended to a prior distribution \(\mathbb{P}\) on \(\Hcal(K)\): just define \(\mathbb{P}(B) = \mathbb{P}_0(B\cap \Hcal_0(K))\) for all Borel sets \(B\) on \(\Hcal(K)\).

Lastly, since the value of \(p\) can vary, we need a way to introduce dimension information in our MCMC posterior approximation scheme. There are several alternatives in the literature, such as product space formulations \citep{carlin1995bayesian} or Bayesian averaging/model selection methods \citep{hoeting1999bayesian}, but this is precisely the problem that reversible jump samplers were designed to solve. The use of these samplers fits naturally within our framework, as they allow the complexity of the model to be directly chosen by the data, jointly inferring about the dimensionality and the value of the parameters. We explore this approach in some detail in Section~\ref{sec:rjmcmc}.

\subsection{Functional linear regression}\label{sec:rkhs-linear-model}

In the case of functional linear regression, the simplified RKHS model considered is
\begin{equation}\label{eq:rkhs-model-linear}
  Y = \alpha_0 + \dotprod{X}{\alpha}_K + \epsilon = \alpha_0 + \sum_{j=1}^p \beta_j X(t_j) + \epsilon,
\end{equation}
where \(\alpha(\cdot)=\sum_{j=1}^p\beta_j K(t_j, \cdot) \in \Hcal_{0}(K)\), \(\alpha_0\in\R\), and \(\epsilon \sim \mathcal N(0,\sigma^2)\) is an error term independent from \(X\). This model is essentially a finite-dimensional approximation from a functional perspective to the more general RKHS model that assumes \(\alpha \in \Hcal(K)\) \citep[e.g.][]{berrendero2024functional}. Since the number of components \(p\) is unknown, the full parameter space is \(\Theta = \bigcup_{p\in\N}\Theta_p\), where \(\Theta_p = \R^p \times [0, 1]^p \times \R \times \R^+\). In the sequel, a generic element of \(\Theta_p\) will be denoted by \(\theta_p = (\beta_1,\dots, \beta_p, t_1,\dots, t_p, \alpha_0, \sigma^2) \equiv (b_p, \tau_p, \alpha_0, \sigma^2)\), though we will occasionally omit the subscript when the value of \(p\) is understood. Before proceeding further, observe that we can rewrite model~\eqref{eq:rkhs-model-linear} in a more explicit and practical fashion in terms of the available sample information. For \(\theta \in \Theta\), the reinterpreted model assumes the form
\begin{equation}\label{eq:rkhs-model-linear-2}
  Y_i \mid X_i, \theta \ \stackrel{\text{ind.}}{\sim} \mathcal N\left(\alpha_0 + \sum_{j=1}^p \beta_j X_i(t_j), \ \sigma^2\right), \quad i =1,\dots, n.
\end{equation}

It is worth mentioning that the model remains linear in the sense that it fundamentally involves a random variable \(\dotprod{X}{\alpha}_K = \Psi_X(\alpha)\) belonging to the linear span of the process \(X\) in \(L^2(\Omega)\). Also, note that given the number of components \(p\) and the time instants \(t_j\), the model becomes a multiple linear model with the \(X(t_j)\) as scalar covariates. In fact, this RKHS model is particularly suited as a basis for variable selection methods \citep{berrendero2019rkhs}, and furthermore the general RKHS model entails the classical \(L^2\)-model~\eqref{eq:l2-linear-model} under certain conditions \citep[see][]{berrendero2024functional}. In addition, this model could be easily extended to the case of several covariates via an expression of type \(Y=\alpha_0 + \Psi_{X^{1}}(\alpha_1) + \cdots + \Psi_{X^{q}}(\alpha_q) + \epsilon\). Then, as argued in \citet{grollemund2019bayesian} for a similar situation, we could recover the full posterior by looking alternately at the posterior distribution of each covariate conditional on the rest of them.

\subsubsection*{Prior distributions}

A simple and intuitive prior distribution for the parameter vector \(\theta \in \Theta\), suggested by the structure of the parameter space and usually employed in the literature, is given by
\begin{equation}\label{eq:prior-linear}
\begin{aligned}
    p & \sim \pi(p),\\
    t_j  & \stackrel{\text{ind.}}{\sim}\mathcal U[0, 1],\quad & j &= 1,\dots,p,\\
    \beta_j  & \stackrel{\text{ind.}}{\sim}\mathcal N(0, \eta^2),\quad & j &= 1,\dots,p,\\
    \pi(\alpha_0, \sigma^2) & \propto 1/\sigma^2,
\end{aligned}
\end{equation}
where  \(\pi(p)\) is any discrete distribution on \(\N\) (e.g.~uniform on a given finite set) and \(\eta^2 \in \R^+\) is a hyperparameter of the model that depends strongly on the expected scale of the data. On the one hand, note the use of a joint prior distribution on \(\alpha_0\) and \(\sigma^2\), which is a widely used non-informative prior known as Jeffrey's prior \citep{jeffreys1946invariant}. One should be wary of the Jeffreys-Lindley paradox when assigning improper priors \citep[see][]{robert2014jeffreys}, but since all the parameters involved are common to all possible values of \(p\), this formulation should not present difficulties. On the other hand, the prior on the coefficients \(\beta_j\) is deliberately kept simple and somewhat non-informative, as it will greatly speed up and simplify the computations of the RJMCMC sampler later on.

\subsection{Functional logistic regression}\label{sec:rkhs-logistic-model}

In the case of functional logistic regression, we regard the binary response variable \(Y\in\{0, 1\}\) as a Bernoulli random variable given the regressor \(X=x \in L^2[0, 1]\), and as usual suppose that \(\log\left(p(x)/(1-p(x))\right)\) is linear in \(x\), where \(p(x)=\mathbb P(Y=1| X=x)\). Then, following the approach suggested by \citet{berrendero2023functional}, a simplified logistic RKHS model is given, in terms of the correspondence \(\dotprod{X}{\alpha}_K = \Psi_X(\alpha)\), by the equation
\begin{equation}\label{eq:rkhs-model-logistic}
  \mathbb P(Y=1 \mid X) = \frac{1}{1 + \exp\{-\alpha_0 - \dotprod{X}{\alpha}_K\}}, \quad \alpha_0 \in \R, \ \alpha \in \Hcal_{0}(K).
\end{equation}

Indeed, note that this can be seen as a finite-dimensional approximation (with a functional interpretation) to the general RKHS functional logistic model proposed by these authors, which can be obtained by replacing \(\Hcal_{0}(K)\) with \(\Hcal(K)\). Now, if we aim at a classification problem, our strategy will be similar to that followed in the functional linear model: after incorporating the sample information, we can rewrite~\eqref{eq:rkhs-model-logistic} as
\begin{equation}\label{eq:rkhs-model-logistic-2}
  Y_i \mid X_i,\theta \ \stackrel{\text{ind.}}{\sim} \operatorname{Bernoulli}(p_i), \quad i=1,\dots, n,
\end{equation}
with
\begin{equation*}\label{eq:rkhs-model-logistic-2-parameter}
  p_i = \mathbb P(Y_i=1 \mid X_i,\theta) = \frac{1}{\displaystyle 1 + \exp\left\{-\alpha_0 - \sum_{j=1}^p \beta_j X_i(t_j)\right\}}, \quad i=1,\dots, n,
\end{equation*}
where in turn \(\alpha_0,\beta_j\in\R\) and \(t_j\in[0, 1]\).

In much the same way as the linear regression model described above, this RKHS-based logistic regression model offers some advantages over the classical \(L^2\)-model. First, it has a more straightforward interpretation and allows for a workable Bayesian approach. Secondly, it can be shown that under mild conditions the general RKHS logistic functional model holds whenever the conditional distributions \(X | Y=i\) (\(i=0,1\)) are homoscedastic Gaussian processes, and in some cases it also entails the \(L^2\)-model~\citep[see][]{berrendero2023functional}; this arguably provides a solid theoretical motivation for the reduced model. Incidentally, these models also shed light on the near-perfect classification phenomenon for functional data, described by \citet{delaigle2012achieving} and further examined for example in the works of \citet{berrendero2018use} or \citet{torrecilla2020optimal}.

Furthermore, a maximum likelihood approach for parameter estimation (although not considered here) is possible as well, since the finite-dimensional approximation  mitigates the problem of non-existence of the MLE in the functional case. However, let us recall that even in finite-dimensional settings there are cases of quasi-complete separation in which the MLE does not exist \citep{albert1984existence}. Additionally, the non-existence issue of the MLE becomes more pronounced as the dimension increases, as exemplified in the theory recently developed by \citet{candes2020phase}. In any event, we argue that the simplified RKHS model presented here is a compelling and feasible approach to functional logistic regression, since it bypasses the main difficulties of the usual maximum likelihood techniques.

\subsubsection*{Prior distributions}

As far as prior distributions go, we proceed as we did in the linear model. However, following the advice in \citet{gelman2008weakly} and \citet{ghosh2018use} we do change the prior of the coefficients \(\beta_j\) and the constant \(\alpha_0\), since their interpretation is different now:
\begin{equation}\label{eq:prior-logistic}
  \begin{aligned}
      p & \sim \pi(p),\\
      t_j & \stackrel{\text{ind.}}{\sim}\mathcal U[0, 1],\quad & j &= 1,\dots,p,\\
      \beta_j & \stackrel{\text{ind.}}{\sim} t_5(0, 2.5),\quad & j &= 1,\dots,p,\\
      \alpha_0 & \sim \text{Cauchy}(0, 10).
  \end{aligned}
  \end{equation}
The scaled Student's \(t\) and Cauchy distributions represent a robust and weakly informative prior that provides shrinkage and can handle the case of separation in logistic regression. The specific values of the parameters have been chosen via experimentation following the initial recommendations in \citet{ghosh2018use}, which also establishes the need to scale the regressors to have mean 0 and standard deviation 0.5. Lastly, note that in this case the nuisance parameter \(\sigma^2\) does not appear.

\subsection{Reversible jump samplers for prediction}\label{sec:rjmcmc}

For the inference step in our Bayesian procedure, the usual approach would be to consider a fixed number of components, chosen separately via some model selection criterion. Instead, we aim to construct a fully Bayesian methodology that allows us to model the number of components and the component parameters jointly. Since we do not impose conjugate priors and the posterior distribution does not have a recognizable shape, a simple way to achieve the desired outcome is to use reversible jump MCMC (RJMCMC) techniques to approximate the posterior. Originally envisioned for approximate inference in Bayesian mixtures, they can be used to sample models with an unknown number of components, providing a certain level of flexibility and theoretically allowing the exploration of the whole parameter space \citep[see][]{richardson1997bayesian}.

The basis of the RJMCMC mechanism is a clever reformulation of the standard MCMC technique: on each iteration, apart from updating the current parameters, it tries to increase or decrease the dimension, creating new components or eliminating some already present. The acceptance fraction for these new moves is selected so that detailed balance as a whole is maintained, but this includes the possibly challenging computation of a certain Jacobian that is the key to dimension matching \citep{green1995reversible}. However, in nested models such as ours, we can simplify this expression by only allowing on each iteration either the birth of a new component or the death of an existing one (i.e.~changing the dimension by one unit at a time). If we also make the proposal distribution for the newly birthed components independent of previous values, the Jacobian term disappears, and the acceptance fraction reduces to \citep{brooks2003efficient}
\[
  \alpha_\text{nested} = \min\left(1, \frac{\mathcal{L}(\theta_{p+1})}{\mathcal{L}(\theta_p)}\frac{\pi(\theta_{p+1})}{\pi(\theta_p)}\frac{b_{p, p+1}}{d_{p+1, p}}\frac{1}{q(\theta_{+1})}\right).
\]
Here \(p\) is the current number of components, \(b_{p, p+1}\) is the probability of increasing the dimension from \(p\) to \(p+1\), \(d_{p+1, p}\) is the probability of the reverse move (death), \(q(\theta_{+1})\) is the proposal distribution for the added source, \(\pi\) is the prior probability and \(\mathcal L\) represents the likelihood. This is already a rather manageable expression that can be implemented efficiently, but we introduce another simplification: the proposal distribution is chosen to match the prior distribution, so that the corresponding terms cancel out. Nonetheless, in real-world scenarios one could consider more sophisticated proposal distributions that might better explore the parameter space; see for example \citet{davies2023transport} or \citet{korsakova2024neural} for some interesting ideas.

From a higher level perspective, the RJMCMC algorithm is an iterative procedure that produces a chain of \(M\) approximate samples \((p_m, \theta^*_{p_m})\) of the posterior distribution \(\pi(p, \theta_p| \mathcal D_n)\), each possibly of a different dimension; see Appendix~\ref{app:validation} for a visual representation. Given a previously unseen regressor \(X_{\text{test}}\), with each of these samples we can generate an individual response following our RKHS models, denoted by \(F(X_{\text{test}}, \theta^*_{p_m})\). In the linear case we sample from \(Y | X_{\text{test}}, \theta^*_{p_m}\) as in~\eqref{eq:rkhs-model-linear-2}, and in the logistic case we directly consider the probabilities \(\mathbb{P}(Y|X_{\text{test}},\theta^*_{p_m})\) in~\eqref{eq:rkhs-model-logistic-2} instead. We propose to combine these predictions in four different ways.

\subsubsection*{One-stage methods}

In these methods we essentially make use of the so-called posterior predictive distribution (PP), and they are in turn divided in two approaches.

  \paragraph*{(I) Weighted sum (W-PP).} The first idea involves utilizing all available information by summarizing and aggregating every individual RKHS response. We start by considering a point summary statistic \(g\) (e.g.~mean, median, mode) and consolidating the predictions from all sub-models, each having distinct dimension. Then, we bring together these predictions through a weighted sum, in which the weights correspond to the relative frequencies of the corresponding values of \(p\):
  \[
  \hat Y = \sum_{p} \tilde\pi(p\mid\mathcal D_n) g(\{F(X_{\text{test}}, \theta^*_{p_m})\}_{m:p_m=p}),
  \]
  where \(\tilde \pi(p|\mathcal D_n) = M^{-1}\sum_m \mathbb I(p_m=p)\) and \(\mathbb I\) is the indicator function. Note that this approach is reminiscent of the factorization of the posterior distribution as \(\pi(p, \theta_p|\mathcal D_n) = \sum_{p}\pi(p|\mathcal D_n)\pi(\theta_p|p, \mathcal D_n)\). Additionally, in the logistic case we employ the usual thresholding procedure to convert the final probability to a binary class label in \(\{0,1\}\). We shall see in Section~\ref{sec:results} that this method produces the best results in practice.

  \paragraph*{(II) Maximum a posteriori (MAP-PP).} We also consider a MAP strategy, where only the most probable sub-model is used and the rest of the samples are discarded. In this case, if \(\tilde p = \arg\max_p \tilde  \pi(p|\mathcal D_n)\), predictions are computed as
  \[
  \hat Y = g(\{F(X_{\text{test}}, \theta^*_{p_m})\}_{m:p_m=\tilde p}).
  \]
  Although this method potentially ignores many samples in the posterior approximation, this omission may help reduce the noise and prevent outliers from affecting the final prediction.

\subsubsection*{Two-stage methods}

In these methods we focus only on the marginal posterior distribution of \(\tau_p = (t_1, \dots, t_p)\), disregarding the rest of the parameters and effectively constructing a variable selection procedure. After choosing a set of time instants \(\{\hat t_j\}\) using the approximate posterior samples \(\{\tau^*_{p_m}\}\), we can reduce the original regressors to just the marginals \(\{X_i(\hat t_j)\}\) and apply any of the well-known finite-dimensional prediction algorithms suited for this situation. We have again two different strategies.

\paragraph*{(III) Weighted sum (W-VS).} We can mirror the weighted sum approach of the one-stage methods,  with prediction computed as
\[
\hat Y = \sum_{p} \tilde\pi(p\mid \mathcal D_n) H(X_{\text{test}}, \hat \tau_{p}),
\]
where \(\hat \tau_{p}=(\hat t_1, \dots ,\hat t_p)=g(\{\tau^*_{p_m}\}_{m:p_m=p})\) and \(g\) is a component-wise summary statistic. The function \(H\) represents the prediction for \(X_{\text{test}}\) of a regular linear/logistic regression algorithm, fitted with the transformed data set \(\{(X_i(\hat \tau_p), Y_i): i=1,\dots,n\}\). As before, in the logistic case the responses are thresholded to obtain a class label.

\paragraph*{(IV) Maximum a posteriori (MAP-VS).} We also consider a MAP approach to variable selection with only the information of the most probable model, i.e., 
\[
\hat Y = H(X_{\text{test}}, \hat \tau_{\tilde p}).
\]


\section{Posterior consistency}\label{sec:consistency}

This section demonstrates how the RKHS-based Bayesian methodology proposed herein establishes a robust theoretical foundation for subsequent prediction procedures. Firstly, let us briefly recall what we understand by posterior consistency. Note that to avoid confusion, throughout this section we will sometimes use bold letters to represent random variables. Consider a sample space \(\mathcal X\) and \(X_1,\dots, X_n\) an i.i.d.\ sample of the data \(X\). Let us fix a prior distribution \(\Pi\) for random variables \(\bm\theta\) on the parameter space \(\Theta\), that is, \(\bm\theta \sim \Pi\), and let \(P_\theta\) represent a sampling model (a distribution  on \(\mathcal X\) indexed by \(\theta \in \Theta\)) such that \(X | \bm \theta \sim P_{\bm \theta}\). Furthermore, assume that the model is well-specified, i.e., there is a true value \(\theta_{0}\in\Theta\) such that \(X \sim P_{\theta_0}\), and denote by \(P_0^\infty\) the joint probability measure of \((X_1, X_2, \dots)\) when \(\theta_0\) is the true value of the parameter.

\begin{definition}[\citealp{ghosh2003bayesian}]
  We say that the posterior distribution is (strongly) consistent at \(\theta_0\) if for every neighborhood \(U\) of \(\theta_0\) it holds that
  \[
    \lim_{n\to\infty} \Pi(\bm\theta \in U \mid X_1, \dots,X_n) =1 \quad P_0^\infty-\text{a.s.}
  \]
  For a metric space \((\Theta, d)\), this is equivalent to
  \[
    \lim_{n\to\infty} \Pi(d(\bm \theta, \theta_0) < \epsilon \mid X_1, \dots, X_n) = 1 \quad P_0^\infty-\text{a.s.}, \quad \text{for all } \epsilon > 0.
  \]
\end{definition}

Note that the conditional probabilities are computed under the assumed joint distribution of \((\bm \theta, (X_1, X_2,\dots))\). Essentially, we are saying that the posterior concentrates around \(\theta_0\) for almost all sequences of data. Thus, if consistency holds, the effect of the prior gets diluted as more and more data is used for the inference.

\subsubsection*{Doob's theorem}

It turns out that, under very general conditions, the posterior distribution is consistent at almost every value of \(\theta_0\) with respect to the measure induced by the prior \citep{doob1949application}. Let the sample space \(\mathcal X\) and the parameter space \(\Theta\) be complete separable metric spaces, endowed with their respective Borel sigma-algebras. Consider the model \(\bm \theta \sim \Pi\) and \(X_1,X_2, \ldots | \bm \theta \sim P_{\bm \theta}\) i.i.d., where \(\bm\theta\) is a random variable taking values in \(\Theta\). Observe that this induces a posterior distribution \(\Pi(\bm\theta | X_1,\dots, X_n)\).

\begin{theorem}[Doob's consistency theorem]
  If \(\theta \mapsto P_\theta\) is one-to-one and \(\theta \mapsto P_\theta(A)\) is measurable for all measurable sets \(A\subseteq \mathcal X\), then the posterior distribution is consistent at \(\Pi\)-almost all values of \(\Theta\). That is, there exists \(\Theta_*\subseteq \Theta\) such that \(\Pi(\Theta_*)=1\) and for all \(\theta_0\in\Theta_*\), if \(X_1,X_2,\ldots \sim P_{\theta_0}\) i.i.d., then for any neighborhood \(B\) of \(\theta_0\) we have
  \[
    \lim_{n\to\infty} \Pi(\bm \theta \in B \mid X_1,\dots, X_n) = 1 \quad P_{0}^\infty-\text{a.s.}
  \]
\end{theorem}

See chapter 1.3 of \citet{ghosh2003bayesian} for more details on this result. As a side note, in a nonparametric setting where the object of interest is a random function (e.g.\ a probability density), there is also a stronger consistency result by \citet{schwartz1965bayes} which omits the \(\Pi\)-almost sure qualification under some more restrictive conditions. Moreover, there are some extensions of this result that deal with independent but not identically distributed data \citep{choi2008remarks}.

\subsection{Consistency in our RKHS model}

Now we study in detail how Doob's theorem can be applied in our linear RKHS model, but the posterior consistency results we obtain hold \textit{mutatis mutandis} in the logistic case. In this section we will assume that the covariance function \(K\) of the underlying stochastic process \(X\) is strictly positive definite, which is not a very restrictive condition in practice. In the linear case the sample space is \(\mathcal X \times \mathcal Y = L^2[0,1]\times \R\), which is already a complete separable metric space. Next, consider for each \(p\in\N\) the subset of the \((2p+2)\)-Euclidean space \(\Theta_p = \{(b, \tau, \alpha_0, \sigma^2): b=(\beta_1,\dots,\beta_p) \in \R^p, \tau=(t_1,\dots,t_p) \in [0,1]^p, \alpha_0\in \R, \sigma^2 \in \R^+_0\}\). Then, we can write our infinite-dimensional parameter space as
\begin{equation}\label{eq:parameter-space-union}
  \Theta = \bigcup_{p=1}^\infty \Theta_p.
\end{equation}

At this point we can follow an approach very similar to the one in \citet{miller2023consistency}, where posterior consistency is established in a mixture model with an unknown number of components, which has an infinite-dimensional parameter space that factorizes in the same way as~\eqref{eq:parameter-space-union}. Note that given \(\theta \in \Theta\) there is a unique \(p=p(\theta)\) such that \(\theta \in \Theta_p\). Considering that we will only be interested in small balls around the true value of the parameter, we can define a metric for \(\theta, \theta' \in \Theta\) by
\begin{equation}\label{eq:metric-doob}
  d(\theta, \theta')= \begin{cases}
    \min \left\{\|\theta - \theta'\|, 1\right\}, \quad & \text{if } p(\theta)=p(\theta'), \\
    1, \quad                                           & \text{otherwise}.
  \end{cases}
\end{equation}
Since each \(\Theta_p\) is itself a complete separable metric space with the inherited Euclidean norm, Proposition A.1 in \citet{miller2023consistency} ensures that \((\Theta, d)\) is a complete separable metric space. Further, we equip both \(\mathcal X \times \mathcal Y\) and \(\Theta\) with their respective Borel sigma-algebras. 

In terms of \(\theta\in\Theta\), the data distribution is \(P_\theta(X,Y)=P_{b, \tau, \alpha_0, \sigma^2}(X,Y)\), and formally the joint distribution \(P_{\theta}(X,Y)\) factorizes as \(P_{\theta}(X,Y)=P(X)P_\theta(Y|X)\). Here \(P(X)\) is the distribution of the underlying process \(X\), and in our RKHS setting, \(P_\theta(Y|X) \equiv \mathcal N(\alpha_0 + \sum_{j=1}^{p(\theta)}\beta_j X(t_j),\, \sigma^2)\). Now, let us suppose that \(\theta\mapsto P_\theta(X, Y)(A)\) is measurable for all measurable sets \(A\subseteq \mathcal X\times \mathcal Y\), which is true under mild conditions (see Appendix~\ref{app:theory}). Moreover, for convenience, we will denote the sequences \((X,Y)_{1:n} = (X_1,Y_1), \dots, (X_n, Y_n)\) and \((X,Y)_{1:\infty} = (X_1, Y_1), (X_2, Y_2), \dots\).

Now, the full hierarchical model under consideration is
\begin{equation}\label{eq:model-linear}
  \begin{aligned}
    \text{(no.\ of components)}\quad & \mathcal P \sim \pi,\\
    \text{(component values)}\quad   & \bm b \mid \mathcal P=p \sim F_p,\\
    \text{(component times)}\quad    & \bm \tau \mid \mathcal P=p \sim G_p,\\
    \text{(intercept)}\quad          & \bm{\alpha_0} \sim C,\\
    \text{(error variance)}\quad     & \bm{\sigma^2} \sim D,\\
    \text{(observed data)}\quad      & (X,Y)_{1:n} \mid \bm{b, \tau, \alpha_0, \sigma^2} \sim P_{\bm{b, \tau, \alpha_0, \sigma^2}}(X,Y) \quad \text{i.i.d.},
  \end{aligned}
\end{equation}
where \(\pi\), \(F_p\), \(G_p\), \(C\) and \(D\) are probability measures on \(\N\), \(\R^p\), \([0,1]^p\), \(\R\) and \(\R^+_0\), respectively. Note that since \(P_\theta(X,Y)\) is invariant under permutations of the component labels \(b\) and \(\tau\), we can only show consistency up to one such permutation. To that effect, and mirroring the strategy in \citet{miller2023consistency}, let \(S_p\) denote the set of permutations of \(\{1,\dots,p\}\), and for \(\nu\in S_p\) and \(\theta \in \Theta_p\), denote by \(\theta[\nu]\) the result of applying the permutation \(\nu\) to the component labels of \(\theta\). That is, if \(\theta=(\beta_1,\dots,\beta_p, t_1,\dots, t_p,\alpha_0,\sigma^2)\), then \(\theta[\nu]=(\beta_{\nu_1},\dots,\beta_{\nu_p}, t_{\nu_1},\dots, t_{\nu_p},\alpha_0,\sigma^2)\). Next, define \(\tilde{B}(\theta_0, \epsilon)  =\bigcup_{\nu\in S_p} \{\theta \in \Theta: d(\theta, \theta_0[\nu]) < \epsilon \}\)  for \(\theta_0\in\Theta_p\) and \(\epsilon>0\), which is the set of all parameters that are within \(\epsilon\) of some permutation of (the component labels of) \(\theta_0\). Lastly, define the random variable \(\bm \theta = (\bm{b, \tau, \alpha_0, \sigma^2} )\), which takes values in \(\Theta\), and denote by \(\Pi\) the prior distribution on \(\bm\theta\) implied by the model in~\eqref{eq:model-linear}. In order for identifiability to hold, we need to place some restrictions on the prior:

\begin{condition}[Identifiability constraints] Under the model in~\eqref{eq:model-linear}, for all \(p\in\N\):\label{cond:condition-ident}
  \begin{enumerate}[label=(\roman*)]
    \item \(\Pi(t_i=t_j|\mathcal P=p)=0\) for all \(1\leq i < j \leq p\).\label{cond:condition-ident-1}
    \item There exists \(\delta>0\) such that \(\Pi(|\beta_j|<\delta|\mathcal P=p)=0\) for all \(1\leq j \leq p\).\label{cond:condition-ident-2}
  \end{enumerate}
\end{condition}

Both assumptions can be interpreted as a way of pursuing parsimony in the model, aiming for as few components as possible. In practical and computational terms, we can think of \(\delta\) as the \textit{machine precision number}, so that virtually all continuous prior distributions satisfy the associated condition when implemented numerically in a computer. With this setup in mind, we are now ready to state our main consistency result:

\begin{theorem}\label{th:consistency-theorem}
  Suppose that Condition~\ref{cond:condition-ident} holds. Then there exists \(\Theta_*\subseteq \Theta\) such that \(\Pi(\bm\theta \in \Theta_*)=1\) and for all \(\theta_0\in\Theta_*\), if \((X,Y)_{1:\infty} \sim P_{\theta_0}(X,Y)\) i.i.d., then for all \(\epsilon > 0\)
  \[
    \lim_{n\to\infty} \Pi(\bm \theta \in \tilde{B}(\theta_0,\epsilon) \mid (X,Y)_{1:n}) = 1 \quad P_{0}^\infty(X,Y)-\text{a.s.}
  \]
  and
  \[
    \lim_{n\to\infty} \Pi(\mathcal P=p(\theta_0) \mid (X,Y)_{1:n}) = 1 \quad P_{0}^\infty(X,Y)-\text{a.s.}
  \]
\end{theorem}

The second conclusion is of certain relevance in itself, because the estimation of the number of components in mixture-like models is a hard problem in general \citep[see][and references therein]{miller2018mixture}. Moreover, it is worth pointing out that the proof of Theorem~\ref{th:consistency-theorem} can be easily adjusted to guarantee consistency when the number of components is fixed beforehand (and thus the parameter space is finite-dimensional). Indeed, in this case Doob's theorem applies directly under the sole condition that the times be distinct with prior probability one. The coefficients \(\beta_j\) do not cause a problem for identifiability now, since the dimension of every parameter is the same.

\begin{corollary}
  Assume model~\eqref{eq:model-linear} in which the value of \(p\) is fixed, and hence \(\Theta_p\) is the (finite-dimensional) parameter space. Suppose that Condition~\ref{cond:condition-ident}-\ref{cond:condition-ident-1} holds as well. Then, if \(\theta_0\) is the true value of the parameter, the posterior is consistent at \(\theta_0\) with \(\Pi\)-probability one.
\end{corollary}

Note that by allowing \(\beta_j\) to be zero in this situation we can sometimes circumvent the fact that the true value of the parameter might not have exactly \(p\) components, as long as \(p\) is larger than the true value \(p(\theta_0)\). Indeed, if \(\theta_0\in\Theta\) with \(p(\theta_0) < p\) and \((X,Y)_{1:\infty} \sim P_{\theta_0}(X,Y)\) i.i.d., then we can find \(\theta_1\in\Theta_p\), which is just \(\theta_0\) completed with zeros, such that \(P_{\theta_0}\)(X, Y) = \(P_{\theta_1}(X,Y)\) and the result holds almost surely.

\subsubsection*{A remark on Lebesgue consistency}

All in all, Theorem~\ref{th:consistency-theorem} guarantees consistency for \(\Pi\)-almost every parameter in the support of the prior distribution. However, even though we can choose the prior so that \(\supp( \Pi) = \Theta\), in principle there is no assurance that the \(\Pi\)-null set in which consistency may fail will not be a large set with respect to other (e.g.\ Lebesgue) measures. In fact, when the parameter space is infinite-dimensional there are examples of big inconsistency sets, even for reasonably chosen prior distributions~\citep{diaconis1986consistency}. Nonetheless, this problem can be alleviated when the parameter space is a countable union of disjoint finite-dimensional sets, as we can further refine our almost sure statement. First, note that there is a natural extension of the Lebesgue measure to our parameter space \(\Theta\): just consider the genuine Lebesgue measure \(\lambda_p\) on \(\Theta_p\), and for all \(B\subseteq \Theta\) measurable define \(\lambda(B) = \sum_{p=1}^\infty \lambda_p(\Theta_p \cap B)\). Then, if we choose a prior distribution with respect to which this measure is absolutely continuous, the consistency set \(\Theta_*\) in Theorem~\ref{th:consistency-theorem} will satisfy \(\lambda(\Theta \setminus \Theta_*)=0\). A similar approach is considered in \citet{nobile1994bayesian} and \citet{miller2023consistency} to establish ``Lebesgue''-almost sure consistency in finite mixture models with a prior on the number of components. In our case, the requirement of absolute continuity can be relaxed so that sets with nonzero Lebesgue measure have nonzero prior probability \textit{for some permutation of the component labels}. We give more details on this result in Appendix~\ref{app:theory}.

\subsection{A sketch of the proof}

 We now present a proof of our consistency result. The general strategy will be to apply Doob's theorem to a subset of the parameter space \(\Theta\), where permutations are suitably taken into account and full identifiability holds, and then extend the conclusions to the whole parameter space. As we will see shortly, save for an eventual permutation, identifiability of the map \(\theta \mapsto P_\theta(X,Y)\) is obtained in the RKHS case when the covariance function \(K\) of the underlying stochastic process is non-degenerate.

\textit{Reduced parameter space}. Consider the space \(\R_\delta = (-\infty, -\delta]\cup[\delta, +\infty)\), with \(\delta\) the fixed tolerance given in Condition~\ref{cond:condition-ident}-\ref{cond:condition-ident-2}, and define 
\[\tTheta_p = \R^p_\delta \times [0,1]^p_{\text{ord}} \times \R \times \R^+_0,
\]
where \(
[0,1]^p_{\text{ord}} =\{(t_1,\dots, t_p) \in [0,1]^p: t_1 < \cdots < t_p\}
\). Now consider \(\tTheta = \bigcup_{p\in \N}\tTheta_p\), and note that the sets \(\tTheta_1,\tTheta_2,\ldots\) are still disjoint. Then, again by Proposition A.1 in \citet{miller2023consistency}, we can conclude that \(\tTheta\) is a complete separable metric space under the metric \(d\) in~\eqref{eq:metric-doob}. We will henceforth say that a parameter in \(\tTheta\) is ``ordered''.

\textit{Transformation into ordered form}. For \(\theta\in\Theta_p\), define \(T(\theta)=\theta[\nu]\) if there is a \(\nu\in S_p\) such that \(\theta[\nu]\in \tTheta_p\); otherwise set \(T(\theta)=\theta\). Note that we can only transform \(\theta\) to be in \(\tTheta_p\) if the times \(t_j\) are all distinct and the coefficients \(\beta_j\) satisfy \(|\beta_j|\geq\delta\). But since the prior distribution assigns probability one to both events by Condition~\ref{cond:condition-ident}, we have \(\Pi(T(\bm\theta)\in\tTheta)=1\). It will be useful later to observe that for any set \(B\subseteq \tTheta_p\), if we denote \(B[\nu] = \{\theta[\nu]: \theta \in B\}\), we have
\begin{equation}\label{eq:inverse-image-T}
  \bigcup_{\nu\in S_p} B[\nu] = T^{-1}(B).
\end{equation}

\textit{Collapsed model}. Note that \(P_{T(\theta)}(X,Y)=P_{\theta}(X,Y)\), and let \(\tilde Q\) denote the distribution of \(T(\bm \theta)\) restricted to \(\tTheta\). Then the following model holds on the reduced space \(\tTheta\):
\begin{equation}\label{eq:model-linear-collapsed}
  \begin{aligned}
     & T(\bm \theta) \sim \tilde{Q},                                               \\
     & (X,Y)_{1:n}\mid T(\bm\theta) \sim P_{T(\bm\theta)}(X,Y) \quad \text{i.i.d.}
  \end{aligned}
\end{equation}

\textit{Verifying conditions}. We will now show that the conditions of Doob's theorem hold on \(\tTheta\). First, since \(\theta \mapsto P_\theta(X, Y)(A)\) is measurable on \(\Theta\), it is also measurable on \(\tTheta\) for all sets \(A\subseteq \mathcal X \times \mathcal Y\) measurable. For the identifiability part, suppose by contradiction that there are two parameters \(\theta,\theta' \in \tTheta\) such that \(\theta\neq \theta'\) and \(P_{\theta}(X, Y) = P_{\theta'}(X, Y)\). Then necessarily \(P_{\theta}(Y|X) = P_{\theta'}(Y|X)\), which in turn implies that the means and variances of these distributions are equal, i.e., \(\sigma^2=(\sigma^2)'\) and
\[
\alpha_0 + \sum_{j=1}^{p(\theta)} \beta_j X(t_j) = \alpha_0' + \sum_{j=1}^{p(\theta')} \beta'_j X(t'_j),
\] 
where \(t_i\neq t_j\) and \(t_i'\neq t_j'\) for \(i\neq j\). Reordering the terms and combining those where the impact points coincide, this means that a linear combination of marginals of the process \(X\) equals a constant. By taking variances, we can see that all the coefficients in this linear combination must vanish, since the covariance function of the process is strictly positive definite. But \(\beta_j\) and \(\beta_j'\) cannot be \(0\) for any \(j\) (by definition of \(\tTheta\)), so it must be the case that \(\theta'=\theta[\nu]\) for some \(\nu \in S_p\), where \(p=p(\theta)=p(\theta')\). However, \(t_1< \cdots < t_p\) and \(t'_1 < \cdots < t'_p\), so  \(\nu\) must be the identity permutation, that is, \(\theta'=\theta\), contradicting the initial assumption.

\textit{Applying Doob's theorem}. Next we analyze the conclusions of Doob's theorem applied to the collapsed model~\eqref{eq:model-linear-collapsed}: there exists \(\tTheta_* \subseteq \tTheta\) with \(\Pi(T(\bm\theta)\in\tTheta_*)=1\) such that, if \(T(\theta_0)\in \tTheta_*\) and it holds that \((X,Y)_{1:\infty} \sim P_{T(\theta_0)}(X, Y)\) i.i.d., then for any neighborhood \(B\subseteq \tTheta\) of \(T(\theta_0)\) we have
\begin{equation}\label{eq:consistency-collapsed}
  \Pi(T(\bm\theta) \in B \mid (X,Y)_{1:n}) \xrightarrow[]{n\to\infty} 1 \quad P_{T(\theta_0)}^\infty-\text{a.s.}
\end{equation}
Now define \(\Theta_*\) to be the set of all parameters in \(\Theta\) that can be obtained by permuting a parameter in \(\tTheta_*\), i.e., \(\Theta_* = \bigcup_{p=1}^\infty \bigcup_{\nu\in S_p}(\tTheta_* \cap \tTheta_p)[\nu]\). Then, by~\eqref{eq:inverse-image-T} we get \[
  \Pi(\bm\theta\in \Theta_*) = \Pi\left(T(\bm\theta) \in \bigcup_{p=1}^\infty (\tTheta_*\cap \tTheta_p)\right) = \Pi(T(\bm\theta)\in \tTheta_*) = 1.\]

\textit{Extending the result to \(\Theta\)}. Let \(\theta_0\in\Theta_*\), suppose that \((X,Y)_{1:\infty}\sim P_{\theta_0}(X, Y)\) i.i.d., and define \(p_0=p(\theta_0)\) and \(S_0 = S_{p_0}\). Fix \(\epsilon\in(0,1)\) and consider the set \(B\) of all ordered parameters that are within \(\epsilon\) of the ordered version of \(\theta_0\), i.e.,
\begin{equation}\label{eq:set-B}
  B = \left\{ \theta \in \tTheta: d(\theta, T(\theta_0)) < \epsilon \right\}.
\end{equation}
Observe that, since \(\epsilon < 1\), we have \(B\subseteq \tTheta_{p_0}\) by definition of \(d\). Moreover, \(\bigcup_{\nu\in S_{0}}B[\nu]\subseteq \tilde{B}(\theta_0, \epsilon)\). Then, again by~\eqref{eq:inverse-image-T}, we can write
\begin{equation}\label{eq:consistency-intermediate}
  \begin{aligned}
    \Pi(\bm\theta \in \tilde{B}(\theta_0, \epsilon) \mid (X,Y)_{1:n}) & \geq \Pi(\bm\theta \in \bigcup_{\nu\in S_{0}} B[\nu] \mid (X,Y)_{1:n}) \\
                                                                      & = \Pi(T(\bm\theta) \in B \mid (X,Y)_{1:n}).
  \end{aligned}
\end{equation}
Now, \(T(\theta_0)\in{\tTheta_*}\) because \(\theta_0\in\Theta_*\), and in that case we know that the collapsed model is consistent at \(T(\theta_0)\). Note that we also have \((X,Y)_{1:\infty} \sim P_{T(\theta_0)}(X, Y)\) i.i.d.\ (since \(P_{\theta_0}=P_{T(\theta_0)}\)), and the set \(B\) in~\eqref{eq:set-B} is a neighborhood of \(T(\theta_0)\) in \(\tTheta\). Then, by~\eqref{eq:consistency-collapsed}, we have \(\Pi(T(\bm\theta) \in B | (X,Y)_{1:n}) \xrightarrow[]{n\to\infty} 1, \ P_{\theta_0}^\infty (X,Y)-\text{a.s.}\), and this fact together with~\eqref{eq:consistency-intermediate} proves consistency for \(\theta_0\) in the original model~\eqref{eq:model-linear}. Lastly, since \(\epsilon < 1\) implies \(\tilde{B}(\theta_0,\epsilon) \subseteq \Theta_{p_0}\), we have also proved the second assertion of our theorem:
\begin{align*}
  \Pi(\mathcal P=p_0\mid (X,Y)_{1:n}) & = \Pi(\bm\theta \in \Theta_{p_0}\mid (X, Y)_{1:n})\\
                                      & \geq \Pi(\bm\theta \in\tilde{B}(\theta_0,\epsilon) \mid (X,Y)_{1:n}) \\
                                      & \xrightarrow[n\to\infty]{} 1 \quad P_{\theta_0}^\infty (X,Y)-\text{a.s.}\tag*{\(\square\)} % qed symbol
\end{align*}

As a final comment, it is worth reiterating that in this theoretical aspect of our work we have closely followed the techniques recently developed in \citet{miller2023consistency}, where the author provides a simplification of the work by \citet{nobile1994bayesian} in studying posterior consistency in finite-dimensional mixture models with a prior on the number of components. While the methods are quite similar, we have succeeded in extending this theory to a fundamentally different situation, namely functional regression, which thanks to the RKHS formulation shares the key properties that allow for a treatment analogous to the finite-dimensional mixture case.


\section{Experimental results}\label{sec:results}

In this section we present the results of the experiments carried out to test the performance of our Bayesian methods in different scenarios, together with an overview of their computational implementation. Further details such as simulation parameters or algorithmic decisions, as well as additional experiments, figures and tables are provided in Appendices~\ref{app:model-choice},~\ref{app:experiments} and~\ref{app:source-code}, while the code itself is publicly available on GitHub at \url{https://github.com/antcc/rk-bfr-jump}. 

For simulated data we consider \(n=200\) training samples and \(n'=100\) testing samples on an equispaced grid of \(100\) points on \([0, 1]\), and for the real data sets we perform a 66\%/33\% train/test split. We fit our models on the training data, using RJMCMC to approximate the posterior, and then use the test set for out-of-sample predictions. We independently repeat the whole process 10 times (each with a different train/test configuration) to account for the stochasticity in the sampling step, and average the results across these executions. For the purposes of prediction we consider the point statistics \textit{trimmed mean} (10\%), \textit{median} and \textit{mode} to aggregate together predictions and summarize parameters (see Section~\ref{sec:rjmcmc}). Moreover, the values of \(\{t_j\}\) that fall outside the specified grid are truncated to the nearest neighbor in the grid, with the additional restriction that time instants in different components of our models cannot be equal.

The Python library used to perform the RJMCMC approximation is \textit{Eryn} \citep{karnesis2023eryn}. It is a toolbox for Bayesian inference that allows trans-dimensional posterior approximation, running multiple chains in an ensemble configuration with different starting points, and incorporating a parallel tempering mechanism \citep{hukushima1996exchange} to increase both convergence speed and the acceptance rate.  Because of execution time constraints, the hyperparameters of the Eryn sampler are selected manually based on an initial set of experiments, as well as recommendations from the authors.  Moreover, some amount of post-processing is needed to mitigate the well-known \textit{label switching} phenomenon that occurs in MCMC approximations of mixture-like models \citep{stephens2000dealing}; see Appendix~\ref{app:label-switching}.

\subsubsection*{Data sets}

We consider a set of functional regressors common to linear and logistic regression problems. They are four zero-mean Gaussian processes (GPs), each with a different covariance function. In particular, we consider a Brownian motion, a fractional Brownian motion, an Ornstein-Uhlenbeck process, and a GP with a squared exponential kernel.

\paragraph*{Linear regression data sets.} We employ two different types of simulated data sets, all with a common value of \(\alpha_0=5\) and \(\sigma^2=0.5\):
\begin{itemize}
  \item A finite-dimensional RKHS response with three components for each of the four GP regressors mentioned above: \(Y=5 -5X(0.1) + 5X(0.6) + 10X(0.8) + \epsilon\).
  \item A ``component-less'' response generated by an \(L^2\)-model with a smooth underlying coefficient function, namely \(\beta(t)=\log(1+4t)\), again for the same four GPs.
\end{itemize}
As for the real data sets, we use the Tecator data set \citep{borggaard1992optimal} to predict fat content based on near-infrared absorbance curves of 193 meat samples, as well as what we call the Moisture \citep{kalivas1997two} and Sugar \citep{bro1999exploratory} data sets. The first consists of near-infrared spectra of 100 wheat samples and the objective is to predict the samples' moisture content, whereas the second contains 268 samples of sugar fluorescence data in order to predict ash content. The regressors of the three data sets are measured on a grid of 100, 101 and 115 equispaced points on \([0, 1]\), respectively.

\paragraph*{Logistic regression data sets.} Again we consider two different types of simulated data sets,  with a common value of \(\alpha_0=-0.5\):
\begin{itemize}
  \item Four logistic finite-dimensional RKHS responses with the same functional parameter as in the linear regression case (one for each GP). Specifically,
        \[
          \mathbb P(Y=1\mid X) = \frac{1}{1 + \exp\left\{0.5 +5X(0.1) - 5X(0.6) - 10X(0.8)\right\}}.
        \]
  \item Four logistic responses following an \(L^2\)-model with the same coefficient function as in the linear regression case, i.e., \(\beta(t)=\log(1+4t)\).
\end{itemize}
Additionally, we use three real data sets well known in the literature. The first one is a subset of the Medflies data set \citep{carey1998relationship}, consisting on samples of the number of eggs laid daily by 534 flies over 30 days, to predict whether their longevity is high or low. The second one is the Berkeley Growth Study data set \citep{tuddenham1954physical}, which records the height of 54 girls and 39 boys over 31 different points in their lives. Finally, we selected a subset of the Phoneme data set \citep{hastie1995penalized}, based on 200 digitized speech frames over 128 equispaced points to predict the phonemes ``aa'' and ``ao''.

\subsubsection*{Comparison algorithms}

We have included a fairly comprehensive suite of comparison algorithms, chosen among the most common frequentist methods used in machine learning and FDA, and following a standard choice of implementation and hyperparameters. There are purely functional methods (such as the usual \(L^2\) regression based on model~\eqref{eq:l2-linear-model}), finite-dimensional models that work on the discretized data (e.g.\ penalized finite-dimensional regression), and variable selection/dimension reduction procedures (like Principal Component Analysis or Partial Least Squares). The main parameters of all these algorithms are selected by cross-validation, and when a number of components needs to be specified, we use the same range as in our own models so that comparisons are fair. A more detailed account of these algorithms is available in Appendix~\ref{app:data-sets}.

\subsubsection*{Results display}

We have adopted a visual approach to presenting the experimentation results, using colored graphs to help visualize them. In each case, the mean and standard deviation of the score obtained across the 10 random runs is shown, depicting our methods in blue and the comparison algorithms in orange. We also show the global mean of all the comparison algorithms with a dashed vertical line, excluding extreme negative results to avoid distortion. Moreover, we separate one-stage and two-stage methods, the latter being the ones that perform variable selection or dimension reduction prior to a multiple linear/logistic regression method (represented with ``+r''/``+log'' in the figures). We name our methods according to the acronyms described in Section~\ref{sec:rjmcmc}. 

\subsection{Functional linear regression}\label{sec:experiments-linear}

The initial experiments carried out indicate that low values of \(p\) provide sufficient flexibility in most scenarios, so we allow the number of components to vary in the set \(\{1,2,\dots,10\}\). Following \citet{nobile2007bayesian} we select a truncated Poisson prior with a low rate parameter \(\lambda=3\) for \(p\), so that simpler models are favored. However, in the experiments with real data we set \(p\sim \mathcal U\{1,2,\dots,10\}\) to allow a less informative exploration of the parameter space. Moreover, for simplicity we choose to scale the regressors and response to have standard deviation unity in the inference step, but then convert the results back to the original scale for prediction. This allows us to set a reasonable value of \(\eta^2=25\) in the weakly informative prior of \(\beta_j\) (see~\eqref{eq:prior-linear}). Lastly, the metric used to evaluate the performance is the Root Mean Square Error (RMSE).

\subsubsection*{Simulated data sets}

In Figure~\ref{fig:reg_rkhs} we see the results for the RKHS response. This is our baseline and the most favorable case for us, as the underlying model coincides with our assumed model. Indeed, we can see that in most instances our algorithms are the ones with lower RMSE, as expected, and there is not much variance among our different approaches to prediction, at least in the one-stage methods. We even manage to consistently beat the \textit{lasso} finite-dimensional regression mechanism, which can select all 100 components for its model, versus our 10 components at most. This is an indicator that our models make a more efficient selection of variables, encapsulating more information with fewer components and justifying our partiality to parsimonious models.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=.78\textwidth]{reg_rkhs}
  \caption{Mean and standard error of RMSE of predictors (lower is better) for 10 runs with GP regressors, one on each column, that obey an underlying linear RKHS model.}\label{fig:reg_rkhs}
\end{figure}

Figure~\ref{fig:reg_l2} shows the results for an underlying \(L^2\)-model, which would be our direct competitor and a more representative test for our Bayesian model. In this case the outcome is satisfactory, as for the most part our models are on a par with the rest, even defeating other methods that were designed with the \(L^2\)-model in mind. Especially interesting is the comparison with the standard \(L^2\) functional regression (\textit{flin} in the graphics), which we outperform most of the time.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=.78\textwidth]{reg_l2}
  \caption{Mean and standard error of RMSE of predictors (lower is better) for 10 runs with GP regressors, one on each column, that obey an underlying linear \(L^2\)-model.}\label{fig:reg_l2}
\end{figure}

\subsubsection*{Real data}

Figure~\ref{fig:reg_real} depicts the results for the real data sets, where we can see that the performance of our one-stage methods is about the same as that of the comparison methods. However, our variable selection methods are somewhat worse that the reference methods, although not by a wide margin. We have to bear in mind that real data is more complex and noisy than simulated data, and it is possible that after a suitable pre-preprocessing we would have obtained better results. Nonetheless, our goal was to perform a general comparison with a uniform methodology, without focusing too much on the specifics of any particular data set. 

On another note, we see that some of our Bayesian models have a higher standard deviation, partly because there is an intrinsic randomness in the sampling mechanism, and it can be the cause of the occasional worse performance. In relation to this, we observe that the methods that use the trimmed mean as a summary statistic tend to have a worse score, as this statistic is more sensitive to outliers than the median or the mode.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=.64\textwidth]{reg_real}
  \caption{Mean and standard error of RMSE of predictors (lower is better) for 10 runs with real data sets, one on each column.}\label{fig:reg_real}
\end{figure}

\subsection{Functional logistic regression}\label{sec:experiments-logistic}

In this case we set the same prior distribution for \(p\) as in the linear case, but now in the fitting phase the regressors are scaled to have standard deviation 0.5 to accommodate the priors in~\eqref{eq:prior-logistic}. We use the standard threshold of 0.5 to convert probabilities to class labels, and the performance metric is the accuracy, i.e., the rate of correctly predicted samples. 

\subsubsection*{Simulated data sets}

First, in Figure~\ref{fig:clf_rkhs} we see the results for the GP regressors with a logistic RKHS response. Our models perform fairly well in this advantageous situation, and even more so in the one-stage case. We again improve the results of other models that can select far more components than our self-imposed limit of 10. Moreover, whenever our two-stage methods score lower, the differences account for only a couple of misclassified samples at worst.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=.78\textwidth]{clf_rkhs}
  \caption{Mean and standard error of accuracy of classifiers (higher is better) for 10 runs with GP regressors, one on each column, that obey an underlying logistic RKHS model.}\label{fig:clf_rkhs}
\end{figure}

Subsequently, Figure~\ref{fig:clf_l2} shows that in the \(L^2\) scenario the results are again promising, since our models score consistently on or above the mean of the reference models, and in many instances surpass most of them. In this case we also beat the alternative functional logistic regression method (\textit{flog}). In addition, in this situation the overall accuracy of all methods is poor (around 60\%), so this is indeed a difficult problem in which even small increases in accuracy are relevant.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=.78\textwidth]{clf_l2}
  \caption{Mean and standard error of accuracy of classifiers (higher is better) for 10 runs with GP regressors, one on each column, that obey an underlying logistic \(L^2\)-model.}\label{fig:clf_l2}
\end{figure}

\subsubsection*{Real data}

As for the real data sets, in Figure~\ref{fig:clf_real} we see positive results in general, obtaining in some cases accuracies well above the mean of the reference models. In particular, the weighted sum methods tend to have slightly better results than the MAP methods, which is a trend that was also present in the simulated data sets. This was somewhat expected, since the weighted predictions use the full range of information available.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=.64\textwidth]{clf_real}
  \caption{Mean and standard error of accuracy of classifiers (higher is better) for 10 runs with real data sets, one on each column.}\label{fig:clf_real}
\end{figure}


\section{Conclusion}\label{sec:conclusion}

In this work we have introduced a natural and computationally feasible way of integrating Bayesian inference into functional regression models, by means of a RKHS approach that simplifies the usually hard task of setting a prior distribution on a functional space. The proposed RKHS formulation gives a common framework to all finite-dimensional models based on linear combinations of the marginals of the underlying process, establishing a solid theoretical foundation for these popular models while retaining the functional perspective. Our approximation has the advantage of working with simpler functional parameters, thus increasing the interpretability and ease of implementation, as well as allowing direct comparison of models with different dimension. In addition, this approach works especially well in the logistic case, bypassing the difficulties associated with maximum likelihood techniques and providing a tractable alternative to the more studied methods in the literature.

We have also proved a posterior consistency result that ensures the coherence and correctness of the Bayesian methods we developed. These kinds of results have in other contexts more intricate and restrictive conditions to arrive at essentially the same conclusions as we did, but again the introduction of RKHS's is the key point to greatly simplifying them. It is worth emphasizing that the methodology introduced in \citet{miller2023consistency}, initially intended for mixtures, can be adapted to a completely different scenario to deal with functional data, giving a positive answer to a problem outside the usual scope of application of Doob's results.

Lastly, we have presented numerical evidence that supports the proposed Bayesian methodology and its predictive performance with simulated and real data sets. Thanks to our RKHS formulation, we can effectively leverage the capabilities of RJMCMC samplers and integrate the unknown number of components \(p\) in the Bayesian procedure, which to our knowledge is a novel application in the FDA context. This practical side of our work goes to show that the prediction methods we constructed from the posterior distribution are competitive against several non-cherry-picked frequentist techniques, especially those based on the usual \(L^2\)-models, while still remaining viable implementation-wise. Of course, we are not suggesting that the \(L^2\) perspective should be abandoned, but merely offering a simple, theoretically-backed alternative that can perform better in many situations.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Acknowledgments
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

{\footnotesize
\subsubsection*{Acknowledgments}
The authors would like to acknowledge the computational resources provided by the Centro de Computación Científica-Universidad Autónoma de Madrid (CCC-UAM). This research was partially supported by grants PID2019-109387GB-I00 and PRE2020-095147 of the Spanish Ministry of Science and Innovation (MCIN), co-financed by the European Social Fund (ESF). J. R. Berrendero and A. Cuevas also acknowledge financial support from grant CEX2019-000904-S funded by MCIN/AEI/10.13039/501100011033.
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% References
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliography{bibliography}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Appendices
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\appendix
\input{paper-supplement.tex}


\end{document}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\typeout{get arXiv to do 4 passes: Label(s) may have changed. Rerun} % arxiv directive
