%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Copyright (c) 2025 The authors
%
% This work is licensed under a
% Creative Commons Attribution-ShareAlike 4.0 International License.
%
% You should have received a copy of the license along with this
% work. If not, see <http://creativecommons.org/licenses/by-sa/4.0/>.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass{article}

%% Packages
\usepackage{arxiv}
\usepackage{authblk}
\usepackage[T1]{fontenc}   
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{bigints}
\usepackage{chngcntr}
\usepackage[english]{babel}
\usepackage{mathtools}
\usepackage{enumitem}
\usepackage{amssymb}
\usepackage{microtype}
\usepackage[table,usenames,dvipsnames]{xcolor}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{placeins}
\usepackage{bm}
\usepackage[authoryear]{natbib}
\usepackage[
  colorlinks,
  linkcolor=blue,
  citecolor=blue,
  urlcolor=blue,
  filecolor=blue,
  backref=page
]{hyperref}
\usepackage{graphicx}

%% Graphics path
\graphicspath{%
  {./},
  {./figures}
}

%% Bibliography style
\bibliographystyle{ba.bst}

%% Show links only in year in citations
\citefix{} % defined in arxiv.sty

%% Insert space before backref pages in reference list
\NewCommandCopy{\oldbackref}{\backref}
\renewcommand*{\backref}[1]{
  \hspace{0.1em}\oldbackref{#1}
}

%% Author block typeset
\renewcommand\Authfont{\bfseries}
\setlength{\affilsep}{0em}

%% Space between rows in tables
\renewcommand{\arraystretch}{1.2}

%% Emphasis in tables
\newcommand\firstcolor[1]{\textbf{#1}}
\newcommand\secondcolor[1]{{\color{RoyalBlue}\textit{#1}}}


%% Maths settings
\numberwithin{equation}{section}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{definition}[theorem]{Definition}


%% Custom commands
\renewcommand{\epsilon}{\varepsilon}

\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Hcal}{\mathcal{H}}
\newcommand{\tTheta}{\tilde{\Theta}}
\newcommand{\I}{\mathbb{I}}

\DeclareMathOperator{\Var} {Var}
\DeclareMathOperator{\Cov} {Cov}
\DeclareMathOperator{\supp} {supp}
\DeclareMathOperator{\KL} {KL}

\newcommand\dotprod[2]{\left\langle#1,#2\right\rangle}
\newcommand{\textoversim}[1]{\overset{\text{#1}}{\sim}}

%% Title and author information
\title{A Bayesian approach to functional regression: theory and computation}

\date{\today}

\newbox{\orcid}\sbox{\orcid}{\includegraphics[scale=0.06]{orcid.pdf}} 
\author[1,2]{%
	\href{https://orcid.org/0000-0003-0728-7748}{\usebox{\orcid}\hspace{1mm}José R.~Berrendero\thanks{\texttt{\href{mailto:joser.berrendero@uam.es}{joser.berrendero@uam.es}}}}%
}
\author[1]{%
	\href{https://orcid.org/0009-0004-7554-9193}{\usebox{\orcid}\hspace{1mm}Antonio Coín\thanks{\texttt{\href{mailto:antonio.coin@uam.es}{antonio.coin@uam.es}} (corresponding author)}}%
}
\author[1,2]{%
	\href{https://orcid.org/0000-0002-7993-0096}{\usebox{\orcid}\hspace{1mm}Antonio Cuevas\thanks{\texttt{\href{mailto:antonio.cuevas@uam.es}{antonio.cuevas@uam.es}}}}%
}
\affil[1]{Departamento de Matemáticas, Universidad Autónoma de Madrid (UAM), Madrid, Spain}
\affil[2]{Instituto de Ciencias Matemáticas ICMAT (CSIC-UAM-UC3M-UCM), Madrid, Spain}
\newcommand\shortauthor{J. R. Berrendero, A. Coín and A. Cuevas}

%% PDF metadata
\hypersetup{
pdftitle={A Bayesian approach to functional regression: theory and computation},
pdfsubject={Preprint},
pdfauthor={José R.~Berrendero, Antonio Coín, Antonio Cuevas},
pdfkeywords={functional data analysis, functional regression, reproducing kernel Hilbert space, reversible jump MCMC, Bayesian inference, posterior consistency},
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Document body
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\maketitle

%% Abstract
\begin{abstract}
  We propose a novel Bayesian methodology for inference in functional linear and logistic regression models based on the theory of reproducing kernel Hilbert spaces (RKHS's). We introduce general models that build upon the RKHS generated by the covariance function of the underlying stochastic process, and whose formulation includes as particular cases all finite-dimensional models based on linear combinations of marginals of the process, which can collectively be seen as a dense subspace made of simple approximations. By imposing a suitable prior distribution on this dense functional space we can perform data-driven inference via standard Bayes methodology, estimating the posterior distribution through reversible jump Markov chain Monte Carlo methods. In this context, our contribution is two-fold. First, we derive theoretical results that guarantee strong posterior consistency and contraction at an optimal rate under mild conditions. Second, we show that several prediction strategies stemming from our Bayesian procedure are competitive against other usual alternatives in both simulations and real data sets, including a Bayesian-motivated variable selection method.
  \end{abstract}

%% Keywords
\keywords{functional data analysis \and functional regression \and reproducing kernel Hilbert space \and Bayesian inference \and reversible jump MCMC \and posterior consistency}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Introduction}\label{sec:intro}

The problem of predicting a scalar response from a functional covariate is one that has gained traction over the last few decades, as more and more real-world data is being generated with an ever-increasing level of granularity in the measurements. While in principle the functional data could simply be regarded as a discretized vector in a very high dimension, there are often many advantages in taking into account its functional nature, ranging from modeling the correlation among points that are close in the domain, to extracting information that may be hidden in the derivatives of the function in question. As a consequence, numerous proposals have arisen on how to suitably deal with functional data, all of them encompassed under the term Functional Data Analysis (FDA), which essentially explores statistical techniques to process, model and make inference on data varying over a continuum. A partial survey on such methods is \citet{cuevas2014partial} or \citet{goia2016introduction}, while a more detailed exposition of the theory and applications can be found for example in \citet{hsing2015theoretical} and \citet{horvath2012inference}.

FDA is undoubtedly an active area of research, which finds applications in a wide variety of fields, such as biomedicine, finance, meteorology or chemistry \citep{ullah2013applications}. Accordingly, there are many recent contributions on how to tackle functional data problems, both from a theoretical and practical standpoint. Chief among them is the approach of reducing the problem to a finite-dimensional one, for example using a truncated basis expansion or spline interpolation methods \citep{muller2005generalized, aguilera2013comparative}. At the same time, much effort has been put into building a sound theoretical basis for FDA, generalizing different frequentist concepts to the infinite-dimensional framework. Examples of this endeavor include the definition of centrality measures and depth-based notions for functional data \citep{lopez2009concept}, functional ANOVA tests \citep{cuevas2004anova}, a functional Mahalanobis distance \citep{galeano2015mahalanobis, berrendero2020mahalanobis}, or an extension of Fisher's discriminant analysis for random functions \citep{shin2008extension}, among many others. As the name suggests, FDA techniques are heavily inspired by functional analysis tools and methods: Hilbert spaces, linear operators, orthonormal bases, and so on. Incidentally, a notion that also intersects with the classical theory of machine learning and pattern recognition, and that has attained popularity in recent years, is that of reproducing kernel Hilbert spaces (RKHS's) and their applications in functional data problems \citep{kupresanin2010rkhs, yuan2010reproducing, berrendero2018use}. On the other hand, Bayesian inference methods are ubiquitous in the realm of statistics, and though they also make use of random functions, the approach is slightly different from the FDA case. While there are recent works that offer a Bayesian treatment of functional data \citep[e.g.][]{crainiceanu2010bayesian, shi2011gaussian}, there is still no systematic approach to Bayesian methodologies within FDA. It is precisely at this relatively unexplored intersection between FDA and Bayesian methods that our work is aimed.

In particular, our goal is to study functional regression problems, the infinite-dimensional equivalents of the typical statistical regression problems, from a Bayesian perspective. We follow the path started by \citet{ferguson1974prior} of setting a prior distribution on a functional space and using the corresponding posterior for inference. In our case, we use a particular RKHS as the ambient space, resulting in functional regression models that allow for a simple yet efficient Bayesian treatment of functional data. Moreover, we also study the basic theoretical question of posterior consistency in these RKHS models within the proposed Bayesian framework. Consistency and posterior concentration are a type of frequentist validation criteria that have arguably been an active point of research in the last few decades, particularly in infinite-dimensional settings \citep{amewou2003posterior, choi2008remarks}, and also in the functional regression case \citep{lian2016posterior,abraham2020posterior}. To put it simply, posterior consistency ensures that with enough data points, the Bayesian updating mechanism works as intended and the posterior distribution eventually concentrates around the true value of the parameters, supposing the model is well specified. We leverage the properties of RKHS's and certain extensions of classical results by \citet{doob1949application} and \citet{schwartz1965bayes} to show that posterior consistency and contraction at an optimal rate hold in our models with minimal conditions, thus providing a coherent background to our Bayesian approach.

Finally, this theoretical side is complemented by extensive experimentation that showcases the application of the functional RKHS models in various prediction tasks. Following recent trends in Bayesian computation techniques, the posterior distribution is approximated via Markov chain Monte Carlo (MCMC) methods, specifically the \textit{reversible jump} variant (RJMCMC) proposed by \citet{green1995reversible}. This computational work highlights the predictive performance of the proposed functional regression models, especially when compared with other usual frequentist methods.

\subsubsection*{\(L^2\)-models, shortcomings and alternatives}

In this work we are concerned with functional linear and logistic regression models, that is, situations where the goal is to predict a continuous or dichotomous variable from functional observations. Even though these problems can be formally stated with almost no differences from their finite-dimensional counterparts, there are some fundamental challenges as well as some subtle drawbacks that emerge as a result of working in infinite dimensions. To set a common framework, we will consider throughout a scalar response variable \(Y\) (either continuous or binary) which has some dependence on a stochastic \(L^2\)-process \(X=X(t)=X(t, \omega)\) with trajectories in \(L^2[0, 1]\), observed on a dense grid. We will further suppose for simplicity that \(X\) is centered, that is, its mean function \(m(t)=\E[X(t)]\) vanishes for all \(t\in[0,1]\). In addition, we will tacitly assume the existence of a labeled data set \(\mathcal D_n =\{(X_i, Y_i): i=1,\dots, n\}\) of independent observations from \((X, Y)\), and our aim will be to accurately predict the response corresponding to unlabeled samples from \(X\).

The most common scalar-on-function linear regression model is the classical \(L^2\)-model, widely popularized since the first edition (1997) of the pioneering monograph by~\citet{ramsay2005functional}. It can be seen as a generalization of the usual finite-dimensional model, replacing the scalar product in \(\R^d\) for that of the functional space \(L^2[0,1]\) (henceforth denoted by \(\dotprod{\cdot}{\cdot}\)):
\begin{equation}\label{eq:l2-linear-model}
  Y = \alpha + \dotprod{X}{\beta} + \epsilon = \alpha + \int_0^1 X(t)\beta(t)\, dt + \epsilon,
\end{equation}
where \(\alpha\in \R\), \(\epsilon\) is a random error term independent from \(X\) with \(\E [\epsilon]=0\), and the functional slope parameter \(\beta=\beta(\cdot)\) is a member of the infinite-dimensional space \(L^2[0, 1]\). In this case, the inference on \(\beta\) is hampered by the fact that \(L^2[0,1]\) is an extremely broad space that contains many non-smooth or ill-behaved functions, so any estimation procedure involving optimization on it will typically be hard. In spite of this, model~\eqref{eq:l2-linear-model} is not flexible enough to include ``simple'' impact points models based on linear combinations of the marginals of \(X\), such as \(Y=\alpha + \beta_1 X(t_1)+ \cdots + \beta_p X(t_p) + \epsilon\) for some constants \(\beta_j\in\R\) and instants \(t_j\in[0,1]\), which are especially appealing to practitioners confronted with functional data problems; see \citet{berrendero2024functional} for additional details on this. Moreover, the non-invertibility of the covariance operator associated with \(X\), which plays the role of the covariance matrix in the infinite case, invalidates the usual least squares theory \citep{cardot2011functional}. Thus, some regularization or dimensionality reduction technique is needed for parameter estimation; see \citet{reiss2017methods} for a summary of several widespread methods.

A similar \(L^2\)-based functional logistic equation can be derived for the binary classification problem via the logistic function:
\begin{equation}\label{eq:l2-logistic-model}
  \mathbb P(Y=1 \mid X) = \frac{1}{1 + \exp\{-\alpha - \dotprod{X}{\beta}\}},
\end{equation}
where \(\alpha \in \R\) and \(\beta \in L^2[0, 1]\). In the equivalent finite-dimensional problem the natural way of estimating the slope parameter is via its maximum likelihood estimator (MLE). However, apart from the issues outlined above for the linear model, functional settings pose an additional challenge in the logistic case: under fairly general conditions, the MLE does not exist with probability one \citep[see][]{berrendero2023functional}.

It turns out that in both scenarios a natural alternative to the \(L^2\)-model is the so-called reproducing kernel Hilbert space (RKHS) model, which instead assumes the unknown functional parameter \(\beta\) to be a member of the RKHS associated with the covariance function of the process \(X\). As we will show later on, not only is this model simpler and arguably easier to interpret, but it also constrains the parameter space to smoother and more manageable functions. In fact, it does include a model based on finite linear combinations of the marginals of \(X\) as a particular case, while also generalizing the aforementioned \(L^2\)-models under some conditions. These RKHS-based models and their idiosyncrasies have been explored in \citet{berrendero2019rkhs, berrendero2024functional} in the linear case, and in \citet{berrendero2023functional} in the logistic case.

A major aim of this work is to motivate these models within the functional framework, while also providing efficient techniques to apply them in practice. Our main contribution is the proposal of a Bayesian approach for inference in these RKHS models, in which a prior distribution is imposed on \(\beta\) to use the posterior probabilities for prediction. Although placing a prior distribution on a functional space is generally a hard task, the specific parametric formulation we propose greatly facilitates this. Similar Bayesian schemes have recently been explored in \citet{grollemund2019bayesian} and \citet{abraham2024informative}, albeit not in a RKHS setting. Another set of techniques extensively studied in this context are variable selection methods, which aim to select the marginals \(\{X(t_j)\}\) of the process that better summarize it according to some optimality criterion (see \citealp{ferraty2010most} or \citealp{berrendero2016variable} by way of illustration). As it happens, some RKHS-based variable selection methods have already been proposed \citep[e.g.][]{bueno2019variable}, but in general they have their own dedicated algorithms and procedures. As will shortly become apparent, our Bayesian methodology allows us to easily isolate the marginal posterior distribution corresponding to a finite set of points \(\{t_j\}\), providing a Bayesian variable selection process along with the other prediction methods that naturally arise from our approach.

\subsubsection*{Some essentials on RKHS's and notation}\label{sec:rkhs}

The methodology proposed in this work relies heavily on the use of RKHS's, so we will briefly outline the main characteristics of these spaces from a probabilistic point of view \citep[for a more detailed account, see][]{berlinet2004reproducing}. Let us denote by \(K(t, s)= \mathbb E[X(t)X(s)]\) the covariance function (the ``kernel'') of the centered process \(X\), and in what follows suppose that it is continuous. To construct the corresponding RKHS \(\Hcal(K)\), we start by defining the functional space \(\Hcal_0(K)\) of all finite linear combinations of evaluations of \(K\), that is,
\begin{equation}\label{eq:h0}
  \Hcal_0(K) = \left\{ f \in L^2[0,1]: \ f(\cdot) = \sum_{j=1}^p \beta_j K(t_j, \cdot), \ p \in \N, \ \beta_j \in \R, \ t_j \in [0, 1] \right\}.
\end{equation}
This space is endowed with the inner product \(\dotprod{f}{g}_K = \sum_{j, k} \beta_j \gamma_k K(t_j, s_k)\), given that \(f(\cdot)=\sum_j \beta_j K(t_j, \cdot) \) and \(g(\cdot)=\sum_k \gamma_k K(s_k, \cdot)\). Then, \(\Hcal(K)\) is defined to be the completion of \(\Hcal_0(K)\) under the norm induced by the scalar product \(\dotprod{\cdot}{\cdot}_K\). As it turns out, functions in this space satisfy the so-called \textit{reproducing property}: \(\dotprod{K(t, \cdot)}{f}_K = f(t)\) for all \(f \in \Hcal(K)\) and \(t \in [0, 1]\). An important consequence of this identity is that \(\Hcal(K)\) is a space of genuine functions and not of equivalence classes, since the values of the functions at specific points are in fact relevant, unlike in \(L^2\)-spaces.

Now, a particularly useful approach in statistics is to regard \(\Hcal(K)\) as an isometric copy of a well-known space related to \(X\). Specifically, via \textit{Loève's isometry} \citep{loeve1948fonctions} one can establish a congruence \(\Psi_X\) between \(\Hcal(K)\) and the linear span of the process, \(\mathcal L(X)\), in the space of all random variables with finite second moment, \(L^2(\Omega)\) \citep[see Lemma 1.1 in][]{lukic2001stochastic}. This isometry is essentially the completion of the correspondence
\begin{equation*}
  \sum_{j=1}^p \beta_j X(t_j) \longleftrightarrow \sum_{j=1}^p \beta_j K(t_j, \cdot),
\end{equation*}
and can be formally defined, in terms of its inverse, as \(\Psi^{-1}_X(U)(t) = \E[U X(t)]\) for \(U \in \mathcal L(X)\).
Despite the close connection between the process \(X\) and the space \(\Hcal(K)\), special care must be taken when dealing with concrete realizations, since in general the trajectories of \(X\) do not belong to the corresponding RKHS with probability one \citep[][Corollary~7.1]{lukic2001stochastic}. As a consequence, the expression \(\dotprod{x}{f}_K\) is ill-defined and lacks meaning when \(x\) is a realization of \(X\). However, following Parzen's approach in his seminal work \citep[][Theorem~4E]{parzen1961approach}, we can leverage Loève's isometry and identify \(\dotprod{X}{f}_K\) with the random variable in \(\mathcal L(X)\) associated with \(f\in \Hcal(K)\), so that \(\dotprod{x}{f}_K \) with \(x=X(\omega)\) is defined as the image \(\Psi_x(f) := \Psi_X(f)(\omega)\). This notation, viewed as a formal extension of the inner product, often proves to be useful and convenient.

\subsubsection*{Organization of the article}

The rest of the paper is organized as follows. In Section~\ref{sec:methodology} we explain the Bayesian methodology and the functional regression models we propose, including an overview of the reversible jump MCMC scheme. In Section~\ref{sec:consistency} we derive theoretical posterior consistency and posterior concentration results. The empirical findings of the experimentation are contained in Section~\ref{sec:results}, along with a short discussion of computational details. Lastly, the conclusions drawn from this work are presented in Section~\ref{sec:conclusion}. Additional details, proofs and results are included in Appendices~\ref{app:model-choice},~\ref{app:proofs},~\ref{app:experiments} and~\ref{app:source-code}.


\section{A Bayesian methodology for RKHS-based functional regression models}\label{sec:methodology}

In principle, the functional RKHS models contemplated in this work are those obtained by considering a functional parameter \(\beta \in \Hcal(K)\) and replacing the scalar product for \(\dotprod{X}{\beta}_K\) in the \(L^2\)-models~\eqref{eq:l2-linear-model} and~\eqref{eq:l2-logistic-model}, which has tangible benefits both in theory and practice on account of the RKHS properties. However, to further simplify things we will follow a parametric approach and suppose that \(\beta\) is in fact a member of the dense subspace \(\Hcal_0(K)\) defined in~\eqref{eq:h0}, i.e.:
\begin{equation}\label{eq:beta}
  \beta(\cdot) = \sum_{j=1}^p \beta_j K(t_j, \cdot).
\end{equation}
As we said before, with a slight abuse of notation we will understand the expression \(\dotprod{x}{\beta}_K\) as \(\Psi_x(\beta)\), where \(x=X(\omega)\) is a realization of \(X\) and \(\Psi_x\) is Loève's isometry. Hence, taking into account that \(\beta(\cdot)=\sum_j \beta_j K(t_j,\cdot)\) and that \(\Psi_X(K(t, \cdot)) = X(t)\) by definition, we can identify \(\dotprod{x}{\beta}_K\) with \(\sum_j \beta_j x(t_j)\). In addition, we will assume that \(X\) has a version with continuous sample paths, so that point evaluation is well-defined.

Although natural in this context, the crucial assumption that \(\beta \in \Hcal_0(K)\) may seem too restrictive at first glance, since we are essentially truncating the dimensionality of the model. Nevertheless, in this way we get a simpler, finite-dimensional approximation of the functional RKHS model, which we argue reduces the overall complexity while still capturing most of the relevant information. Plus, the simplified model remains ``truly functional'' in the sense that the number of components \(p\) and the time instants \(t_j\) are not fixed beforehand. In short, we are exploiting the RKHS perspective to give a functional nature to the collection of finite-dimensional models based on random linear combinations of the marginals, offering a unified treatment of all the so-called \textit{impact points} models (\citealp[see][]{lindquist2009logistic, kneip2016functional,poss2020superconsistent}).

In view of~\eqref{eq:beta}, to place a prior distribution on the unknown function \(\beta\) (that is, a prior distribution on the functional space \(\Hcal_{0}(K)\)) it suffices to consider a discrete distribution on the number of components \(p\), and then select \(p\)-dimensional continuous priors for the coefficients \(\beta_j\) and the times \(t_j\) given \(p\). Thanks to this parametric approach, the challenging task of setting a prior distribution on a space of functions is considerably simplified, while simultaneously not constraining the model to any specific distribution (in contrast to, say, Gaussian process regression methods). Moreover, note that our simplifying assumption on \(\beta\) is not particularly limiting from a Bayesian point of view, since any distribution \(\mathbb{P}_0\) on \(\Hcal_0(K)\) can be directly extended to a distribution \(\mathbb{P}\) on \(\Hcal(K)\) by defining \(\mathbb{P}(B) = \mathbb{P}_0(B\cap \Hcal_0(K))\) for all Borel sets \(B\) on \(\Hcal(K)\).

Lastly, since the value of \(p\) can vary, we need a way to introduce dimension information in our MCMC posterior approximation scheme. There are several alternatives such as product space formulations \citep{carlin1995bayesian} or Bayesian averaging/model selection methods \citep{hoeting1999bayesian}, but this is precisely the problem that reversible jump samplers were designed to solve. The use of these samplers fits naturally within our framework, as they allow the complexity of the model to be directly chosen by the data, jointly inferring about the dimensionality and the value of the parameters.

\subsection{Functional linear regression}\label{sec:rkhs-linear-model}

In the case of functional linear regression, the simplified RKHS model considered is
\[
  Y = \alpha + \dotprod{X}{\beta}_K + \epsilon = \alpha + \sum_{j=1}^p \beta_j X(t_j) + \epsilon,
\]
where \(\beta(\cdot)=\sum_{j=1}^p\beta_j K(t_j, \cdot) \in \Hcal_0(K)\), \(\alpha\in\R\), and \(\epsilon \sim \mathcal N(0,\sigma^2)\) is an error term independent from \(X\). This model is essentially a finite-dimensional approximation from a functional perspective to the more general RKHS model that assumes \(\beta \in \Hcal(K)\) \citep{berrendero2024functional}. Since the number of components \(p\) is unknown, the full parameter space is \(\Theta = \bigcup_{p\in\N}\Theta_p\), where \(\Theta_p = \R^p \times [0, 1]^p \times \R \times \R^+_0\). In the sequel, a generic element of \(\Theta_p\) will be denoted by \(\theta_p = (\beta_1,\dots, \beta_p, t_1,\dots, t_p, \alpha, \sigma^2) \equiv (b_p, \tau_p, \alpha, \sigma^2)\), though we will occasionally omit the subscript when the value of \(p\) is understood. For \(\theta \in \Theta\), the reinterpreted model in terms of the available sample information is
\begin{equation}\label{eq:rkhs-model-linear-2}
  Y_i \mid X_i, \theta \ \stackrel{\text{ind.}}{\sim} \mathcal N\left(\alpha + \sum_{j=1}^p \beta_j X_i(t_j),\, \sigma^2\right), \quad i =1,\dots, n.
\end{equation}

It is worth mentioning that the model remains linear in the sense that it fundamentally involves a random variable \(\dotprod{X}{\beta}_K = \Psi_X(\beta)\) in the linear span of the process \(X\). Moreover, this RKHS model is particularly suited as a basis for variable selection methods \citep[see][]{berrendero2019rkhs}.

\subsubsection*{Prior distributions}

A simple and intuitive prior distribution \(\Pi\) for the parameter vector \(\theta \in \Theta\), suggested by the structure of the parameter space and usually employed in the literature, is given by
\begin{equation}\label{eq:prior-linear}
  \begin{array}{r@{\ }c@{\ }l@{\quad}l}
    p                    & \sim               & \pi,                   &                \\[0.3em]
    t_j                  & \textoversim{ind.} & \mathcal{U}[0,1],      & j = 1,\dots,p, \\[0.3em]
    \beta_j              & \textoversim{ind.} & \mathcal{N}(0,\eta^2), & j = 1,\dots,p, \\[0.3em]
    \Pi(\alpha,\sigma^2) & \propto            & \dfrac{1}{\sigma^2},   &
  \end{array}
\end{equation}
where  \(\pi\) is any discrete distribution on \(\N\) (e.g.~uniform on a given finite subset) and \(\eta^2 \in \R^+\) is a hyperparameter of the model that depends strongly on the expected scale of the data. On the one hand, note the use of a joint prior distribution on \(\alpha\) and \(\sigma^2\), which is a widely used non-informative prior known as Jeffrey's prior \citep{jeffreys1946invariant}. One should be wary of the Jeffreys-Lindley paradox when assigning improper priors \citep[see][]{robert2014jeffreys}, but since the two parameters involved are common to all possible values of \(p\), this formulation should not present difficulties. On the other hand, the prior on the coefficients \(\beta_j\) (which are interchangeable when coupled with their respective \(t_j\)) is deliberately kept simple and independent of \(p\), as this will greatly speed up and simplify the computations of the RJMCMC sampler later on. Nevertheless, the methods could be adapted to more complicated and dependent priors without too much trouble.

\subsection{Functional logistic regression}\label{sec:rkhs-logistic-model}

In the case of functional logistic regression, we regard the binary response variable \(Y\in\{0, 1\}\) as a Bernoulli random variable given the functional regressor \(X\). Then, following the approach suggested by \citet{berrendero2023functional}, a simplified logistic RKHS model is given by the equation
\begin{equation}\label{eq:rkhs-model-logistic}
  \mathbb P(Y=1 \mid X) = \frac{1}{1 + \exp\{-\alpha - \dotprod{X}{\beta}_K\}}, \quad \alpha \in \R, \ \beta \in \Hcal_{0}(K).
\end{equation}

Indeed, note that this can be seen as a finite-dimensional approximation (with a functional interpretation) to the general RKHS functional logistic model proposed by these authors, which can be obtained by replacing \(\Hcal_{0}(K)\) with \(\Hcal(K)\). After incorporating the sample information, we can rewrite~\eqref{eq:rkhs-model-logistic} in parametric form for \(\theta\in\Theta\) as
\begin{equation}\label{eq:rkhs-model-logistic-2}
  Y_i \mid X_i,\theta \ \stackrel{\text{ind.}}{\sim} \operatorname{Bernoulli}(H_\theta(X_i)), \quad i=1,\dots, n,
\end{equation}
where
\[
  H_\theta(X_i) = \mathbb P(Y_i=1 \mid X_i,\theta) = \frac{1}{\displaystyle 1 + \exp\left\{-\alpha - \sum_{j=1}^p \beta_j X_i(t_j)\right\}}, \quad i=1,\dots, n.
\]

In much the same way as the linear regression model described above, this RKHS-based logistic regression model offers some advantages over the classical \(L^2\)-model. First, it has a more straightforward interpretation and allows for a workable Bayesian approach. Secondly, it can be shown that under mild conditions the general RKHS logistic functional model holds whenever the conditional distributions \(X | Y=i\) (\(i=0,1\)) are homoscedastic Gaussian processes, and in some cases it also entails the \(L^2\)-model~\citep[see][]{berrendero2023functional}; this arguably provides a solid theoretical motivation for the reduced model. Incidentally, these models also shed light on the near-perfect classification phenomenon for functional data, described by \citet{delaigle2012achieving} and further examined for example in the works of \citet{berrendero2018use} or \citet{torrecilla2020optimal}.

Furthermore, a maximum likelihood approach for parameter estimation (although not considered here) is possible as well, since the finite-dimensional approximation  mitigates the problem of non-existence of the MLE in the functional case. However, let us recall that even in finite-dimensional settings there are cases of quasi-complete separation in which the MLE does not exist \citep{albert1984existence}. Additionally, the non-existence issue of the MLE becomes more pronounced as the dimension increases, as exemplified in the theory recently developed by \citet{candes2020phase}. In any event, we argue that the simplified RKHS model presented here is a compelling and feasible approach to functional logistic regression, since it bypasses the main difficulties of the usual maximum likelihood techniques.

\subsubsection*{Prior distributions}

As far as prior distributions go, we proceed as we did in the linear model. However, following the advice in \citet{gelman2008weakly} and \citet{ghosh2018use}, we do change the prior of the coefficients \(\beta_j\) and the constant term \(\alpha\), since their interpretation is different now:
\begin{equation}\label{eq:prior-logistic}
  \begin{array}{r@{\ }c@{\ }l@{\quad}l}
    p       & \sim               & \pi,                   &                \\[0.3em]
    t_j     & \textoversim{ind.} & \mathcal{U}[0,1],      & j = 1,\dots,p, \\[0.3em]
    \beta_j & \textoversim{ind.} & t_5(0,2.5),            & j = 1,\dots,p, \\[0.3em]
    \alpha  & \sim               & \mathrm{Cauchy}(0,10), &
  \end{array}
  \end{equation}
The scaled Student's \(t\) and Cauchy distributions represent robust and weakly informative priors that provide shrinkage and can handle the case of separation in logistic regression. The specific values of the parameters have been chosen via experimentation following the initial recommendations in \citet{ghosh2018use}, which also establishes the need to scale the regressors to have mean 0 and standard deviation 0.5. Note that in this case the nuisance parameter \(\sigma^2\) does not appear in the model.

\subsection{Reversible jump samplers for prediction}\label{sec:rjmcmc}

For the inference step in our Bayesian procedure, the usual approach would be to consider a fixed number of components, chosen separately via some model selection criterion. Instead, we aim to construct a fully Bayesian methodology that allows us to model the number of components and the component parameters jointly. Since we do not impose conjugate priors and the posterior distribution does not have a recognizable shape, a simple way to achieve the desired outcome is to use reversible jump MCMC (RJMCMC) techniques to approximate the posterior. Originally envisioned for approximate inference in Bayesian mixtures, they can be used to sample models with an unknown number of components, providing a certain level of flexibility and theoretically allowing the exploration of the whole parameter space \citep[see][]{richardson1997bayesian}.

The basis of the RJMCMC mechanism is a clever reformulation of the standard MCMC technique: on each iteration, apart from updating the current parameters, it tries to increase or decrease the dimension, creating new components or eliminating some already present. The acceptance fraction for these new moves is selected so that detailed balance as a whole is maintained, but this includes the possibly challenging computation of a certain Jacobian that is the key to dimension matching \citep{green1995reversible}. However, in nested models such as ours, we can simplify this expression by only allowing on each iteration either the birth of a new component or the death of an existing one (i.e.~changing the dimension by one unit at a time). If we also make the proposal distribution for the newly birthed components independent of previous values, the Jacobian term disappears, and the acceptance fraction reduces to \citep{brooks2003efficient}
\[
  \alpha_\text{nested} = \min\left(1, \frac{\mathcal{L}(Y | X, \theta_{p+1})}{\mathcal{L}(Y| X, \theta_p)}\frac{\Pi(\theta_{p+1})}{\Pi(\theta_p)}\frac{b_{p, p+1}}{d_{p+1, p}}\frac{1}{q(\theta_{+1})}\right).
\]
Here \(p\) is the current number of components, \(b_{p, p+1}\) is the probability of increasing the dimension from \(p\) to \(p+1\) (birth), \(d_{p+1, p}\) is the probability of the reverse move (death), \(q(\theta_{+1})\) is the proposal distribution for the added source, \(\Pi\) is the prior probability and \(\mathcal L\) represents the likelihood. This is already a rather manageable expression that can be implemented efficiently, but we introduce another simplification: the proposal distribution is chosen to match the prior distribution, so that the corresponding terms cancel out. Nonetheless, in real-world scenarios one could consider more sophisticated proposal distributions that might better explore the parameter space; see for example \citet{davies2023transport} or \citet{korsakova2024neural} for some interesting ideas.

From a higher level perspective, the RJMCMC algorithm is an iterative procedure that produces a chain of \(M\) approximate samples \((p_m, \theta^*_{p_m})\) of the posterior distribution \(\Pi_n(p, \theta_p| \mathcal D_n)\), each possibly of a different dimension; see Appendix~\ref{app:validation} for a visual representation. Given a previously unseen regressor \(X_{\text{test}}\), with each of these samples we can generate an individual approximate response following our RKHS models, denoted by \(F(X_{\text{test}}, \theta^*_{p_m})\). In the linear case we sample from \(Y | X_{\text{test}}, \theta^*_{p_m}\) as in~\eqref{eq:rkhs-model-linear-2}, and in the logistic case we directly consider the probabilities \(\mathbb{P}(Y=1|X_{\text{test}},\theta^*_{p_m})\) in~\eqref{eq:rkhs-model-logistic-2}. We propose to combine these predictions in four different ways.

\subsubsection*{One-stage methods}

In these methods we essentially make use of the so-called posterior predictive distribution (PP), and they are in turn divided in two approaches.

\paragraph*{(I) Weighted sum (W-PP).} The first idea involves utilizing all available information by summarizing and aggregating every individual RKHS response. We start by considering a point summary statistic \(g\) (such as the mean, median or mode) and consolidating the predictions from all sub-models, each having distinct dimension. Then, we bring together these predictions through a weighted sum, in which the weights correspond to the relative frequencies of the corresponding values of \(p\) (their approximate posterior):
\[
  \hat Y = \sum_{p} \tilde\Pi_n(p\mid\mathcal D_n) g(\{F(X_{\text{test}}, \theta^*_{p_m})\}_{m:p_m=p}),
  \]
where \(\tilde \Pi_n(p|\mathcal D_n) = M^{-1}\sum_m \mathbb I(p_m=p)\) and \(\mathbb I\) is the indicator function. In the logistic case, we employ the usual thresholding procedure to convert the final probability to a binary class label in \(\{0,1\}\). Note that this approach is reminiscent of the factorization \(\Pi_n(p, \theta_p|\mathcal D_n) = \sum_{p}\Pi_n(p|\mathcal D_n)\Pi_n(\theta_p|p, \mathcal D_n)\) of the posterior distribution. We shall see in Section~\ref{sec:results} that this method produces the best results in practice.

\paragraph*{(II) Maximum a posteriori (MAP-PP).} We also consider a MAP strategy, where only the most probable sub-model is used and the rest of the samples are discarded. In this case, if \(\tilde p = \arg\max_p \tilde  \Pi_n(p|\mathcal D_n)\), predictions are computed as \(\hat Y = g(\{F(X_{\text{test}}, \theta^*_{p_m})\}_{m:p_m=\tilde p})\). Although this method may ignore many samples in the posterior approximation, this omission can help reduce noise and prevent outliers from affecting the final prediction.

\subsubsection*{Two-stage methods}

In these methods we focus only on the marginal posterior distribution of \(\tau_p = (t_1, \dots, t_p)\), disregarding the rest of the parameters and effectively constructing a variable selection procedure. After choosing a set of time instants \(\hat \tau_p=(\hat t_1,\dots, \hat t_p)\) using the approximate posterior samples \(\{\tau^*_{p_m}\}\), we can reduce the original functional regressors to just the marginals \(\{X_i(\hat \tau_p)\}=\{(X_i(\hat t_1),\dots, X_i(\hat t_p))\}\) and apply any of the well-known finite-dimensional prediction algorithms suited for this situation.

\paragraph*{(III) Weighted sum (W-VS).} We can mirror the weighted sum approach of the one-stage methods, with prediction computed as
\[
  \hat Y = \sum_{p} \tilde\Pi_n(p\mid \mathcal D_n) G(X_{\text{test}}, \hat \tau_{p}),
\]
where \(\hat \tau_{p}=(\hat t_1, \dots ,\hat t_p)=g(\{\tau^*_{p_m}\}_{m:p_m=p})\) and \(g\) is a component-wise summary statistic. The function \(G\) represents the prediction for \(X_{\text{test}}\) of a regular linear/logistic regression algorithm, fitted with the transformed data set \(\{(X_i(\hat \tau_p), Y_i): i=1,\dots,n\}\).

\paragraph*{(IV) Maximum a posteriori (MAP-VS).} We also consider a MAP approach to variable selection with only the information of the most probable sub-model, i.e., \(\hat Y = G(X_{\text{test}}, \hat \tau_{\tilde p})\).


\section{Posterior consistency}\label{sec:consistency}

This section explores the theoretical foundations of the proposed Bayesian models in the context of predictive inference. In particular, we establish results grounded in the theory of posterior consistency, which provides rigorous justification for the reliability of these models in asymptotic settings. For an in-depth treatment of posterior consistency and related concepts, we refer the reader to \citet{ghosal2017fundamentals}.

Firstly, let us recall what we understand by posterior consistency and posterior contraction rates. Consider the general setting of an i.i.d.\ sample \(X_1,\dots, X_n\) from a random variable \(X\) taking values in a certain space \(\mathcal X\). Let us fix a prior distribution \(\Pi\) for random variables \(\theta\) on the parameter space \(\Theta\), that is, \(\theta \sim \Pi\), and let \(P_\theta\) represent a sampling model (a distribution on \(\mathcal X\) indexed by \(\theta \in \Theta\)) such that \(X | \theta \sim P_{\theta}\). Furthermore, assume that the model is well-specified, i.e., there is a true value \(\theta_{0}\in\Theta\) such that \(X \sim P_{\theta_0}\), and denote by \(P_0^\infty\) and \(P_0^{(n)}\) the joint probability measure of \((X_1, X_2, \dots)\) and \((X_1, X_2, \dots, X_n)\), respectively, when \(\theta_0\) is the true value of the parameter.

\begin{definition}
We say that the posterior distribution is (strongly) consistent at \(\theta_0\) if for every neighborhood \(B\) of \(\theta_0\) (which for a metric space can just be the open balls around \(\theta_0\)) we have
  \[
    \lim_{n\to\infty} \Pi_n(\theta \in B \mid X_1, \dots, X_n) = 1 \quad P_0^\infty-\text{a.s.}
  \]
\end{definition}

Note that the conditional probabilities are computed under the assumed joint distribution of \((\theta, (X_1, X_2,\dots))\). Essentially, we are saying that the posterior concentrates around \(\theta_0\) for almost all sequences of data, and thus the effect of the prior gets diluted as more and more data is available for the inference. Furthermore, when the parameter space carries a metric, say \(d\), we are also interested in the speed at which the posterior approaches the true parameter \(\theta_0\). This is what we call a \textit{posterior contraction rate}, which is a significant refinement of the comparatively weaker concept of consistency.

\begin{definition}
  A sequence \(\epsilon_n\to 0\) is a posterior contraction rate at \(\theta_0\) with respect to \(d\) if for every \(M_n\to\infty\) the posterior satisfies \(\Pi_n(\theta: d(\theta_0, \theta) \geq M_n \epsilon_n | X_1,\dots, X_n) \to 0\) in \(P_0^{(n)}\)-probability.
\end{definition}

Naturally, once we have established a contraction rate, every slower sequence is also a valid contraction rate. Even though we are interested in the fastest possible rate, this is often difficult to compute or does not exist at all. Hence, we are usually content with finding a good enough rate, which we call ``the'' rate of contraction for the model.

In the rest of the section we analyze how these frequentist concepts apply to our Bayesian functional models; we focus on the linear case, but the results we obtain generally hold \textit{mutatis mutandis} in the logistic case. All technical details and proofs are deferred to Appendix~\ref{app:proofs}. For the functional linear model, our data is an i.i.d.\ sample \((X_1, Y_1), \ldots, (X_n, Y_n)\) of a random vector \((X,Y)\), where \(X=X(t)\) is a second-order stochastic process taking values in \(\mathcal X=\mathcal C[0,1]\), the space of continuous functions, and \(Y\) is a random variable taking values in \(\mathcal Y=\R\). The state space is \(\mathcal X \times \mathcal Y = \mathcal C[0,1]\times \R\), which is a complete separable metric space. Meanwhile, the infinite-dimensional parameter space can be characterized via the Euclidean spaces \(\Theta_p = \{(b, \tau, \alpha, \sigma^2): b=(\beta_1,\dots,\beta_p) \in \R^p, \tau=(t_1,\dots,t_p) \in [0,1]^p, \alpha\in \R, \sigma^2 \in \R^+_0\}\) as the infinite union
\begin{equation}\label{eq:parameter-space-union}
  \Theta = \bigcup_{p=1}^\infty \Theta_p,
\end{equation}
and note that given \(\theta \in \Theta\) there is a unique \(p=p(\theta)\) such that \(\theta \in \Theta_p\). As usual, we equip both \(\mathcal X \times \mathcal Y\) and \(\Theta\) with their respective Borel sigma-algebras.

In terms of \(\theta\in\Theta\), the data distribution can be written as \(P_\theta(X,Y)=P_{b, \tau, \alpha, \sigma^2}(X,Y)\), and formally the joint distribution  factorizes as \(P_{\theta}(X,Y)=Q_X P_\theta(Y|X)\). Here \(Q_X\) is the distribution of the underlying process \(X\), and in our RKHS setting, \(P_\theta(Y|X)\) is the normal distribution \(\mathcal N(\alpha + \sum_{j=1}^{p(\theta)} \beta_j X(t_j),\, \sigma^2)\). Moreover, for convenience, we will denote the sequences \((X,Y)_{1:\infty} := (X_1, Y_1), (X_2, Y_2), \dots\) and \((X,Y)_{1:n} := (X_1,Y_1), (X_2, Y_2), \dots, (X_n, Y_n)\). The full hierarchical model under consideration is
\begin{equation}\label{eq:model-linear}
  \begin{aligned}
    \text{(no.\ of components)}\quad & \mathcal P \sim \pi,                                                                                    \\
    \text{(component values)}\quad   & b \mid \mathcal P=p \sim \phi_p,                                                                        \\
    \text{(component times)}\quad    & \tau \mid \mathcal P=p \sim \psi_p,                                                                     \\
    \text{(intercept)}\quad          & \alpha \sim \Gamma,                                                                                     \\
    \text{(error variance)}\quad     & \sigma^2 \sim \Delta,                                                                                   \\
    \text{(observed data)}\quad      & (X,Y)_{1:n} \mid b, \tau, \alpha, \sigma^2 \sim P_{b, \tau, \alpha, \sigma^2}(X,Y) \quad \text{i.i.d.},
  \end{aligned}
\end{equation}
where \(\pi\), \(\phi_p\), \(\psi_p\), \(\Gamma\) and \(\Delta\) are probability measures on \(\N\), \(\R^p\), \([0,1]^p\), \(\R\) and \(\R^+_0\), respectively. Lastly, define the random variable \(\theta = (b, \tau, \alpha, \sigma^2)\), which takes values in \(\Theta\), and denote by \(\Pi\) the prior distribution on \(\theta\) implied by the model in~\eqref{eq:model-linear}.

\subsection{Consistency via Doob's theorem}

It turns out that, under very general conditions, the posterior distribution is always consistent at almost every value of \(\theta_0\) with respect to the measure induced by the prior \citep{doob1949application}. We state this classical result in the general setting introduced earlier.

\begin{theorem}[Doob's consistency theorem]\label{th:doob}
Let the state space \(\mathcal X\) and the parameter space \(\Theta\) be complete separable metric spaces, endowed with their respective Borel sigma-algebras. If \(\theta \mapsto P_\theta\) is one-to-one and \(\theta \mapsto P_\theta(A)\) is measurable for all measurable sets \(A\subseteq \mathcal X\), then the posterior distribution is consistent at \(\Pi\)-almost all values of \(\Theta\). That is, there exists \(\Theta_*\subseteq \Theta\) such that \(\Pi(\Theta_*)=1\) and for all \(\theta_0\in\Theta_*\), if \(X_1,X_2,\ldots \sim P_{\theta_0}\) i.i.d., then for any neighborhood \(B\) of \(\theta_0\) we have
  \[
    \lim_{n\to\infty} \Pi_n(\theta \in B \mid X_1,\dots, X_n) = 1 \quad P_{0}^\infty-\text{a.s.}
  \]
\end{theorem}

At this point we can follow an approach very similar to the one in \citet{miller2023consistency}, where posterior consistency is established through Doob's theorem in a mixture model with an unknown number of components, which has an infinite-dimensional parameter space that factorizes in the same way as~\eqref{eq:parameter-space-union}. Considering that we will only be interested in small balls around the true value of the parameter, we can define a bounded metric for \(\theta, \theta' \in \Theta\) by
\begin{equation}\label{eq:metric-doob}
  d_\Theta(\theta, \theta')= \begin{cases}
    \min \left\{\|\theta - \theta'\|, 1\right\}, \quad & \text{if } p(\theta)=p(\theta'), \\
    1, \quad                                           & \text{otherwise}.
  \end{cases}
\end{equation}
Since each \(\Theta_p\) is itself a complete separable metric space with the inherited Euclidean norm, Proposition A.1 in \citet{miller2023consistency} ensures that \((\Theta, d_\Theta)\) is a complete separable metric space. Note that \(P_\theta(X,Y)\) is invariant under permutations of the component labels \(b\) and \(\tau\), so in an identifiable model we can only show consistency in the parameter space up to one such permutation. To that effect, let \(S_p\) denote the set of permutations of \(\{1,\dots,p\}\), and for \(\theta \in \Theta_p\) and any \(\nu\in S_p\), denote by \(\theta[\nu]\) the result of applying the permutation \(\nu\) to the component labels of \(\theta\). That is, if \(\theta=(\beta_1,\dots,\beta_p, t_1,\dots, t_p,\alpha,\sigma^2)\), then \(\theta[\nu]=(\beta_{\nu_1},\dots,\beta_{\nu_p}, t_{\nu_1},\dots, t_{\nu_p},\alpha,\sigma^2)\). Next, for \(\theta_0\in\Theta_p\) and \(\epsilon>0\) define the neighborhood \(\tilde{B}(\theta_0, \epsilon)  =\bigcup_{\nu\in S_p} \{\theta \in \Theta: d_\Theta(\theta_0[\nu], \theta) < \epsilon \}\), which consists of all parameters that are within \(\epsilon\) of some permutation of (the component labels of) \(\theta_0\). Now, for identifiability to hold, we need to place some restrictions on the prior.

\begin{condition}[Identifiability constraints] Under the model in~\eqref{eq:model-linear}, for all \(p\in\N\):\label{cond:condition-ident}
  \begin{enumerate}[label=(\roman*)]
    \item \(\Pi(t_i=t_j|\mathcal P=p)=0\) for all \(1\leq i < j \leq p\).\label{cond:condition-ident-1}
    \item There exists \(\delta>0\) such that \(\Pi(|\beta_j|<\delta|\mathcal P=p)=0\) for all \(1\leq j \leq p\).\label{cond:condition-ident-2}
  \end{enumerate}
\end{condition}
Both assumptions can be interpreted as a way of pursuing parsimony in the model, aiming for as few components as possible. In practical and computational terms, we can think of \(\delta\) as the \textit{machine precision number}, so that virtually all continuous prior distributions satisfy the associated condition when implemented numerically in a computer. With this setup in mind, we are ready to state our own consistency result.

\begin{theorem}\label{th:consistency-doob-linear}
  Suppose that Condition~\ref{cond:condition-ident} holds and the covariance function \(K\) of the underlying process \(X\) is strictly positive definite. Then, there exists \(\Theta_*\subseteq \Theta\) such that \(\Pi(\theta \in \Theta_*)=1\) and for all \(\theta_0\in\Theta_*\), if \((X,Y)_{1:\infty} \sim P_{\theta_0}(X,Y)\) i.i.d., then for all \(\epsilon > 0\)
  \[
    \lim_{n\to\infty} \Pi_n(\theta \in \tilde{B}(\theta_0,\epsilon) \mid (X,Y)_{1:n}) = 1 \quad P_{0}^\infty(X,Y)-\text{a.s.}
  \]
  and
  \[
    \lim_{n\to\infty} \Pi_n(\mathcal P=p(\theta_0) \mid (X,Y)_{1:n}) = 1 \quad P_{0}^\infty(X,Y)-\text{a.s.}
  \]
\end{theorem}

The hypothesis of positive definiteness of \(K\) is needed to ensure identifiability. On the other hand, the second conclusion is of certain relevance in itself, because the estimation of the number of components in mixture-like models is a hard problem in general \citep[see][and references therein]{miller2018mixture}. Moreover, it is worth pointing out that the proof of Theorem~\ref{th:consistency-doob-linear} can be easily adjusted to guarantee consistency when the number of components is fixed beforehand and the parameter space is finite-dimensional.

\subsubsection*{A remark on Lebesgue consistency}

All in all, Theorem~\ref{th:consistency-doob-linear} guarantees consistency for \(\Pi\)-almost every parameter in the support of the prior distribution. However, even though we can choose the prior so that \(\supp( \Pi) = \Theta\), in principle there is no assurance that the \(\Pi\)-null set in which consistency may fail will not be a large set with respect to other measures. In fact, when the parameter space is infinite-dimensional there are examples of big inconsistency sets, even for reasonably chosen prior distributions~\citep{diaconis1986consistency}. Nonetheless, this problem can be alleviated when the parameter space is a countable union of disjoint finite-dimensional sets. First, note that there is a natural extension of the Lebesgue measure to our parameter space \(\Theta\): just consider the genuine Lebesgue measure \(\lambda_p\) on \(\Theta_p\), and for all \(B\subseteq \Theta\) measurable define \(\lambda_\infty(B) = \sum_{p=1}^\infty \lambda_p(\Theta_p \cap B)\). Then, if we choose a prior distribution with respect to which this measure is absolutely continuous, the inconsistency set in Theorem~\ref{th:consistency-doob-linear} will satisfy \(\lambda_\infty(\Theta \setminus \Theta_*)=0\) and thus be ``small'' with respect to a Lebesgue-type measure. In our case, the requirement of absolute continuity can be relaxed so that sets with nonzero Lebesgue measure have nonzero prior probability for some permutation of the component labels.

\begin{proposition}\label{prop:consistency-lebesgue-linear}
  Suppose that Condition~\ref{cond:condition-ident} holds. Furthermore, assume that for all \(p\in\N\) we have
  \begin{enumerate}[label=(\roman*)]
    \item \(\Pi(\mathcal P = p) > 0\).\label{cond:condition-lebesgue-1}
    \item \(\sum_{\nu\in S_p} \Pi(\theta[\nu] \in B|\mathcal P = p) = 0\) implies \(\lambda_p(B)=0\), for all \(B\subseteq \Theta_p\) measurable.\label{cond:condition-lebesgue-2}
  \end{enumerate}
  Then the conclusion of Theorem~\ref{th:consistency-doob-linear} remains valid with \(\lambda_\infty(\Theta \setminus \Theta_*)=0\).
\end{proposition}

The first condition is a somewhat technical requirement. The second condition is met, for example, if \(\theta| p\) has a density with respect to the Lebesgue measure that is invariant to permutations of the component labels and positive on all of \(\Theta_p\). A similar approach is considered in \citet{nobile1994bayesian} and \citet{miller2023consistency} to establish ``Lebesgue''-almost sure consistency in finite mixture models with a prior on the number of components.

\subsection{Consistency and contraction rates via Schwartz's theorem}\label{sec:consistency-schwartz}

The Doob-type results of the previous section, although interesting, are still assertions on consistency from the point of view of the prior distribution. We now seek additional results that offer more definitive statements and, more importantly, establish posterior contraction rates. In a nonparametric setting where the object of interest is a probability density, there is a stronger consistency result by \citet{schwartz1965bayes} which omits the \(\Pi\)-almost sure qualification under some more restrictive conditions, though it requires a dominated model and a density to estimate. To this end, we consider the conditional model \(Y|X=x, \theta \sim f_\theta(y|x)\), where \(\theta\in\Theta\) is a parameter vector and \(f_\theta(y|x)\) is the density of \(Y\) given \(X=x\) with respect to the Lebesgue measure \(\lambda\) on \(\mathcal Y\) (which under our assumptions is the normal density given by~\eqref{eq:rkhs-model-linear-2}). We could work with this model and analyze the fixed design case with non-i.i.d.\ data \citep[see][]{choi2008remarks}, but we choose to focus on the arguably more significant random design case.

In general, in infinite-dimensional models there is no reference measure with respect to which to define a density. However, in our specific functional regression setting we can find such a density through the following observation: it follows from the disintegration theorem and a straightforward application of Dynkin's \(\pi\)-\(\lambda\) theorem that the function \((x,y)\mapsto f_\theta(y|x)\) is a joint density of \((X,Y)\) with respect to the product measure \(\rho=Q_X\times \lambda\), where \(Q_X\) is the law of the process \(X\). In this way, denoting \(f_\theta := f_\theta(y|x)\), we can express our full model (with respect to the product measure \(\rho\)) as
\[
  (X, Y)_{1:n} \mid f_\theta \stackrel{\text{i.i.d.}}{\sim} f_\theta \quad \text{and} \quad f_\theta \sim \Pi_{\mathcal F},
\]
where \(\Pi_{\mathcal F}\) is a prior distribution on the parameter class \(\mathcal F=\{f_\theta: \theta \in \Theta\}\), which is the set of all densities following our model relative to the dominating measure \(\rho\) on \(\mathcal X \times \mathcal Y\). In practice, we work with the prior distribution \(\Pi\) on \(\Theta\) specified in~\eqref{eq:model-linear}, and interpret \(\Pi_{\mathcal F}\) as the pushforward measure of \(\Pi\) by the measurable mapping \(\Phi: \Theta \to \mathcal F\) given by \(\Phi(\theta)= f_\theta\). As always, we assume the existence of a ``true'' density \(f_0 :=f_{\theta_0}(y|x)\in \mathcal F\) that generates the observations, and denote \(\Theta_0\equiv \Theta_{p(\theta_0)}\).

Now we introduce a few concepts that lie at the core of most extensions of Schwartz's consistency theorem. Recall that the Kullback-Leibler divergence from \(f_0\) to \(f_\theta\) is defined as \(D_{\mathrm{KL}}(f_0 \,\|\, f_\theta) = \int f_0 \log (f_0/f_\theta)\, d\rho\). We say that \(f_0\) belongs to the \textit{Kullback-Leibler support of \(\Pi_{\mathcal F}\)}, and write it as \(f_0 \in \operatorname{KL}(\Pi_{\mathcal F})\), if \(\Pi_{\mathcal F}\left(f_\theta: D_{\mathrm{KL}}(f_0 \,\|\, f_\theta) < \epsilon\right) > 0\) for all \(\epsilon > 0\). This can be interpreted as saying that the prior places sufficient mass ``near'' \(f_0\). On the other hand, let \(N(\epsilon, \mathcal F, d)\) denote the \(\epsilon\)-covering number of the set \(\mathcal F\) with respect to the distance \(d\), i.e., the minimal number of \(d\)-balls of radius \(\epsilon\) needed to cover \(\mathcal F\). Finally, define the Hellinger distance \(d_H\) between two densities as \(d^2_H(f_\theta, f_{\theta'}) = 1 - \int \sqrt{f_\theta f_{\theta'}}\, d\rho\). The following result will give us consistency in the sense of Schwartz, by means of a sieve that controls the complexity of the model as the sample size \(n\) increases, measured in terms of the \textit{metric entropy} (the logarithm of the \(\epsilon\)-covering number) of the model.

\begin{theorem}[Theorem 6.23 in \citet{ghosal2017fundamentals}]\label{th:consistency-hellinger}
  Suppose that for every \(\epsilon > 0\) there exist measurable sets \(\mathcal F_n \subseteq \mathcal F\) and a constant \(C>0\) such that, for sufficiently large \(n\),
  \begin{enumerate}[label=(\roman*)]
    \item \(\log N(\epsilon, \mathcal F_n, d_H) \leq n\epsilon^2\).
    \item \(\Pi_{\mathcal F}(\mathcal F \setminus \mathcal F_n) \leq \exp\{-Cn\}\).
  \end{enumerate}
Then the posterior distribution is consistent relative to \(d_H\) at every \(f_0\in \KL(\Pi_{\mathcal F})\).
\end{theorem}

Looking at the form of the parameter space \(\Theta\) in~\eqref{eq:parameter-space-union}, a natural choice for the sieve is \(\Theta_n = \bigcup_{p=1}^{p_n} \Theta_p\), for \(p_n=O(h(n))\) and \(h(n)\to \infty\) a function to be determined later, with the corresponding sieve of densities defined as \(\mathcal F_n=\{f_\theta\in\mathcal F: \theta \in \Theta_n \}\). Moreover, to keep the conditions as simple as possible, it is more convenient to work with compact parameter spaces \(\Theta_p\) and adapt the prior distributions in~\eqref{eq:model-linear} accordingly; we assume this is so for the remainder of the section. Plus, we need to impose some smoothness restrictions on the underlying process \(X\).

\begin{condition}[Functional constraints]\label{cond:condition-schwartz-X}
  Suppose that each trajectory \(x\) of \(X\) is \(Q_X\)-almost surely Lipschitz with Lipschitz constant \(L(x)>0\), and assume that \(\mathbb E_{Q_X}[L^2]<\infty\).
\end{condition}

See Appendix~\ref{app:proofs} for a brief discussion and examples of processes that satisfy this condition. We can now state a new Hellinger consistency result for our model, which transforms the requirement that \(f_0\in \KL(\Pi_\mathcal F)\) into a condition on the prior \(\Pi\) on \(\Theta_0\) and places an additional mild constraint on the tails of the marginal prior distribution of \(p\). These conditions guarantee that we can choose a suitable growth order for \(p_n\) in the sieve to apply Theorem~\ref{th:consistency-hellinger}.

\begin{theorem}\label{th:consistency-schwartz-linear}
  Assume Condition~\ref{cond:condition-schwartz-X} holds, and suppose that \(\Pi(p)=O(e^{-\delta p (\log p)^k})\) as \(p\to\infty\), with \(\delta > 0\) and \(k>1\). If \(\Pi\) assigns positive mass to every open neighborhood of \(\theta_0\) in \(\Theta_0\), then the posterior distribution in the model \(\theta \sim \Pi\) and \((X,Y)_{1:n}|\theta \sim f_\theta\) i.i.d.\ is consistent relative to \(d_H\) at \(f_0\), i.e., for every \(\epsilon>0\) it holds that
  \[
    \lim_{n\to\infty} \Pi_n(\theta: d_H(f_0, f_\theta) < \epsilon \mid (X,Y)_{1:n})=1\quad P_0^\infty(X,Y)-\text{a.s.}
  \]
\end{theorem}

An immediate consequence is that if \(\Pi(p)\) is supported on a finite set (as we do in our experiments), then the prior tail condition is automatically verified. Moreover, since it only needs to hold for large values of \(p\), we can always consider a hybrid prior distribution that changes at some large cutoff point to have rapidly decaying tails.

Lastly, to find posterior contraction rates for our model we consider an extension of Theorem~\ref{th:consistency-hellinger} that employs the so-called \textit{second Kullback-Leibler variation}, defined by \(V_2(f_0, f_\theta)=\E_{f_0}[(\log(f_0/f_\theta) - D_{\mathrm{KL}}(f_0 \,\|\, f_\theta))^2]\), to quantify the Kullback-Leibler property of \(f_0\) in terms of convergence speed. In particular, for \(\epsilon>0\) we consider the neighborhood
\[
B_2(f_0, \epsilon) = \{f\in \mathcal F: D_{\mathrm{KL}}(f_0 \,\|\, f_\theta) < \epsilon^2,\  V_2(f_0, f_\theta) < \epsilon^2\}.
\]

\begin{theorem}[Theorem~8.9 in \citet{ghosal2017fundamentals}]\label{th:contraction-rate}
  Suppose that there exist measurable sets \(\mathcal{F}_n \subseteq \mathcal F\) and a constant \( C > 0 \) such that, for a sequence \(\epsilon_n \to 0\) with \( n\epsilon_n^2 \to \infty\), the following hold for sufficiently large \(n\):
  \begin{enumerate}[label=(\roman*)]
    \item \( \log N(\epsilon_n/2, \mathcal{F}_n, d_H) \leq n \epsilon_n^2\).
    \item \( \Pi_{\mathcal F}(\mathcal{F} \setminus \mathcal{F}_n) \leq \exp\{-(C+4)n \epsilon_n^2\}\).
    \item \( \Pi_{\mathcal F}(B_2(f_0, \epsilon_n)) \geq \exp\{-C n\epsilon_n^2\}\).
  \end{enumerate}
  Then the posterior rate of contraction at \(f_0 \) with respect to \(d_H\) is \(\epsilon_n \).
\end{theorem}

Note that the condition \(n\epsilon_n^2\to\infty\) excludes the parametric rate \(n^{-1/2}\), since in nonparametric situations the rates are usually slower. Condition (i) bounds the complexity of the model, condition (ii) merely expresses that subsets of negligible prior mass do not play a role in the rate of contraction, and condition (iii) ensures that sufficient prior mass is put ``near'' the true density \(f_0\). Following a similar proof strategy as in Theorem~\ref{th:consistency-schwartz-linear}, we can find a convenient sieve to apply Theorem~\ref{th:contraction-rate} and prove the following result in our model.

\begin{theorem}\label{th:contraction-rate-schwartz-linear}
Assume Condition~\ref{cond:condition-schwartz-X} holds. Let \(0<\gamma<1/2\) and suppose that the prior distribution satisfies \(\Pi(p)=O(e^{-\delta p \log p})\) as \(p\to\infty\), with \(\delta>16(3-4\gamma)/(1-2\gamma)\). If \(\Pi\) has a density on \(\Theta_0\) with respect to Lebesgue measure that is bounded away from zero in a neighborhood of \(\theta_0\), then the posterior distribution in the model \(\theta \sim \Pi\) and \((X,Y)_{1:n}|\theta \sim f_\theta\) i.i.d.\ contracts at a rate \(\epsilon_n= n^{-\gamma}\) relative to \(d_H\) at \(f_0\), i.e., for every \(M_n\to\infty\) it holds that
\[
    \Pi_n(\theta: d_H(f_0, f_\theta) \geq M_n \epsilon_n \mid (X,Y)_{1:n}) \to 0 \quad \text{in } P_0^{(n)}(X,Y)\text{-probability}.
    \]
\end{theorem}

The requirements of this result are akin to those of Theorem~\ref{th:consistency-schwartz-linear}, but the conclusion is considerably stronger. To verify the condition on the density of \(\Pi\), provided that it exists, it suffices to show that it is continuous and positive at \(\theta_0\) (which our proposed prior distributions satisfy). The contraction rate \(\epsilon_n=n^{-\gamma}\) with \(0<\gamma<1/2\) is optimal in the sense that it is as close as desired to the parametric rate of \(n^{-1/2}\), and it is \textit{minimax optimal} for estimators of \(f_\theta\) given the model \(\mathcal F_n\) \citep[see][p.~198 and references therein]{ghosal2017fundamentals}.

To conclude, it is worth pointing out that a contraction rate relative to \(d_H\) is also a contraction rate relative to any distance bounded above by a multiple of the Hellinger distance, so we immediately have consistency and a contraction rate in more descriptive distances. Let \([h]_M\) be the real-valued function \(h\) truncated to the interval \([-M, M]\).

\begin{corollary}\label{th:corollary-contraction-rate}
  Under the same conditions as in Theorem~\ref{th:contraction-rate-schwartz-linear}, \(\epsilon_n = n^{-\gamma}\) is a posterior contraction rate relative to the total variation distance \(d_1(f_0, f_\theta)=\int |f_0 - f_\theta|\, d\rho\) and the mean-variance discrepancy metric \(d_{2,Q_X}(\theta_0, \theta) = \|[\mu_{\theta_0}]_M - [\mu_\theta]_M\|_{2,Q_X} + |\sigma_0^2 - \sigma^2|\), for any \(M>0\), where \(\|\cdot\|_{2,Q_X}\) is the \(L^2(Q_X)\)-norm and \(\mu_\theta(x)= \alpha + \sum_{j=1}^{p(\theta)} \beta_j x(t_j)\).
\end{corollary}


\section{Experimental results}\label{sec:results}

In this section we present the results of the experiments carried out to test the performance of our Bayesian methods in different scenarios, together with an overview of their computational implementation. Further details such as simulation parameters, execution times or algorithmic decisions, as well as additional experiments, figures and tables are provided in Appendices~\ref{app:model-choice},~\ref{app:experiments} and~\ref{app:source-code}, while the code itself is publicly available on GitHub at \url{https://github.com/antcc/rk-bfr-jump}.

For simulated data we consider \(n=200\) training samples and \(n'=100\) testing samples, with functional regressors measured on an equispaced grid of \(100\) points on \([0, 1]\), and for the real data sets we perform a 66\%/33\% train/test split. We fit our models on the training data, using RJMCMC to sample from the approximate posterior, and then compute out-of-sample predictions on the testing set. We independently repeat the whole process 10 times (each with a different train/test configuration) to account for the stochasticity in the sampling step, and average the results across these executions. For the purposes of prediction we consider the point statistics \textit{trimmed mean} (10\%), \textit{median} and \textit{mode} to aggregate together predictions and summarize parameters (see Section~\ref{sec:rjmcmc}). Moreover, the values of \(\{t_j\}\) that fall outside the specified grid are truncated to the nearest neighbor in the grid, with the additional restriction that time instants in different components of our models cannot be equal.

The Python library used to perform the RJMCMC approximation is \textit{Eryn} \citep{karnesis2023eryn}. It is a toolbox for Bayesian inference that allows trans-dimensional posterior approximation, running multiple chains in an ensemble configuration with different starting points, and incorporating a parallel tempering mechanism \citep{hukushima1996exchange} to increase both convergence speed and acceptance rate. Because of execution time constraints, the hyperparameters of the Eryn sampler are selected manually based on preliminary experiments and the authors' recommendations. This reduction in computational cost renders the sampling step practically viable; information on the execution times is provided in Appendix~\ref{app:execution-times}. Moreover, some amount of post-processing is needed to mitigate the well-known \textit{label switching} phenomenon that occurs in MCMC approximations of mixture-like models \citep{stephens2000dealing}; see Appendix~\ref{app:label-switching}.

\subsubsection*{Data sets}

We consider a set of functional regressors common to linear and logistic regression problems. They are four zero-mean Gaussian processes (GPs), each with a different covariance function. In particular, we consider a Brownian motion, a fractional Brownian motion, an Ornstein-Uhlenbeck process, and a GP with a squared exponential kernel.

\paragraph*{Linear regression data sets.} We employ two different types of simulated data sets, all with a common value of \(\alpha=5\) for the intercept and \(\sigma^2=0.5\) for the error variance:
\begin{itemize}
  \item A finite-dimensional RKHS response with three components for each of the four GP regressors mentioned above: \(Y=5 -5X(0.1) + 5X(0.6) + 10X(0.8) + \epsilon\).
  \item A ``component-less'' response generated by an \(L^2\)-model with a smooth underlying coefficient function, namely \(\beta(t)=\log(1+4t)\), again for the same four GPs.
\end{itemize}
As for the real data sets, we use the Tecator data set \citep{borggaard1992optimal} to predict fat content based on near-infrared absorbance curves of 193 meat samples, as well as what we call the Moisture \citep{kalivas1997two} and Sugar \citep{bro1999exploratory} data sets. The first consists of near-infrared spectra of 100 wheat samples and the objective is to predict the samples' moisture content, whereas the second contains 268 samples of sugar fluorescence data in order to predict ash content. The regressors of the three data sets are measured on a grid of 100, 101 and 115 equispaced points on \([0, 1]\), respectively.

\paragraph*{Logistic regression data sets.} Again we consider two different types of simulated data sets, with a common value of \(\alpha=-0.5\) for the intercept:
\begin{itemize}
  \item Four logistic finite-dimensional RKHS responses with the same functional parameter as in the linear regression case (one for each GP). Specifically,
        \[
          \mathbb P(Y=1\mid X) = \frac{1}{1 + \exp\left\{0.5 +5X(0.1) - 5X(0.6) - 10X(0.8)\right\}}.
        \]
  \item Four logistic responses following an \(L^2\)-model with the same coefficient function as in the linear regression case, i.e., \(\beta(t)=\log(1+4t)\).
\end{itemize}
Additionally, we use three real data sets well known in the literature. The first one is a subset of the Medflies data set \citep{carey1998relationship}, consisting on samples of the number of eggs laid daily by 534 flies over 30 days, to predict whether their longevity is high or low. The second one is the Berkeley Growth Study data set \citep{tuddenham1954physical}, which records the height of 54 girls and 39 boys over 31 different points in their lives. Finally, we selected a subset of the Phoneme data set \citep{hastie1995penalized} based on 200 digitized speech frames over 128 equispaced points to predict the phonemes ``aa'' and ``ao''.

\subsubsection*{Comparison algorithms}

We have included a fairly comprehensive suite of comparison algorithms, chosen among the most common frequentist methods used in machine learning and FDA, and following a standard choice of implementation and hyperparameters. There are purely functional methods (such as the usual \(L^2\) regression based on model~\eqref{eq:l2-linear-model}), finite-dimensional models that work on the discretized data (e.g.\ penalized finite-dimensional regression), and variable selection/dimension reduction procedures (like Principal Component Analysis or Partial Least Squares). The main parameters of all these algorithms are selected by cross-validation, and when a number of components needs to be specified, we use the same range as in our own models so that comparisons are fair. A more detailed account of these algorithms is available in Appendix~\ref{app:data-sets}.

\subsubsection*{Results display}

We have adopted a visual approach to presenting the experimentation results, using colored graphs to help visualize them. In each case, the mean and standard deviation of the score obtained across the 10 independent runs is shown, depicting our methods in blue and the comparison algorithms in orange. We also show the global mean of all the comparison algorithms with a dashed vertical line, excluding extreme negative results to avoid distortion. Moreover, we separate one-stage and two-stage methods, the latter being the ones that perform variable selection or dimension reduction prior to a multiple linear/logistic regression method (represented with ``+r''/``+log'' in the figures). We name our methods according to the acronyms described in Section~\ref{sec:rjmcmc}.

\subsection{Functional linear regression}\label{sec:experiments-linear}

The initial experiments carried out indicate that low values of \(p\) provide sufficient flexibility in most scenarios, so we allow the number of components to vary in the set \(\{1,2,\dots,10\}\). Following \citet{nobile2007bayesian} we select a truncated Poisson prior with a low rate parameter \(\lambda=3\) for \(p\), so that simpler models are favored. However, in the experiments with real data we set \(p\sim \mathcal U\{1,2,\dots,10\}\) to allow a less informative exploration of the parameter space. Moreover, for simplicity we choose to scale the regressors and response to have standard deviation unity in the inference step, but then convert the results back to the original scale for prediction. This allows us to set a reasonable value of \(\eta^2=25\) in the weakly informative prior of \(\beta_j\) (see~\eqref{eq:prior-linear}). Lastly, the metric used to evaluate the performance is the Root Mean Square Error (RMSE).

\subsubsection*{Simulated data sets}

In Figure~\ref{fig:reg_rkhs} we see the results for the RKHS response. This is our baseline and the most favorable case for us, as the underlying model coincides with our assumed model. Indeed, we can see that in most instances our algorithms are the ones with lower RMSE, as expected, and there is not much variance among our different approaches to prediction, at least in the one-stage methods. We even manage to consistently beat the \textit{lasso} finite-dimensional regression mechanism, which can select all 100 components for its model, versus our 10 components at most. This is an indicator that our models make a more efficient selection of variables, encapsulating more information with fewer components and justifying our partiality to parsimonious models.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=.95\textwidth]{reg_rkhs}
  \caption{Mean and standard error of RMSE of predictors (lower is better) for 10 runs with GP regressors, one on each column, that obey an underlying linear RKHS model.}\label{fig:reg_rkhs}
\end{figure}

Figure~\ref{fig:reg_l2} shows the results for an underlying \(L^2\)-model, which would be our direct competitor and a more representative test for our Bayesian model. In this case the outcome is satisfactory, as for the most part our models are on a par with the rest, even surpassing other methods that were designed with the \(L^2\)-model in mind. Especially interesting is the comparison with the standard \(L^2\) functional regression (\textit{flin} in the graphics), which we outperform most of the time.

\begin{figure}[ht!]
  \vspace{2em}
  \centering
  \includegraphics[width=.95\textwidth]{reg_l2}
  \caption{Mean and standard error of RMSE of predictors (lower is better) for 10 runs with GP regressors, one on each column, that obey an underlying linear \(L^2\)-model.}\label{fig:reg_l2}
\end{figure}

\subsubsection*{Real data}

Figure~\ref{fig:reg_real} depicts the results for the real data sets, where we can see that the performance of our one-stage methods is about the same as that of the comparison methods. However, our variable selection methods are somewhat worse that the reference methods, although not by a wide margin. We have to bear in mind that real data is more complex and noisy than simulated data, and it is possible that after a suitable pre-preprocessing we would have obtained better results. Nonetheless, our goal was to perform a general comparison with a uniform methodology, without focusing too much on the specifics of any particular data set. On another note, we see that some of our Bayesian models have a higher standard deviation, partly because there is an intrinsic randomness in the sampling mechanism, and it can be the cause of the occasional worse performance. In relation to this, we observe that the methods that use the trimmed mean as a summary statistic tend to have a worse score, as this statistic is more sensitive to outliers than the median or the mode, even with the 10\% trimming.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=.8\textwidth]{reg_real}
  \caption{Mean and standard error of RMSE of predictors (lower is better) for 10 runs with real data sets, one on each column.}\label{fig:reg_real}
\end{figure}

\subsection{Functional logistic regression}\label{sec:experiments-logistic}

In this case we set the same prior distribution for \(p\) as in the linear case, but now in the fitting phase the regressors are scaled to have standard deviation 0.5 to accommodate the priors in~\eqref{eq:prior-logistic}. We use the standard threshold of 0.5 to convert probabilities to class labels, and the performance metric is the accuracy, i.e., the rate of correctly predicted samples.

\subsubsection*{Simulated data sets}

First, in Figure~\ref{fig:clf_rkhs} we see the results for the GP regressors with a logistic RKHS response. Our models perform fairly well in this advantageous situation, and even more so in the one-stage case. We again improve the results of other models that can select far more components than our self-imposed limit of 10. Moreover, whenever our two-stage methods score lower, the differences account for only a couple of misclassified samples at worst.

Subsequently, Figure~\ref{fig:clf_l2} shows that in the \(L^2\) scenario the results are again promising, since our models score consistently on or above the mean of the reference models, and in many instances exceed most of them. In this case we also beat the alternative functional logistic regression method (\textit{flog}). In addition, in this situation the overall accuracy of all methods is poor (around 60\%), so this is indeed a difficult problem in which even small increases in accuracy are relevant.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=.925\textwidth]{clf_rkhs}
  \caption{Mean and standard error of accuracy of classifiers (higher is better) for 10 runs with GP regressors, one on each column, that obey an underlying logistic RKHS model.}\label{fig:clf_rkhs}
\end{figure}

\begin{figure}[ht!]
  \vspace{2em}
  \centering
  \includegraphics[width=.925\textwidth]{clf_l2}
  \caption{Mean and standard error of accuracy of classifiers (higher is better) for 10 runs with GP regressors, one on each column, that obey an underlying logistic \(L^2\)-model.}\label{fig:clf_l2}
\end{figure}

\subsubsection*{Real data}

As for the real data sets, in Figure~\ref{fig:clf_real} we see positive results in general, obtaining in some cases accuracies well above the mean of the reference models. In particular, the weighted sum methods tend to have slightly better results than the MAP methods, which is a trend that was also present in the simulated data sets and in the linear regression experiments. This was somewhat expected, since the weighted predictions use the full range of information available for prediction.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=.71\textwidth]{clf_real}
  \caption{Mean and standard error of accuracy of classifiers (higher is better) for 10 runs with real data sets, one on each column.}\label{fig:clf_real}
\end{figure}


\section{Conclusion}\label{sec:conclusion}

We have introduced a natural and computationally feasible way of integrating Bayesian inference into functional regression models, by means of a RKHS approach that simplifies the inherently hard task of setting a prior distribution on an infinite-dimensional space. The proposed RKHS formulation gives a common framework to all finite-dimensional models based on linear combinations of the marginals of the underlying process, establishing a solid theoretical foundation for these popular points-of-impact models while retaining a functional viewpoint. Our approximation relies on simpler functional parameters, thus enhancing interpretability and ease of implementation, and also enables meaningful model comparisons across different dimensions and facilitates the analysis of the effects of local characteristics of the process.

We have also proved posterior consistency results that ensure the coherence and correctness of the Bayesian methods we developed. These kinds of results have in other contexts more intricate and restrictive conditions to arrive at essentially the same conclusions as we did, but again the introduction of RKHS's is the key point to greatly simplifying them. On the one hand, we have successfully adapted the methodology recently introduced in \citet{miller2023consistency}---originally intended for mixture models---to the fundamentally different scenario of functional regression, obtaining ``Lebesgue''-almost sure consistency for both the model parameters and the unknown number of components. On the other hand, we have derived alternative consistency results and optimal contraction rates through a more sophisticated approach based on Schwartz's theorem, which is the standard tool in nonparametric consistency problems. In addition, it is worth mentioning that the theorems we have proved are applicable to a wide range of prior distributions, including many that are rather simple and hence easier to work with.

Lastly, we have presented numerical evidence that supports the proposed Bayesian methodology and its predictive performance with simulated and real data sets. Thanks to our RKHS formulation, we can effectively leverage the capabilities of RJMCMC samplers and integrate the unknown number of components into the Bayesian procedure, which to our knowledge is a novel application in FDA. Moreover, a key finding in our empirical studies is that a relatively low number of components is sufficient to obtain good results. This practical side of our work showcases that the Bayesian prediction methods we constructed are competitive against several non-cherry-picked frequentist techniques, especially those based on the usual \(L^2\)-models, while still remaining viable implementation-wise. Of course, our proposed model is not without limitations, and we are not suggesting that the \(L^2\) perspective should be abandoned, but merely offering a simple, theoretically-backed alternative that can perform better in many situations.


  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %% Acknowledgments
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

  {\footnotesize
  \subsubsection*{Acknowledgments}
    The authors would like to acknowledge the valuable computational resources provided by the Centro de Computación Científica-Universidad Autónoma de Madrid (CCC-UAM). This research was partially supported by grants PID2023-148081NB-I00 and PRE2020-095147 of the Spanish Ministry of Science and Innovation (MCIN), co-financed by the European Social Fund (ESF). J. R. Berrendero and A. Cuevas also acknowledge financial support from grant CEX2023-001347-S funded by MICIU/AEI/10.13039/501100011033.
    }


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% References
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliography{bibliography}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Appendices
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\appendix
\counterwithin{theorem}{section} % redefine numbering
\input{paper-supplement.tex}


\end{document}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\typeout{get arXiv to do 4 passes: Label(s) may have changed. Rerun} % arxiv directive
