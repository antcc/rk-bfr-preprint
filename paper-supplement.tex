%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Copyright (c) 2024 The authors
%
% This work is licensed under a
% Creative Commons Attribution-ShareAlike 4.0 International License.
%
% You should have received a copy of the license along with this
% work. If not, see <http://creativecommons.org/licenses/by-sa/4.0/>.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Supplementary material
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Model choice and implementation details}\label{app:model-choice}

\subsection{Posterior distributions}\label{app:posterior}

For the posterior distributions in our Bayesian formulation, we only compute a function proportional to their log-density, since that is enough for a MCMC algorithm to work. Consider the parameter vector \((p, \theta_p)\), where \(\theta_p=(b_p, \tau_p, \alpha_0, \sigma^2)\) with \(b_p=(\beta_1, \dots, \beta_p)\) and \(\tau_p = (t_1,\dots, t_p)\). Recall that we have a labeled data set of independent observations from \((X, Y)\), denoted by \(\mathcal D_n =\{(X_i, Y_i): i=1,\dots, n\}\). A standard algebraic manipulation in the posterior expression using Bayes' formula yields the following results.

\begin{proposition} Under the linear RKHS model and the prior distribution in Section~\ref{sec:rkhs-linear-model}, the log-posterior distribution up to an additive constant is
  \begin{align*}
  \log\pi(p, \theta_p\mid \mathcal D_n) \propto {} & -\frac{1}{2}\left(\frac{\|b_p\|^2}{\eta^2} - \frac{\|\bm{Y} - \alpha_0\bm{1} + \mathcal X_{\tau_p} b_p\|^2}{\sigma^2} \right) - (n+2)\log \sigma\\
   &- p\log \eta - \frac{p}{2}\log(2\pi) + \log \pi(p),
  \end{align*}
  where \(\bm Y=(Y_1,\dots,Y_n)^T\), \(\bm{1}\) is an \(n\)-dimensional vector of ones, and \(\mathcal X_{\tau_p}\) is the data matrix \((X_i(t_j))_{i,j}\), for \(i=1,\dots,n\) and \(j=1,\dots,p\).
\end{proposition}

\begin{proposition}
  Under the logistic RKHS model and the prior distribution in Section~\ref{sec:rkhs-logistic-model}, the log-posterior distribution up to an additive constant is 
  \begin{align*}
    \log \pi(p, \theta_p \mid \mathcal D_n) \propto {} & \sum_{i=1}^n \left[Y_i \left(\alpha_0 + \dotprod{X_i}{\alpha}_K\right) - \log\left(1 + \exp\left\{\alpha_0 +\dotprod{X_i}{\alpha}_K\right\}\right) \right]\\
     & - 3\sum_{j=1}^p \log\left(4\beta_j^2 + 125\right) + p \left[\frac{15}{2}\log 5 + 4 \log 2 - \log (3\pi)\right]\\
     & - \log\left(100 + \alpha_0^2\right) + \log \pi(p),
  \end{align*}
  where \(\dotprod{X_i}{\alpha}_K = \sum_{j=1}^p \beta_j X_i(t_j)\).
\end{proposition}

The discrete distribution \(\pi(p)\) used in the experiments is either uniform in \(\{1,\dots, 10\}\), with density \(\pi_{\text{uniform}}(p)=1/10\), or a Poisson distribution with rate parameter \(\lambda=3\) truncated to \(\{1,\dots, 10\}\), with density
\[
\pi_{\text{Poisson}}(p) = \frac{3^p}{Cp!}, \quad p\in\{1,\dots,10\},
\]
where the normalization constant is \(C = \sum_{k=1}^{10} \frac{3^k}{k!}\).

\subsection{Label switching}\label{app:label-switching}

A well-known issue that arises when using MCMC methods in mixture-like models such as the one proposed in this work is \textit{label switching}, which in short refers to the non-identifiability of the components of the model caused by their interchangeability. In our case, this happens because the likelihood and the prior are symmetric with respect to the ordering of the parameters \(b\) and \(\tau\), i.e., \(\pi(Y|X,\theta)=\pi(Y|X, \nu(\theta))\) for any permutation \(\nu\) that rearranges the indices \(j=1,\dots, p\). Thus, since the components are arbitrarily ordered, they may be inadvertently exchanged from one iteration to the next in a MCMC algorithm. This can cause nonsensical answers when summarizing the marginal posterior distributions of the parameters to perform inference, as different labelings might be mixed on each component \citep{stephens2000dealing}. This is primarily the reason why we do not directly use summaries of the posterior distribution of the individual parameters in our prediction methods.

Moreover, the use of trans-dimensional samplers exacerbates this problem, since the change in dimension can further disrupt the internal ordering of the components \citep[see][]{roodaki2014relabeling}. However, this phenomenon is perhaps surprisingly a condition for the convergence of the MCMC method: as pointed out by many authors \citep[e.g.][]{celeux2000computational}, a lack of switching would indicate that not all modes of the posterior distribution were being explored by the sampler. For this reason, many ad-hoc solutions revolve around post-processing and relabeling the samples to eliminate the switching effect, but they generally do not prevent it from happening in the first place.

The most straightforward solutions consist on imposing an artificial identifiability constraint on the parameters to break the symmetry of their posterior distributions; see \citet{jasra2005markov} and references therein. A common approach that seems to work well is to simply enforce an ordering in the parameters in question, which in our case would mean requiring for example that \(\beta_i < \beta_j\) for \(i < j\), or the analogous with the times in \(\tau\). We have implemented a variation of this method described in \citet{simola2021approximate}, which works by post-processing the samples and relabeling the components to satisfy the order constraint mentioned above, choosing either \(b\) or \(\tau\) depending on which set of ordered parameters would produce the largest separation between any two of them (suitably averaged across all iterations of the chains). This is an area of ongoing research, and thus there are other, more complex relabeling strategies, both deterministic and probabilistic. A summary of several such methods can be found for example in \citet{sperrin2010probabilistic} or \citet{rodriguez2014label}.

\subsection{Affine-invariant ensemble samplers}\label{app:ensemble-sampler}

An interesting and often desirable property of regular MCMC sampling algorithms is that they be \textit{affine-invariant}, which informally means that they regard two distributions that differ in an affine transformation, say \(\pi(x)\) and \(\pi_{A, b}(Ax + b)\), as equally difficult to sample from. This is useful when one is working with very asymmetrical or skewed distributions, for an affine transformation can turn them into ones with simpler shapes. Generally speaking, a MCMC algorithm can be described through a function \(R\) as \(\Lambda(t+1)=R(\Lambda(t), \xi(t), \pi)\), where \(\Lambda(t)\) is the state of the chain at instant \(t\), \(\pi\) is the objective distribution, and \(\xi(t)\) is a sequence of i.i.d.\ random variables that represent the random behavior of the chain. With this notation, the affine-invariance property can be characterized as \(R(A\lambda+b, \xi(t), \pi_{A,b}) = AR(\lambda, \xi(t), \pi) + b\), for all \(A,b\) and \(\lambda\), and almost all \(\xi(t)\). This means that if we fix a random generator and run the algorithm twice, one time using \(\pi\) and starting in \(\Lambda(0)\) and a second time using \(\pi_{A,b}\) with initial point \(\Gamma(0)=A\Lambda(0)+b\), then \(\Gamma(t)=A\Lambda(t)+b\) for all \(t\). In \citet{goodman2010ensemble} the authors consider an ensemble of samplers with the affine invariance property. Specifically, they work with a set \(\Lambda=(\Lambda_1, \dots, \Lambda_L)\) of \textit{walkers}, where \(\Lambda_l(t)\) represents an individual chain at time \(t\). At each iteration, an affine-invariant transformation is used to find the next point, which is constructed using the current values of the rest of the walkers (similar to Gibb's algorithm), namely the \textit{complementary ensemble}
\[
  \Lambda_{-l}(t) = \{\Lambda_1(t+1), \dots, \Lambda_{l-1}(t+1), \Lambda_{l+1}(t), \dots, \Lambda_L(t)\}, \quad l=1,\dots, L.
\]

To maintain the affine invariance and the joint distribution of the ensemble, the walkers are advanced one by one following a Metropolis-Hastings acceptance scheme. The authors consider mainly two types of moves.

\begin{description}
  \item[Stretch move.] For each walker \(1\leq l \leq L\) another walker \(\Lambda_j \in \Lambda_{-l}(t)\) is chosen at random, and the proposal is constructed as
  \[
    \Lambda_l(t) \to \Gamma = \Lambda_j + Z(\Lambda_l(t) - \Lambda_j),
  \]
  where \(Z \stackrel{i.i.d.}{\sim} g(z)\) satisfying the symmetry condition \(g(z^{-1})=zg(z)\). In particular, the suggested density is
  \[
    g_a(z) \propto \begin{cases}
      \frac{1}{\sqrt{z}}, & \text{if } z \in [a^{-1}, a], \\
      0,                  & \text{otherwise.}
    \end{cases}, \quad a > 1.
  \]
  Supposing \(\R^p\) is the sample space, the corresponding acceptance probability (chosen so that the detailed balance equations are satisfied) is:
  \[
    \alpha = \min\left\{1, \ Z^{p-1}\frac{\pi(\Gamma)}{\pi(\Lambda_l(t))}\right\}.
  \]

  \item[Walk move.] For each walker \(1\leq l \leq L\) a random subset \(S_l \subseteq \Lambda_{-l}(t)\) with \(|S_l| \geq 2\) is selected, and the proposed move is
  \[
    \Lambda_l(t) \to \Gamma = \Lambda_l(t) + W,
  \]
  where \(W\) is a normal distribution with mean \(0\) and the same covariance as the sample covariance of all walkers in \(S_l\). The acceptance probability in this case is just the Metropolis ratio, namely \(\alpha=\min\{1, \pi(\Gamma)/\pi(\Lambda_l(t))\}\).
\end{description}

From a computational perspective, the Python library \textit{emcee} \citep{foreman2013emcee}  provides a parallel implementation of this algorithm. The idea is to divide the ensemble \(\Lambda\) into two equally-sized subsets \(\Lambda^{(0)}\) and \(\Lambda^{(1)}\), and then proceed on each iteration in the following alternate fashion:
\begin{enumerate}
  \item Update \textit{all} walkers in \(\Lambda^{(0)}\) through one of the available moves explained above, using \(\Lambda^{(1)}\) as the complementary ensemble.
  \item Use the new values in \(\Lambda^{(0)}\) to update \(\Lambda^{(1)}\).
\end{enumerate}
In this way the detailed balance equations are still satisfied, and each of the steps can benefit from the computing power of an arbitrary number of processors (up to \(L/2\)).  

\subsection{RJMCMC implementation}

For the computational implementation of our Bayesian prediction pipeline we chose the Python library \textit{Eryn} \citep{karnesis2023eryn}. This package is a general-purpose suite of MCMC methods which is reliable, easy to use and performs well in a wide class of problems. It is an evolution of the \textit{emcee} library mentioned in Appendix~\ref{app:ensemble-sampler} that implements affine-invariant ensemble samplers, with a few key improvements:

\begin{description}
  \item[Reversible jump sampling.] The main reason for choosing this library is that it implements a reversible jump MCMC sampling scheme, which is an essential element in our proposed models. It allows trans-dimensional sampling for posterior approximation, letting the user select the likelihood, prior and proposal distributions, and providing a great level of control over the details.

  \item[Parallel tempering.] This is a mechanism to increase the efficiency with which the sampler explores the parameter space. The basic idea is to consider a set of Markov chains in parallel, each one sampling from a transformed posterior distribution \(\pi_T(p, \theta_p|X,Y) = \pi(Y|X, p, \theta_p)^{1/T}\pi(p, \theta_p)\), where \(T\geq 1\) is a temperature parameter. In the words of \citet{karnesis2023eryn}, \textit{``intermediate temperatures `smooth out' the posterior by reducing the contrast between areas of high and low likelihood''}. In practice, these chains periodically exchange information, with the swaps controlled by an acceptance probability that maintains detailed balance, and ultimately we are only interested on the cold chain (\(T=1\)).

  \item[Multiple try.] Since the trans-dimensional moves are harder to manage and generally give a low acceptance rate, this library allows the proposal of several candidates for a given move, using a weight function to assign them a relative importance, and then choosing from them with probability given by the normalized weights. This naturally increases the computational cost, but it often produces better results.
\end{description}

Another advantage of this ensemble approach, apart from the property of affine-invariance, is that it only requires the specification of a few hyperparameters irrespective of the underlying dimension. This contrasts to, say, the \(\mathcal O(N^2)\) degrees of freedom corresponding to the covariance matrix of an \(N\)-dimensional jump distribution in Metropolis-Hastings. We already covered the prior, likelihood and posterior distributions used in Section~\ref{sec:methodology} and in Appendix~\ref{app:posterior}. We give below an overview of other integral parts of our RJMCMC method with some implementation details.

\subsubsection*{Initial values}

We need to specify the initial values for the parameters of all the chains in our sampler. In general, we set these values by sampling from the prior distribution of the corresponding parameters. However, in the linear case the prior on \((\alpha_0, \sigma^2)\) is improper, so we use ad-hoc weakly informative distributions for the initial values instead. For \(\alpha_0\) we consider a normal distribution with mean \(0\) and standard deviation \(10|\bar{Y}_{\text{scaled}}|\), where \(Y_{\text{scaled}}\) is the version of the original data scaled to have standard deviation unity. For \(\sigma^2\) we use an inverse-gamma distribution with shape parameter \(a=2\) and scale parameter \(b=\hat\sigma^2_Y/\operatorname{Var}(Y)\), where \(\hat \sigma^2_Y\) is a rough estimate of an acceptable error in the scale of \(Y\). This estimation is done is practice as two order of magnitudes less than \(|\bar{Y}|\).

\subsubsection*{Moves}

As with any MCMC method, the Markov chains are advanced iteratively through a set of moves. For the \textit{in-model} moves (that is, moves that do not change the dimension), we divide our parameter vector in two parts:

\begin{description}
  \item[Stretch move.] For the parameters \(\alpha_0\) and \(\sigma^2\), which are common to all sub-models, we use the stretch move explained in Appendix~\ref{app:ensemble-sampler}. 
  \item[Group stretch move.] For \(b\) and \(\tau\), we use a variant of the stretch move known as the group stretch move, which was designed as an extension of the original move that can handle reversible jump setups. The main difference is that the random walker \(\Lambda_j\) used to advance the chain is selected from a stationary group that does not change for several iterations (see Section~3.1 in \citealp{karnesis2023eryn} for more information about this move).
\end{description}
In both moves above, the key scaling parameter \(a\) starts with a value of \(2\), and is changed dynamically to guide the sampler towards an acceptance rate of about \(20\%-30\%\), which is within the range usually recommended in the literature \citep[e.g.][]{rosenthal2011optimal}.

For the \textit{between-model} moves (those which change the dimension), we set an equal probability of births and deaths, except on the end points of the range of \(p\). Specifically, \(b_{p,p+1}=d_{p, p-1}=0.5\) if \(1<p<10\), and \(b_{1,2}=d_{10,9}=1\). Note that these are only the probabilities to propose the corresponding move, which will be accepted or rejected according to the acceptance formula in Section~\ref{sec:rjmcmc}. When the birth of a new component is proposed, we use the prior distribution of \(b\) and \(\tau\) to generate the new values, and when a death is proposed, the method selects an existing component at random as a candidate for deletion.

\subsubsection*{Hyperparameters}

Other relevant hyperparameters include the burn-in period for the chains, which is the number of initial samples discarded, the number of actual steps, the number of chains, and the number of temperatures for parallel tempering. In the experiments we use 64 chains and 10 different temperatures, and run them for 5000 iterations in total, discarding the first 4000 as burn-in. Moreover, when the prior on \(p\) is uniform (i.e.~in the experiments with real data) we activate the multiple try scheme and set the number of tries to \(2\). Lastly, a computational decision we made is working with \(\log \sigma\) instead of \(\sigma^2\) so that the domain of this parameter is an unconstrained space, which is a widespread recommendation that helps increase the sampling efficiency.


\newpage
\section{More details on posterior consistency}\label{app:theory}

There are some technicalities to take into account in the theoretical exposition of posterior consistency in Section~\ref{sec:consistency}, especially pertaining to measure theory. For example, to justify the existence of regular conditional distributions such as \(\bm\theta|X_1,\dots,X_n\), one should see Theorem 10.2.1 and Theorem 10.2.2 in \citet{dudley2002real}, which guarantee they are well-defined provided that the underlying spaces are sufficiently regular. Another issue is the measurability of the mapping \(\theta \mapsto P_\theta(X,Y)(A)\), which is assumed in the proof of our consistency results. We illustrate how this can be proved for example in the linear case, under the additional condition of sample continuity, which is arguably not a very restrictive condition in real-life scenarios.

\begin{proposition}
  If the process \(X\) is sample-continuous (i.e.\ the trajectories are continuous functions), then the mapping \(\theta \mapsto P_\theta(X,Y)(A)\) is measurable for every measurable set \(A \subseteq \mathcal X \times \mathcal Y\).
\end{proposition}

\begin{proof}
  We start by checking that \(\theta \mapsto P_\theta(Y|X)(A_1)\) is measurable for every measurable set \(A_1\subseteq \mathcal Y\). Indeed, consider the function \(F(y,\theta)=f(y|X,\theta)\bm{1}_{A_1}(y)\), where \(f(\cdot|X,\theta)\) is the density of the normal distribution \(\mathcal N(\alpha_0 + \sum_j \beta_j X(t_j), \sigma^2)\) and \(\bm{1}_{A_1}\) is the indicator function of the set \(A_1\). It is easy to see that \(F(y,\theta)\) is jointly measurable (it is in fact continuous) if \(X\) has continuous sample paths. Then, by Tonelli's theorem \citep[e.g.][Theorem~2.37]{folland1999real}, the function
  \[
    \theta \mapsto \int_{\mathcal Y} f(y|X,\theta)\bm{1}_{A_1}(y)\, dy = \E_{P_\theta(Y|X)}\left[\bm{1}_{A_1}(Y)\right] = P_\theta(Y|X)(A_1)
  \]
  is measurable. Now, if \(A\) is a measurable subset of \(\mathcal X \times \mathcal Y\), we have the decomposition
  \[
    P_\theta(X,Y)(A)  = \int_{\mathcal X} P_\theta(Y|X=x)(A_x) dP(X)(x),
  \]
  where \(A_x=\{y\in\mathcal Y: (x,y) \in A\}\) is the \(x\)-section of \(A\). We just saw that \(\theta \mapsto P_\theta(Y|X=x)(A_x)\) is measurable, and thus \(\theta \mapsto P_\theta(X,Y)(A)\) is measurable, since integration respects measurability.
\end{proof}

We also examine more closely the claim that our posterior consistency result can be refined so that the set where consistency may fail is ``small'' in some sense. Using the notation introduced in Section~\ref{sec:consistency}, and considering the Lebesgue measure on \(\Theta_p\), denoted by \(\lambda_p\), we need to impose an additional condition:

\begin{condition}[Absolute continuity of \(\lambda_p\)] Under the model implied by~\eqref{eq:model-linear}, for all \(p\in\N\):\label{cond:condition-lebesgue}
  \begin{enumerate}[label=(\roman*)]
    \item \(\Pi(\mathcal P = p) > 0\).\label{cond:condition-lebesgue-1}
    \item \(\sum_{\nu\in S_p} \Pi(\bm \theta[\nu] \in B|\mathcal P = p) = 0\) implies \(\lambda_p(B)=0\), for all \(B\subseteq \Theta_p\) measurable.\label{cond:condition-lebesgue-2}
  \end{enumerate}
\end{condition}

The first condition is a somewhat technical requirement. The second condition is met, for example, if \(\bm \theta|p\) has a density with respect to the Lebesgue measure that is invariant to permutations of the component labels and positive on all of \(\Theta_p\). With these prerequisites in mind, denoting by \(\lambda\) the extension of the Lebesgue measure to \(\Theta\), we get the announced result:

\begin{proposition}\label{th:consistency-theorem-2}
  Assume Condition~\ref{cond:condition-ident} and Condition~\ref{cond:condition-lebesgue}. Then the conclusion of Theorem~\ref{th:consistency-theorem} remains valid with \(\lambda(\Theta \setminus \Theta_*)=0\).
\end{proposition}

\begin{proof}
  Define \(\Theta_*\) as in the proof of Theorem~\ref{th:consistency-theorem}. Recall that \(\Pi(\Theta_*)=1\), and observe that 
  \[
  0=\Pi(\Theta \setminus \Theta_*) = \sum_{p=1}^\infty \Pi(\Theta_p \setminus \Theta_*|\mathcal P = p)\Pi(\mathcal P = p).
  \]
Since \(\Pi(\mathcal P = p)> 0\) for all \(p\in\N\) by Condition~\ref{cond:condition-lebesgue}-\ref{cond:condition-lebesgue-1}, we have \(\Pi(\Theta_p\setminus \Theta_*|\mathcal P = p) = 0\) for all \(p\). Now, for \(\nu \in S_p\), let \(\mu^\nu_p\) be the distribution of \(\bm\theta[\nu]|\mathcal P=p\) under the model. Note that \((\Theta_p\setminus \Theta_*)[\nu] = \Theta_p \setminus \Theta_*\) by definition of \(\Theta_*\), and thus for all \(\nu\in S_p\) it holds that
\begin{equation}\label{eq:measure-zero}
  \mu^\nu_p (\Theta_p \setminus \Theta_*) = \mu_p^{\text{id}} (\Theta_p \setminus \Theta_*) = \Pi (\Theta_p \setminus \Theta_*\mid\mathcal P = p) =0.
\end{equation}
Lastly, Condition~\ref{cond:condition-lebesgue}-\ref{cond:condition-lebesgue-2} means that \(\lambda_p \ll \sum_{\nu\in S_p}\mu^\nu_p\), where \(\ll\) denotes absolute continuity, and this together with~\eqref{eq:measure-zero} implies that \(\lambda_p(\Theta_p \setminus \Theta_*)=0\). But this is valid for all \(p\in\N\), so we can conclude, as claimed, that \(\lambda(\Theta\setminus \Theta_*) = \sum_{p=1}^\infty \lambda_p(\Theta_p\setminus \Theta_*)=0\).
\end{proof}


\newpage
\section{Experimentation}\label{app:experiments}

\subsection{Overview of data sets and comparison algorithms}\label{app:data-sets}

To generate the simulated data sets for the comparison experiments in Section~\ref{sec:results}, we used four types of Gaussian process regressors commonly employed in the literature, each with a different covariance function:
\begin{description}
  \item [BM.] A Brownian motion, with kernel \(K_1(t,s)=\min\{t,s\}\).
  \item [fBM.] A fractional Brownian motion, with kernel \(K_2(t,s)=1/2(s^{2H} + t^{2H} - |t-s|^{2H})\) and Hurst parameter \(H=0.8\).
  \item [O-U.] An Ornstein-Uhlenbeck process, with kernel \(K_3(t,s)=e^{-|t-s|}\).
  \item [Gaussian.] A Gaussian process with a squared exponential kernel (also known as Gaussian kernel), namely \(K_4(t,s)=e^{-(t-s)^2/2\nu^2}\), where \(\nu=0.2\).
\end{description}

For the comparison algorithms themselves, we considered several frequentist methods which were selected among popular ones in FDA and machine learning in general. As specified in the main article, variable selection and dimensionality reduction methods are part of a pipeline followed by a standard multiple regression technique. In the linear regression case, we chose the following algorithms:

\begin{description}
  \item [PLS1.] Partial least squares regression \citep[e.g.][]{abdi2010partial}.
  \item[Lasso.] Linear least squares with \(l^1\) regularization.
  \item [FPLS and FPLS1.] Functional PLS regression through basis expansion, as in \citet{aguilera2010using}.
  \item [FLin.] Standard \(L^2\) functional linear regression model with fixed basis expansion and regularization.
  \item [APLS.] Functional partial least squares regression proposed by \citet{delaigle2012methodology}.
  \item [PLS.] Partial least squares for dimension reduction.
  \item [PCA.] Principal component analysis for dimension reduction.
  \item[Manual.] Dummy variable selection method with a pre-specified number of components (equispaced on \([0, 1]\)).
  \item [FPCA.] Functional principal component analysis.
  \item [Ridge.] Linear least squares with \(l^2\) regularization. This is used as the multiple regression that follows variable selection or dimensionality reduction methods.

\end{description}

In the logistic regression case, all the variable selection and dimension reduction techniques from above were also considered, with the addition of the following classification methods:

\begin{description}
  \item [QDA.] Quadratic discriminant analysis.
  \item [MDC.] Maximum depth classifier \citep[e.g.][]{ghosh2005maximum}.
  \item [Log.] Standard multiple logistic regression with \(l^2\) regularization. This is used as a one-stage method and also as the multiple regression that follows variable selection or dimensionality reduction methods.
  \item [LDA.] Linear discriminant analysis.
  \item [FNC.] Functional nearest centroid classifier with the \(L^2\)-distance.
  \item [FLog.] Functional RKHS-based logistic regression algorithm proposed in \citet{berrendero2023functional}.
  \item [FLDA.] Implementation of the functional version of linear discriminant analysis proposed in \citet{preda2007pls}.
  \item [FKNN.] Functional K-nearest neighbors classifier with the \(L^2\)-distance.
  \item [RKVS.] RKHS-based variable selection and classification method proposed in \citet{berrendero2018use}.
  \item [APLS+NC.] Functional PLS used as a dimension reduction method, as proposed in \citet{delaigle2012achieving} in combination with the nearest centroid (NC) algorithm.
\end{description}

The main hyperparameters of all these algorithms were selected by 10-fold cross-validation, and for those that have a number of components to select, we set 10 as the maximum value so that comparison with our own methods are fair. In particular, regularization parameters are searched among 20 values in the logarithmic space \([10^{-4}, 10^4]\), the number of basis elements for cubic spline bases is in \(\{4,5,\dots,10\}\), the number of basis elements for Fourier bases is one of \(\{1,3,5,7,9\}\), and the number of neighbors in the KNN classifier is in \(\{3,5,7,9,11,13\}\). Most algorithms have been taken from the libraries \textit{scikit-learn} \citep{pedregosa2011scikit} and \textit{scikit-fda} \citep{ramos2024scikit}, the first oriented to machine learning in general and the second to FDA in particular. However, some methods were not found in these packages and had to be implemented from scratch. This is the case of the FLDA, FPLS and APLS methods, which we coded following the corresponding articles.

\subsection{Simulations with non-Gaussian regressors}\label{app:non-gp}

We performed an additional set of experiments in linear and logistic regression in which the regressors are not Gaussian processes (GPs), to see if our methods would hold up in this case. These experiments where run in the same conditions as those reported in the main article.

\subsubsection*{Functional linear regression}

We use a geometric Brownian motion (GBM) as the regressor variable, defined as \(X(t)=\exp\{{\operatorname{BM}(t)}\}\), where \(\operatorname{BM}(t)\) is a standard Brownian motion. In this case we consider two data sets, one with a RKHS response and one with an \(L^2\) response, both with the same parameters as in the corresponding data sets in Section~\ref{sec:results}. The comparison results can be seen in Figure~\ref{fig:reg_non_gp}: in this case our methods still get better results under the RKHS model, while the results under the \(L^2\)-model are essentially the same, which is a positive outcome.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=.48\textwidth]{reg_non_gp}
  \caption{Mean and standard error of RMSE of predictors (lower is better) for 10 runs with GBM regressors. In the first column the response obeys a linear \(L^2\)-model, while in the second columns it follows a linear RKHS model.}\label{fig:reg_non_gp}
\end{figure}

\subsubsection*{Functional logistic regression}

We consider a ``mixture'' situation in which we combine regressors from two different GPs with equal probability and label them according to their origin. First, we consider a homoscedastic case to distinguish between a standard Brownian motion and a Brownian motion with a mean function that is zero until \(t=0.5\), and then becomes \(m(t)=0.75t\). Secondly, we consider a heteroscedastic case to distinguish between a standard Brownian motion and a Brownian motion with variance 2, that is, with kernel \(K(t,s)=2\min\{t,s\}\). 

Figure~\ref{fig:clf_non_gp} shows that our classifiers perform better than most comparison algorithms in both cases. The differences are most notable in the homoscedastic case, and in the heteroscedastic case the overall accuracy is low. Incidentally, this heteroscedastic case of two zero-mean Brownian motions has a special interest, since it can be shown that the Bayes error is zero in the limit of dense monitoring (i.e.\ with an arbitrarily fine measurement grid), a manifestation of the ``near-perfect'' classification phenomenon analyzed for example in \citet{torrecilla2020optimal}. Our results are in line with the empirical studies of this article, where the authors conclude that even though the asymptotic theoretical error is zero, most classification methods are suboptimal in practice (possibly due to the high collinearity of the data), with the notable exception of PCA+QDA.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=.48\textwidth]{clf_non_gp}
  \caption{Mean and standard error of accuracy of classifiers (higher is better) for 10 runs with a mix of regressors coming from two different GPs and labeled according to their origin. In the first column we try to separate two Brownian motions with the same mean but different variance, while in the second column we discriminate between two Brownian motions with different mean functions but the same variance.}\label{fig:clf_non_gp}
\end{figure}

\subsection{Analysis and validation of a model}\label{app:validation}

We give an example through a series of visual representations of how one would analyze the outcome of our Bayesian methods. This is a preliminary step that comes before prediction; the idea is to validate the model and make sure that the resulting samples from the posterior are coherent and useful. For this illustration we consider a data set used in the experiments, for example the one with squared exponential GP regressors, and an underlying linear RKHS response given by (Figure~\ref{fig:dataset-linear})
\[
Y=5 - 5X(0.1) + 5X(0.6) + 10X(0.8) + \varepsilon, 
\] 
with \(\varepsilon \sim \mathcal N(0, 0.5)\). We run the sampler for \(3000\) iterations and discard the first \(2000\), with a \(\text{Poisson}(3)\) prior truncated to \(\{1,\dots,5\}\) for \(p\). 

\begin{figure}[ht!]
  \centering
  \includegraphics[width=.64\textwidth]{reg_analysis_dataset}
  \caption{Data set with squared exponential GP regressors and linear RKHS response.}\label{fig:dataset-linear}
\end{figure}

The first thing we do is look at arbitrary samples in the last iteration of a few chains, to check that we get reasonable values. Then, we examine the acceptance rate of all the moves to see that they are not either very low or very high. Lastly, we compute the so-called Gelman-Rubin statistic \citep{gelman1992inference}, which is a quantitative measurement of the convergence of the chains (it should be near \(1\)).

Next we proceed with the visual checks. We can look at the flat posterior distribution of all parameters for all values of \(p\) and all the chains aggregated together (Figure~\ref{fig:flat-posterior-linear}), or visualize the traces of individual parameters for all values of \(p\) (Figure~\ref{fig:trace-plot}). In addition, we can also look at the posterior of the multidimensional parameters for each \(p\) in a sort of triangular configuration (Figure~\ref{fig:triangular-posterior-linear}).

\begin{figure}[ht!]
  \centering
  \includegraphics[width=.65\textwidth]{reg_analysis_flat_posterior}
  \caption{Aggregated posterior distribution of all the samples \(\theta^*_{p_m}\) for all \(m\). Note that the true values of the parameters are essentially recovered.}\label{fig:flat-posterior-linear}
\end{figure}

\begin{figure}[ht!]
  \centering
  \begin{subfigure}[b]{.65\textwidth}
    \centering
    \includegraphics[width=\textwidth]{reg_analysis_trace_1}
  \end{subfigure}
  \begin{subfigure}[b]{.65\textwidth}
    \centering
    \includegraphics[width=\textwidth]{reg_analysis_trace_2}
  \end{subfigure}
  \begin{subfigure}[b]{.65\textwidth}
      \centering
      \includegraphics[width=\textwidth]{reg_analysis_trace_3}
  \end{subfigure}
  \begin{subfigure}[b]{.65\textwidth}
      \centering
      \includegraphics[width=\textwidth]{reg_analysis_trace_4}
  \end{subfigure}
  \begin{subfigure}[b]{.65\textwidth}
      \centering
      \includegraphics[width=\textwidth]{reg_analysis_trace_5}
  \end{subfigure}
  \caption{Posterior distribution (left) and trace (right) of all the \(\beta_j^*\), combined for all \(p\).}\label{fig:trace-plot}
\end{figure}

\begin{figure}[ht!]
  \centering
  \includegraphics[width=.85\textwidth]{reg_analysis_triangular_posterior}
  \caption{Posterior distribution of \(\tau_p^*\) for each \(p\). The most frequent value is \(p=3\), highlighted in red. In this run there were no samples with \(p=1\) or \(p=2\) after burn-in.}\label{fig:triangular-posterior-linear}
\end{figure}

Looking at the traces is useful to check that the chains are mixing well and that they are correctly exploring the parameter space. We can do the same thing with the posterior values of \(p\) (Figure~\ref{fig:trace-p}). Moreover, we can visualize the tempered posterior distribution of \(p\), that is, the posterior distribution of \(p\) for each temperature (Figure~\ref{fig:tempered-posterior-p}).

\begin{figure}[ht!]
  \centering
  \includegraphics[width=.85\textwidth]{reg_analysis_trace_p}
  \caption{Posterior distribution (left) and trace (right) of \(p\). We see that the true number of components is recovered.}\label{fig:trace-p}
\end{figure}

\begin{figure}[ht!]
  \centering
  \includegraphics[width=.85\textwidth]{reg_analysis_tempered_posterior_p}
  \caption{Tempered posterior distribution of \(p\). We are only interested in the cold chain (\(T=1\)), but by allowing different temperatures we increase the exploration of the parameter space, periodically transferring some of this information to the cold chain.}\label{fig:tempered-posterior-p}
\end{figure}

Lastly, we can perform a posterior predictive check (Figure~\ref{fig:pp-check}). This is arguably the most useful visual test for prediction purposes, since we represent the posterior predictive distribution that will be used for inference and prediction. We can do it on the training data or directly on previously unseen regressors. If the sampling has been successful, the posterior predictive distribution should look like a tubular region around the observed data.

\begin{figure}[ht!]
  \centering
  \includegraphics[width=.6\textwidth]{reg_analysis_pp_check}
  \caption{Posterior predictive distribution \(Y|X, \theta_{p_m}^*\) for each individual chain \(m\), along with the mean of all chains and the actual observed data \(Y\).}\label{fig:pp-check}
\end{figure}

\FloatBarrier{}
\newpage

\subsection{Tables of experimental results}\label{app:tables}

Here we present the tables corresponding to the empirical comparison studies in Appendix~\ref{app:non-gp} and in Section~\ref{sec:results}, which show the numerical values that were depicted there graphically. In each case the best and second-best results are shown in \firstcolor{bold} and \secondcolor{blue}, respectively.

\subsubsection*{Functional linear regression}

\begin{table}[htbp!]
  \footnotesize
  \centering
  \rowcolors{2}{}{teal!8}
  \begin{tabular}{lcccc}
    \toprule
    \textbf{Prediction method} & \textbf{BM}                 & \textbf{fBM}                & \textbf{O-U}                & \textbf{Gaussian}           \\
    \midrule
    pls1 & 0.997 (0.051) & 0.730 (0.042) & 1.065 (0.055) & 0.673 (0.045) \\
    lasso & 0.680 (0.044) & 0.688 (0.031) & 0.684 (0.031) & 0.671 (0.042) \\
    fpls1 & 1.607 (0.078) & 0.751 (0.040) & 2.088 (0.099) & 0.670 (0.046) \\
    flin & 1.829 (0.073) & 0.869 (0.037) & 2.388 (0.078) & 0.957 (0.044) \\
    apls & 0.875 (0.053) & 0.704 (0.044) & 1.005 (0.050) & 0.672 (0.045) \\
    w\_pp\_tmean & \firstcolor{0.668 (0.043)} & \firstcolor{0.676 (0.032)} & 0.676 (0.042) & \firstcolor{0.662 (0.040)} \\
    w\_pp\_mode & 0.670 (0.041) & \firstcolor{0.676 (0.030)} & 0.675 (0.041) & \secondcolor{0.664 (0.040)} \\
    w\_pp\_median & \firstcolor{0.668 (0.043)} & \firstcolor{0.676 (0.032)} & 0.676 (0.042) & \firstcolor{0.662 (0.040)} \\
    map\_pp\_tmean & \secondcolor{0.669 (0.042)} & \secondcolor{0.677 (0.031)} & \secondcolor{0.671 (0.045)} & 0.666 (0.039) \\
    map\_pp\_mode & 0.672 (0.041) & 0.680 (0.029) & 0.672 (0.045) & 0.670 (0.042) \\
    map\_pp\_median & 0.670 (0.043) & \secondcolor{0.677 (0.031)} & \firstcolor{0.670 (0.045)} & 0.667 (0.040) \\
    \bottomrule
    \toprule
    pls+r & 0.996 (0.056) & 0.720 (0.036) & 1.065 (0.055) & 0.674 (0.044) \\
    pca+r & 1.521 (0.070) & 0.720 (0.034) & 2.249 (0.095) & 0.673 (0.045) \\
    manual+r & 1.342 (0.130) & 0.717 (0.040) & 1.719 (0.101) & 0.674 (0.042) \\
    fpls+r & 1.607 (0.078) & 0.752 (0.039) & 2.089 (0.098) & \secondcolor{0.669 (0.046)} \\
    fpca+r & 1.512 (0.071) & 0.721 (0.034) & 2.237 (0.096) & 0.673 (0.045) \\
    w\_vs\_tmean+r & \secondcolor{0.795 (0.069)} & 0.697 (0.029) & 0.987 (0.131) & 0.672 (0.037) \\
    w\_vs\_mode+r & \firstcolor{0.668 (0.040)} & \firstcolor{0.678 (0.034)} & 0.667 (0.041) & 0.680 (0.050) \\
    w\_vs\_median+r & \firstcolor{0.668 (0.039)} & 0.681 (0.031) & \secondcolor{0.666 (0.041)} & \firstcolor{0.666 (0.041)} \\
    map\_vs\_tmean+r & \firstcolor{0.668 (0.040)} & 0.747 (0.048) & 0.971 (0.398) & 0.737 (0.093) \\
    map\_vs\_mode+r & \firstcolor{0.668 (0.040)} & \secondcolor{0.679 (0.034)} & 0.668 (0.042) & 0.707 (0.037) \\
    map\_vs\_median+r & \firstcolor{0.668 (0.040)} & 0.695 (0.029) & \firstcolor{0.664 (0.041)} & 0.700 (0.063) \\
    \bottomrule
  \end{tabular}
  \caption{Mean RMSE of predictors (lower is better) for 10 runs with GP regressors, one on each column, that obey an underlying linear RKHS model. The corresponding standard errors are shown between brackets.}
\end{table}
\FloatBarrier{}
\newpage


\begin{table}[htbp!]
  \vspace{1em}
  \footnotesize
  \centering
  \rowcolors{2}{}{teal!8}
  \begin{tabular}{lcccc}
    \toprule
    \textbf{Prediction method} & \textbf{BM}                 & \textbf{fBM}                & \textbf{O-U}                & \textbf{Gaussian}           \\
    \midrule
    pls1 & 0.678 (0.039) & 0.674 (0.030) & 0.682 (0.043) & 0.669 (0.047) \\
    lasso & 0.667 (0.037) & 0.664 (0.035) & 0.659 (0.039) & 0.664 (0.041) \\
    fpls1 & 0.671 (0.041) & 0.671 (0.044) & 0.677 (0.037) & 0.671 (0.048) \\
    flin & 0.666 (0.037) & 0.664 (0.038) & 0.662 (0.038) & \firstcolor{0.661 (0.040)} \\
    apls & 0.673 (0.045) & 0.684 (0.043) & 0.681 (0.043) & 0.674 (0.041) \\
    w\_pp\_tmean & \firstcolor{0.652 (0.037)} & \firstcolor{0.659 (0.039)} & \firstcolor{0.651 (0.037)} & \firstcolor{0.661 (0.039)} \\
    w\_pp\_mode & 0.655 (0.037) & \secondcolor{0.661 (0.038)} & \secondcolor{0.652 (0.037)} & \firstcolor{0.661 (0.039)} \\
    w\_pp\_median & \firstcolor{0.652 (0.037)} & \firstcolor{0.659 (0.039)} & \firstcolor{0.651 (0.037)} & \firstcolor{0.661 (0.039)} \\
    map\_pp\_tmean & \secondcolor{0.654 (0.037)} & \secondcolor{0.661 (0.038)} & 0.655 (0.038) & \firstcolor{0.661 (0.039)} \\
    map\_pp\_mode & 0.655 (0.035) & 0.664 (0.039) & 0.656 (0.037) & \secondcolor{0.662 (0.037)} \\
    map\_pp\_median & \secondcolor{0.654 (0.037)} & \secondcolor{0.661 (0.038)} & 0.655 (0.038) & \firstcolor{0.661 (0.039)} \\
    \bottomrule
    \toprule
    pls+r & 0.667 (0.037) & 0.669 (0.033) & 0.669 (0.037) & \firstcolor{0.665 (0.044)} \\
    pca+r & 0.665 (0.041) & 0.664 (0.041) & 0.665 (0.041) & \secondcolor{0.666 (0.044)} \\
    manual+r & 0.665 (0.035) & \firstcolor{0.660 (0.037)} & 0.664 (0.040) & \secondcolor{0.666 (0.046)} \\
    fpls+r & 0.666 (0.040) & 0.664 (0.042) & 0.672 (0.036) & 0.668 (0.047) \\
    fpca+r & 0.665 (0.042) & 0.665 (0.042) & 0.663 (0.040) & \secondcolor{0.666 (0.045)} \\
    w\_vs\_tmean+r & \firstcolor{0.651 (0.038)} & \firstcolor{0.660 (0.039)} & \firstcolor{0.649 (0.036)} & 0.675 (0.039) \\
    w\_vs\_mode+r & 0.660 (0.037) & 0.662 (0.039) & 0.663 (0.032) & 0.673 (0.046) \\
    w\_vs\_median+r & \secondcolor{0.652 (0.036)} & \firstcolor{0.660 (0.038)} & \secondcolor{0.653 (0.039)} & 0.672 (0.039) \\
    map\_vs\_tmean+r & 0.654 (0.037) & \secondcolor{0.661 (0.037)} & 0.659 (0.039) & 0.674 (0.041) \\
    map\_vs\_mode+r & 0.661 (0.036) & 0.667 (0.040) & 0.670 (0.036) & 0.704 (0.058) \\
    map\_vs\_median+r & 0.655 (0.035) & 0.663 (0.035) & 0.662 (0.043) & 0.678 (0.042) \\
    \bottomrule
  \end{tabular}
  \caption{Mean RMSE of predictors (lower is better) for 10 runs with GP regressors, one on each column, that obey an underlying linear \(L^2\)-model. The corresponding standard errors are shown between brackets.}
\end{table}
\newpage
\FloatBarrier{}

\begin{table}[htbp!]
  \vspace{1em}
  \footnotesize
  \centering
  \rowcolors{2}{}{teal!8}
  \begin{tabular}{lcc}
    \toprule
    \textbf{Prediction method} & \textbf{GBM + \(\bf{L^2}\)} & \textbf{GBM + RKHS}         \\
    \midrule
    pls1 & 0.668 (0.034) & 1.579 (0.145) \\
    lasso & \secondcolor{0.657 (0.041)} & 0.676 (0.046) \\
    fpls1 & 0.666 (0.035) & 2.654 (0.182) \\
    flin & 0.660 (0.037) & 3.434 (0.384) \\
    apls & 0.662 (0.040) & 1.100 (0.100) \\
    w\_pp\_tmean & \firstcolor{0.655 (0.042)} & \firstcolor{0.662 (0.039)} \\
    w\_pp\_mode & 0.659 (0.043) & 0.666 (0.036) \\
    w\_pp\_median & \firstcolor{0.655 (0.042)} & \firstcolor{0.662 (0.039)} \\
    map\_pp\_tmean & \secondcolor{0.657 (0.043)} & \secondcolor{0.665 (0.038)} \\
    map\_pp\_mode & 0.659 (0.041) & 0.670 (0.037) \\
    map\_pp\_median & \secondcolor{0.657 (0.043)} & \secondcolor{0.665 (0.038)} \\
    \bottomrule
    \toprule
    pls+r & 0.661 (0.040) & 1.574 (0.141) \\
    pca+r & 0.658 (0.040) & 2.315 (0.195) \\
    manual+r & 0.665 (0.039) & 1.478 (0.154) \\
    fpls+r & 0.665 (0.037) & 2.669 (0.189) \\
    fpca+r & 0.659 (0.040) & 2.311 (0.194) \\
    w\_vs\_tmean+r & \firstcolor{0.655 (0.040)} & 0.986 (0.186) \\
    w\_vs\_mode+r & 0.661 (0.044) & \secondcolor{0.665 (0.038)} \\
    w\_vs\_median+r & \secondcolor{0.656 (0.042)} & \firstcolor{0.663 (0.037)} \\
    map\_vs\_tmean+r & 0.662 (0.042) & \secondcolor{0.665 (0.038)} \\
    map\_vs\_mode+r & 0.669 (0.044) & \secondcolor{0.665 (0.038)} \\
    map\_vs\_median+r & 0.663 (0.044) & \secondcolor{0.665 (0.038)} \\
    \bottomrule
  \end{tabular}
  \caption{Mean RMSE of predictors (lower is better) for 10 runs with GBM regressors. In the first column the response obeys a linear \(L^2\)-model, while in the second column it follows a linear RKHS model. The corresponding standard errors are shown between brackets.}
\end{table}
\newpage
\FloatBarrier{}

\begin{table}[htbp!]
  \vspace{1em}
  \footnotesize
  \centering
  \rowcolors{2}{}{teal!8}
  \begin{tabular}{lccc}
    \toprule
    \textbf{Prediction method} & \textbf{Moisture}           & \textbf{Sugar}              & \textbf{Tecator}            \\
    \midrule
    pls1 & 0.232 (0.024) & 2.037 (0.219) & \secondcolor{2.606 (0.283)} \\
    lasso & 0.242 (0.026) & 1.985 (0.226) & 2.842 (0.352) \\
    fpls1 & 0.248 (0.022) & \secondcolor{1.972 (0.201)} & \firstcolor{2.605 (0.262)} \\
    flin & 1.235 (0.138) & \firstcolor{1.966 (0.198)} & 7.486 (0.648) \\
    apls & 0.237 (0.028) & 2.020 (0.226) & 2.641 (0.165) \\
    w\_pp\_tmean & \secondcolor{0.223 (0.018)} & 1.988 (0.216) & 2.721 (0.260) \\
    w\_pp\_mode & \firstcolor{0.222 (0.020)} & 1.996 (0.219) & 2.718 (0.266) \\
    w\_pp\_median & \secondcolor{0.223 (0.018)} & 1.989 (0.217) & 2.721 (0.262) \\
    map\_pp\_tmean & 0.228 (0.021) & 1.997 (0.212) & 2.724 (0.267) \\
    map\_pp\_mode & 0.236 (0.023) & 2.010 (0.210) & 2.741 (0.271) \\
    map\_pp\_median & 0.228 (0.021) & 1.996 (0.212) & 2.720 (0.267) \\
    \bottomrule
    \toprule
    pls+r & \firstcolor{0.225 (0.021)} & \secondcolor{1.998 (0.208)} & \firstcolor{2.536 (0.236)} \\
    pca+r & \secondcolor{0.233 (0.024)} & 2.034 (0.219) & 2.712 (0.183) \\
    manual+r & 0.268 (0.021) & 2.041 (0.214) & \secondcolor{2.586 (0.270)} \\
    fpls+r & 0.241 (0.018) & \firstcolor{1.962 (0.202)} & 2.595 (0.249) \\
    fpca+r & 0.307 (0.055) & 2.054 (0.226) & 2.657 (0.189) \\
    w\_vs\_tmean+r & 0.308 (0.075) & 2.003 (0.221) & 2.822 (0.323) \\
    w\_vs\_mode+r & 0.236 (0.018) & 2.050 (0.221) & 2.713 (0.255) \\
    w\_vs\_median+r & 0.236 (0.025) & 2.000 (0.217) & 2.801 (0.261) \\
    map\_vs\_tmean+r & 0.455 (0.284) & 2.059 (0.233) & 2.900 (0.403) \\
    map\_vs\_mode+r & 0.261 (0.029) & 2.188 (0.321) & 2.833 (0.291) \\
    map\_vs\_median+r & 0.266 (0.030) & 2.082 (0.219) & 2.826 (0.269) \\
    \bottomrule
  \end{tabular}
  \caption{Mean RMSE of predictors (lower is better) for 10 runs with real data sets, one on each column. The corresponding standard errors are shown between brackets.}
\end{table}
\newpage
\FloatBarrier{}

\subsubsection*{Functional logistic regression}

\begin{table}[htbp!]
  \footnotesize
  \centering
  \rowcolors{2}{}{teal!8}
  \begin{tabular}{lcccc}
    \toprule
    \textbf{Classification method} & \textbf{BM}                 & \textbf{fBM}                & \textbf{O-U}                & \textbf{Gaussian}           \\
    \midrule
    qda & 0.510 (0.000) & 0.510 (0.000) & 0.510 (0.000) & 0.500 (0.000) \\
    mdc & 0.804 (0.034) & 0.822 (0.022) & 0.735 (0.037) & 0.839 (0.045) \\
    log & 0.849 (0.031) & \firstcolor{0.848 (0.015)} & 0.824 (0.022) & 0.868 (0.036) \\
    lda & 0.694 (0.032) & 0.621 (0.066) & 0.624 (0.042) & 0.823 (0.028) \\
    fnc & 0.814 (0.034) & \firstcolor{0.848 (0.014)} & 0.736 (0.035) & 0.864 (0.046) \\
    flog & 0.845 (0.036) & 0.837 (0.024) & 0.809 (0.028) & 0.871 (0.033) \\
    flda & 0.846 (0.031) & 0.830 (0.029) & 0.813 (0.029) & 0.854 (0.039) \\
    fknn & 0.851 (0.033) & 0.834 (0.027) & 0.799 (0.024) & 0.847 (0.041) \\
    w\_pp\_tmean & \firstcolor{0.856 (0.030)} & 0.846 (0.011) & 0.828 (0.022) & 0.873 (0.035) \\
    w\_pp\_mode & 0.853 (0.031) & \secondcolor{0.847 (0.012)} & 0.825 (0.025) & \firstcolor{0.878 (0.036)} \\
    w\_pp\_median & \firstcolor{0.856 (0.029)} & \secondcolor{0.847 (0.011)} & 0.827 (0.022) & 0.873 (0.035) \\
    map\_pp\_tmean & 0.854 (0.034) & 0.845 (0.013) & \firstcolor{0.830 (0.025)} & 0.874 (0.035) \\
    map\_pp\_mode & 0.852 (0.032) & 0.846 (0.014) & \secondcolor{0.829 (0.025)} & \secondcolor{0.877 (0.036)} \\
    map\_pp\_median & \secondcolor{0.855 (0.030)} & 0.844 (0.013) & \firstcolor{0.830 (0.025)} & 0.876 (0.036) \\
    \bottomrule
    \toprule
    rkvs+log & \firstcolor{0.848 (0.024)} & 0.838 (0.026) & 0.790 (0.032) & 0.872 (0.041) \\
    pls+nc & 0.816 (0.038) & 0.828 (0.022) & 0.793 (0.029) & 0.867 (0.039) \\
    pls+log & \secondcolor{0.847 (0.034)} & 0.844 (0.021) & 0.817 (0.022) & 0.864 (0.037) \\
    pca+qda & 0.839 (0.034) & 0.840 (0.018) & 0.818 (0.026) & 0.854 (0.033) \\
    pca+log & 0.842 (0.032) & \secondcolor{0.847 (0.016)} & 0.824 (0.019) & 0.868 (0.033) \\
    manual+log & 0.846 (0.032) & \firstcolor{0.850 (0.012)} & 0.821 (0.018) & 0.869 (0.032) \\
    fpca+log & \secondcolor{0.847 (0.030)} & \secondcolor{0.847 (0.014)} & \secondcolor{0.830 (0.024)} & 0.866 (0.036) \\
    apls+nc & 0.819 (0.036) & \secondcolor{0.847 (0.014)} & 0.816 (0.027) & 0.854 (0.036) \\
    apls+log & 0.829 (0.041) & 0.844 (0.012) & 0.816 (0.027) & 0.857 (0.039) \\
    w\_vs\_tmean+log & 0.831 (0.038) & 0.840 (0.017) & 0.823 (0.033) & 0.873 (0.038) \\
    w\_vs\_mode+log & 0.846 (0.021) & 0.828 (0.019) & 0.806 (0.020) & 0.875 (0.040) \\
    w\_vs\_median+log & 0.838 (0.029) & 0.844 (0.017) & \firstcolor{0.834 (0.030)} & 0.875 (0.039) \\
    map\_vs\_tmean+log & 0.816 (0.028) & 0.838 (0.023) & 0.801 (0.027) & \secondcolor{0.876 (0.040)} \\
    map\_vs\_mode+log & 0.839 (0.022) & 0.831 (0.022) & 0.807 (0.014) & \firstcolor{0.877 (0.043)} \\
    map\_vs\_median+log & 0.829 (0.024) & 0.839 (0.019) & 0.816 (0.034) & 0.871 (0.048) \\
    \bottomrule
  \end{tabular}
  \caption{Mean accuracy of classifiers (higher is better) for 10 runs with GP regressors, one on each column, that obey an underlying logistic RKHS model. The corresponding standard errors are shown between brackets.}
\end{table}
\newpage
\FloatBarrier{}

\begin{table}[htbp!]
  \vspace{1em}
  \footnotesize
  \centering
  \rowcolors{2}{}{teal!8}
  \begin{tabular}{lcccc}
    \toprule
    \textbf{Classification method} & \textbf{BM}                 & \textbf{fBM}                & \textbf{O-U}                & \textbf{Gaussian}           \\
    \midrule
    qda & \firstcolor{0.610 (0.000)} & 0.610 (0.000) & \firstcolor{0.620 (0.000)} & \secondcolor{0.610 (0.000)} \\
    mdc & 0.602 (0.033) & \firstcolor{0.619 (0.048)} & \secondcolor{0.615 (0.029)} & 0.603 (0.042) \\
    log & 0.594 (0.017) & 0.577 (0.039) & 0.591 (0.024) & 0.609 (0.037) \\
    lda & 0.507 (0.030) & 0.518 (0.029) & 0.541 (0.039) & 0.591 (0.033) \\
    fnc & \secondcolor{0.607 (0.038)} & \secondcolor{0.614 (0.045)} & 0.609 (0.029) & \firstcolor{0.625 (0.040)} \\
    flog & 0.580 (0.020) & 0.602 (0.037) & 0.600 (0.031) & 0.609 (0.028) \\
    flda & 0.601 (0.027) & 0.609 (0.049) & 0.593 (0.032) & 0.595 (0.048) \\
    fknn & 0.587 (0.056) & 0.576 (0.033) & 0.564 (0.042) & 0.578 (0.041) \\
    w\_pp\_tmean & 0.597 (0.027) & 0.600 (0.026) & 0.592 (0.023) & 0.608 (0.035) \\
    w\_pp\_mode & 0.599 (0.028) & 0.606 (0.027) & 0.595 (0.030) & 0.605 (0.034) \\
    w\_pp\_median & 0.597 (0.023) & 0.599 (0.030) & 0.591 (0.020) & 0.607 (0.037) \\
    map\_pp\_tmean & 0.595 (0.022) & 0.599 (0.027) & 0.594 (0.020) & 0.602 (0.037) \\
    map\_pp\_mode & 0.605 (0.027) & 0.604 (0.030) & 0.602 (0.030) & 0.606 (0.041) \\
    map\_pp\_median & 0.593 (0.026) & 0.599 (0.028) & 0.600 (0.023) & 0.602 (0.034) \\
    \bottomrule
    \toprule
    rkvs+log & 0.569 (0.039) & 0.586 (0.026) & 0.593 (0.035) & 0.611 (0.034) \\
    pls+nc & \firstcolor{0.610 (0.033)} & \firstcolor{0.623 (0.042)} & \firstcolor{0.607 (0.035)} & \firstcolor{0.629 (0.036)} \\
    pls+log & 0.590 (0.029) & 0.589 (0.036) & 0.593 (0.020) & \secondcolor{0.621 (0.043)} \\
    pca+qda & 0.577 (0.036) & \secondcolor{0.615 (0.032)} & 0.599 (0.043) & 0.618 (0.045) \\
    pca+log & 0.590 (0.027) & 0.593 (0.027) & 0.598 (0.037) & 0.616 (0.036) \\
    manual+log & 0.580 (0.026) & 0.585 (0.030) & 0.593 (0.019) & \secondcolor{0.621 (0.044)} \\
    fpca+log & \secondcolor{0.599 (0.021)} & 0.592 (0.034) & 0.600 (0.039) & 0.614 (0.042) \\
    apls+nc & 0.593 (0.024) & 0.569 (0.047) & 0.591 (0.039) & 0.615 (0.023) \\
    apls+log & 0.571 (0.030) & 0.585 (0.033) & 0.594 (0.025) & 0.614 (0.038) \\
    w\_vs\_tmean+log & 0.588 (0.034) & 0.594 (0.022) & 0.596 (0.017) & 0.605 (0.029) \\
    w\_vs\_mode+log & 0.597 (0.024) & 0.597 (0.028) & \secondcolor{0.601 (0.026)} & 0.617 (0.026) \\
    w\_vs\_median+log & 0.591 (0.030) & 0.589 (0.023) & 0.599 (0.035) & 0.600 (0.031) \\
    map\_vs\_tmean+log & 0.590 (0.031) & 0.594 (0.026) & 0.589 (0.030) & 0.609 (0.031) \\
    map\_vs\_mode+log & 0.597 (0.024) & 0.593 (0.022) & 0.599 (0.027) & 0.617 (0.021) \\
    map\_vs\_median+log & 0.587 (0.036) & 0.592 (0.021) & 0.598 (0.036) & 0.605 (0.034) \\
    \bottomrule
  \end{tabular}
  \caption{Mean accuracy of classifiers (higher is better) for 10 runs with GP regressors, one on each column, that obey an underlying logistic \(L^2\)-model. The corresponding standard errors are shown between brackets.}
\end{table}
\newpage
\FloatBarrier{}

\begin{table}[htbp!]
  \vspace{1em}
  \footnotesize
  \centering
  \rowcolors{2}{}{teal!8}
  \begin{tabular}{lcccc}
    \toprule
    \textbf{Classification method} & \textbf{Heteroscedastic}    & \textbf{Homoscedastic}      \\
    \midrule
    qda & 0.530 (0.000) & 0.530 (0.000) \\
    mdc & \secondcolor{0.532 (0.004)} & 0.600 (0.042) \\
    log & 0.520 (0.027) & 0.612 (0.048) \\
    lda & 0.506 (0.041) & 0.559 (0.040) \\
    fnc & 0.457 (0.049) & 0.604 (0.042) \\
    flog & 0.499 (0.034) & 0.617 (0.035) \\
    flda & 0.486 (0.059) & 0.602 (0.040) \\
    fknn & \firstcolor{0.538 (0.037)} & 0.538 (0.049) \\
    w\_pp\_tmean & 0.517 (0.030) & 0.621 (0.043) \\
    w\_pp\_mode & 0.527 (0.012) & 0.623 (0.043) \\
    w\_pp\_median & 0.520 (0.028) & 0.622 (0.040) \\
    map\_pp\_tmean & 0.523 (0.017) & \secondcolor{0.626 (0.044)} \\
    map\_pp\_mode & \secondcolor{0.532 (0.010)} & 0.622 (0.046) \\
    map\_pp\_median & 0.529 (0.013) & \firstcolor{0.629 (0.040)} \\
    \bottomrule
    \toprule
    rkvs+log & 0.509 (0.044) & 0.613 (0.031) \\
    pls+nc & 0.463 (0.042) & 0.593 (0.038) \\
    pls+log & 0.507 (0.036) & 0.600 (0.048) \\
    pca+qda & \firstcolor{0.642 (0.020)} & 0.583 (0.047) \\
    pca+log & 0.513 (0.032) & 0.597 (0.043) \\
    manual+log & 0.512 (0.036) & 0.612 (0.048) \\
    fpca+log & 0.519 (0.033) & 0.599 (0.047) \\
    apls+nc & 0.484 (0.065) & 0.606 (0.037) \\
    apls+log & 0.513 (0.036) & 0.605 (0.040) \\
    w\_vs\_tmean+log & 0.519 (0.033) & 0.624 (0.039) \\
    w\_vs\_mode+log & \secondcolor{0.530 (0.000)} & 0.623 (0.035) \\
    w\_vs\_median+log & \secondcolor{0.530 (0.000)} & \firstcolor{0.633 (0.037)} \\
    map\_vs\_tmean+log & 0.519 (0.033) & 0.626 (0.045) \\
    map\_vs\_mode+log & \secondcolor{0.530 (0.000)} & 0.630 (0.024) \\
    map\_vs\_median+log & \secondcolor{0.530 (0.000)} & \secondcolor{0.632 (0.037)} \\
    \bottomrule
  \end{tabular}
  \caption{Mean accuracy of classifiers (higher is better) for 10 runs with a mix of regressors coming from two different GPs and labeled according to their origin. In the first column we try to separate two heteroscedastic Brownian motions, while in the second column we discriminate between two homoscedastic Brownian motions. The corresponding standard errors are shown between brackets.}
\end{table}
\newpage
\FloatBarrier{}


\begin{table}[htbp!]
  \vspace{1em}
  \footnotesize
  \centering
  \rowcolors{2}{}{teal!8}
  \begin{tabular}{lcccc}
    \toprule
    \textbf{Classification method} & \textbf{Growth}             & \textbf{Medflies}           & \textbf{Phoneme}            \\
    \midrule
    qda & 0.581 (0.000) & 0.579 (0.028) & 0.578 (0.034) \\
    mdc & 0.694 (0.103) & 0.526 (0.024) & 0.704 (0.042) \\
    log & \firstcolor{0.961 (0.028)} & 0.575 (0.022) & \firstcolor{0.809 (0.052)} \\
    lda & 0.894 (0.054) & 0.576 (0.016) & 0.599 (0.049) \\
    fnc & 0.735 (0.117) & 0.550 (0.040) & 0.755 (0.065) \\
    flog & 0.926 (0.043) & 0.596 (0.026) & 0.785 (0.053) \\
    flda & 0.939 (0.051) & 0.550 (0.023) & 0.782 (0.046) \\
    fknn & 0.948 (0.036) & 0.539 (0.028) & \secondcolor{0.796 (0.035)} \\
    w\_pp\_tmean & 0.948 (0.041) & \firstcolor{0.611 (0.029)} & 0.790 (0.037) \\
    w\_pp\_mode & 0.935 (0.046) & 0.603 (0.033) & 0.790 (0.041) \\
    w\_pp\_median & \secondcolor{0.952 (0.036)} & \secondcolor{0.610 (0.029)} & 0.794 (0.041) \\
    map\_pp\_tmean & 0.945 (0.046) & 0.606 (0.035) & 0.791 (0.031) \\
    map\_pp\_mode & 0.935 (0.046) & 0.601 (0.040) & 0.779 (0.039) \\
    map\_pp\_median & \secondcolor{0.952 (0.036)} & 0.606 (0.038) & 0.791 (0.031) \\
    \bottomrule
    \toprule
    rkvs+log & 0.929 (0.050) & 0.589 (0.032) & \secondcolor{0.804 (0.046)} \\
    pls+nc & 0.858 (0.090) & 0.558 (0.032) & 0.776 (0.058) \\
    pls+log & 0.945 (0.032) & 0.574 (0.019) & \firstcolor{0.810 (0.043)} \\
    pca+qda & 0.955 (0.030) & 0.576 (0.025) & 0.754 (0.043) \\
    pca+log & \secondcolor{0.958 (0.035)} & 0.562 (0.028) & 0.793 (0.049) \\
    manual+log & 0.932 (0.055) & \firstcolor{0.615 (0.012)} & 0.730 (0.046) \\
    fpca+log & 0.955 (0.033) & 0.561 (0.024) & 0.769 (0.050) \\
    apls+nc & \firstcolor{0.961 (0.028)} & 0.551 (0.030) & 0.781 (0.047) \\
    apls+log & 0.952 (0.036) & 0.562 (0.015) & 0.776 (0.048) \\
    w\_vs\_tmean+log & \firstcolor{0.961 (0.028)} & \secondcolor{0.597 (0.036)} & 0.749 (0.071) \\
    w\_vs\_mode+log & 0.948 (0.046) & \secondcolor{0.597 (0.025)} & \secondcolor{0.804 (0.037)} \\
    w\_vs\_median+log & 0.952 (0.033) & \secondcolor{0.597 (0.021)} & 0.779 (0.049) \\
    map\_vs\_tmean+log & 0.945 (0.046) & 0.592 (0.047) & 0.746 (0.066) \\
    map\_vs\_mode+log & 0.948 (0.046) & 0.592 (0.036) & 0.779 (0.035) \\
    map\_vs\_median+log & 0.939 (0.047) & 0.592 (0.031) & 0.782 (0.050) \\
    \bottomrule
  \end{tabular}
  \caption{Mean accuracy of classifiers (higher is better) for 10 runs with real data sets, one on each column. The corresponding standard errors are shown between brackets.}
\end{table}
\newpage
\FloatBarrier{}


\section{Source code overview}\label{app:source-code}

The Python code developed for this work is available under a GPLv3 license at the GitHub repository \url{https://github.com/antcc/rk-bfr-jump}. The code is adequately documented and is structured in several directories as follows:

\begin{itemize}
  \item In the \texttt{rkbfr\_jump} folder we find the files responsible for the implementation of our Bayesian models, separated according to the functionality they provide. There is also a \texttt{utils} folder inside with some utility files for simulation, experimentation and visualization.
  \item The \texttt{reference\_methods} folder contains our implementation of the functional comparison algorithms that were not available through a standard Python library.
  \item The \texttt{experiments} folder contains plain text files with the numerical experimental results shown in Appendix~\ref{app:tables}, as well as \texttt{.csv} files that facilitate working with them.
  \item At the root folder we have a Python script \texttt{experiments.py} for executing our experiments, which accepts several user-specified parameters (such as the number of iterations or the type of data set). There is also a \texttt{setup.py} file to install our method as a Python package.
\end{itemize}

When possible, the code was implemented in a generic way that would allow for easy extensions or derivations. It was also developed with efficiency in mind, so many functions and methods exploit the vectorization capabilities of the \textit{numpy} and \textit{scipy} libraries, and are sometimes parallelized using \textit{numba}. Moreover, since we followed closely the style of the \textit{scikit-learn} and \textit{scikit-fda} libraries, our methods are compatible and could be integrated (after some minor tweaking) with both of them. 

The code for the experiments was executed with a random seed set to the value 2024 for reproducibility. We provide a script file \texttt{launch.sh} that illustrates a typical execution. Lastly, there are \textit{Jupyter} notebooks that demonstrate the use of our methods in a more visual way. Inside these notebooks there is a step-by-step guide on how one might execute our algorithms, accompanied by many graphical representations, and offering the possibility of changing multiple parameters to experiment with the code. In addition, there is also a notebook that can be used to generate all the tables and figures of this document and the main article pertaining to the experimental results.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%